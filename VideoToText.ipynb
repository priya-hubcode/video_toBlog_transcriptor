{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa6e97c-2bc0-4cf8-949d-b1b7f90f707c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efcd40-50d4-4bd1-b389-86fd65dfa247",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140a483-ace1-440a-854c-ae3dffeab1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec632355-88e1-4c12-86ae-34e6e04c3263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85506cc7-6f32-484d-99d6-73369c090e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd00892-239e-4982-8a89-9917e9c9303b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a97197-2af4-43b3-b3c2-6ac3ca7c66a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e4f4f-d6a9-45e9-a8e2-f135c14c2873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/linto-ai/whisper-timestamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b7a7d-2d0b-44d7-90bf-e660a3cf44c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943994bb-f7bb-404b-aab3-c4c3d14324c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4285636-26eb-4dd6-9721-c90356f8dcd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18ac55-efce-487d-9d5b-61515384fa71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027541d-c44f-47cd-a193-16f186246746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c61c6-c4f2-424f-8ae6-b1fea08a9119",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7a34cf1f-bbff-4a9c-abd7-d5ea96e6627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube as pyt\n",
    "import os\n",
    "from moviepy.editor import *\n",
    "from moviepy.editor import AudioFileClip\n",
    "import moviepy.editor as mp\n",
    "import whisper\n",
    "import whisper_timestamped as whisper\n",
    "from openai import OpenAI\n",
    "from base64 import b64encode\n",
    "import requests\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "import torch\n",
    "import regex as re\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "path = f'{os.getcwd()}/video_toBlog_transcriptor'\n",
    "url = 'https://www.youtube.com/watch?v=zduSFxRajkE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bcfef-8d38-4e4f-83c3-26c46e509762",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3a0bb06c-06bd-450d-afa4-0daeca7d3bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download is completed successfully\n"
     ]
    }
   ],
   "source": [
    "# link of the video to be downloaded \n",
    "link = \"https://www.youtube.com/watch?v=zduSFxRajkE\"\n",
    "\n",
    "def Download(link):\n",
    "    yt = pyt(link)\n",
    "    stream = yt.streams.get_highest_resolution()\n",
    "    stream.download(output_path=f\"{path}/original_files/video\", filename=\"video_file.mp4\")\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_stream.download(output_path=f\"{path}/original_files/audio\", filename=\"audio_file.mp4\")\n",
    "    print(\"Download is completed successfully\")\n",
    "    \n",
    "def get_title(url):\n",
    "    yt = pyt(url)\n",
    "    return yt.title\n",
    "\n",
    "title = get_title(\"https://www.youtube.com/watch?v=zduSFxRajkE\")\n",
    "\n",
    "Download(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497a98e-e5ca-47a3-99d2-7a99765d71cc",
   "metadata": {},
   "source": [
    "### Chunk video into multiple files based on chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "633ddb23-5810-4ba3-8d43-f25c2160da91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_audio(file_path, chunk_size_mb=12, output_folder=f\"{path}/split_chunks\"):\n",
    "    global split_audio_return\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    if file_size_mb <= chunk_size_mb:\n",
    "        print(\"File size is within the limit. No need to split.\")\n",
    "        return\n",
    "    else:\n",
    "        split_audio_return = True\n",
    "\n",
    "    clip = AudioFileClip(file_path)\n",
    "    total_duration = clip.duration\n",
    "    chunk_duration = (chunk_size_mb / file_size_mb) * total_duration\n",
    "\n",
    "    # Split the audio\n",
    "    start = 0\n",
    "    part = 1\n",
    "    while start < total_duration:\n",
    "        end = min(start + chunk_duration, total_duration)\n",
    "        chunk = clip.subclip(start, end)\n",
    "        chunk_filename = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(file_path))[0]}_part{part}.mp4\")\n",
    "        chunk.write_audiofile(chunk_filename, bitrate=\"64k\", codec=\"aac\")\n",
    "\n",
    "        print(f\"Created chunk: {chunk_filename}\")\n",
    "\n",
    "        start = end\n",
    "        part += 1\n",
    "\n",
    "    clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "13802b2a-b568-4ca1-b653-4499b5461dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 46.61 MB\n",
      "MoviePy - Writing audio in /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part1.mp4\n",
      "MoviePy - Writing audio in /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part2.mp4\n",
      "MoviePy - Writing audio in /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part3.mp4\n",
      "MoviePy - Writing audio in /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/split_chunks/audio_file_part4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# os.path.exists(\"Lets build the GPT Tokenizer.mp4\")\n",
    "split_audio(f\"{path}/original_files/audio/audio_file.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b92908-7fa8-45ba-b3a9-4bbbb096ef68",
   "metadata": {},
   "source": [
    "### Transcribing with Whisper & Writing to JSON File(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e0bf5f6c-043d-46fe-a73c-a095c0d2a1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_audio_file(output_folder=f\"{path}/transcript_json\"):\n",
    "    input_folder_chunks = f'{path}/split_chunks'\n",
    "    for filename in os.listdir(f'{path}/split_chunks'):\n",
    "        print(filename)\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            audio = whisper.load_audio(f'{path}/split_chunks/{filename}')\n",
    "            model = whisper.load_model(\"base\")\n",
    "\n",
    "            result = whisper.transcribe(model, audio, language=\"en\")\n",
    "            json_filename = f\"{os.path.splitext(filename)[0]}_transcript.json\"\n",
    "            output_path = os.path.join(output_folder, json_filename)\n",
    "            # print(json.dumps(result, indent = 2, ensure_ascii = False))\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(result['segments'], f, indent=4)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3f378295-1132-4c49-b5ac-713ee3bbd532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_file_part1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 206351/206351 [01:29<00:00, 2309.67frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_file_part4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 182381/182381 [01:29<00:00, 2048.12frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "audio_file_part3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 206351/206351 [01:41<00:00, 2036.32frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_file_part2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 206351/206351 [01:36<00:00, 2128.73frames/s]\n"
     ]
    }
   ],
   "source": [
    "res = process_audio_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9e621-b2ef-4d30-8ab9-33f97532cd79",
   "metadata": {},
   "source": [
    "### Cleaning & Concatenating the Transcription JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a0063baa-b0ae-4ed9-8102-91f75c7f991f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_part_number(filename):\n",
    "    match = re.search(r'part(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def process_file(filepath, max_id, last_end_time):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        time_adjustment = last_end_time - float(data[0]['start']) if last_end_time else 0\n",
    "        new_data = []\n",
    "        for entry in data:\n",
    "            new_entry = {\n",
    "                'id': max_id + 1,\n",
    "                'start': float(entry['start']) + time_adjustment,\n",
    "                'end': float(entry['end']) + time_adjustment,\n",
    "                'text': entry['text']\n",
    "            }\n",
    "            new_data.append(new_entry)\n",
    "            max_id += 1\n",
    "        return new_data, max_id, new_data[-1]['end'] if new_data else last_end_time\n",
    "\n",
    "def process_transcripts(directory_path, split_audio_return):\n",
    "    files = os.listdir(directory_path)\n",
    "    combined_data = []\n",
    "    max_id = -1\n",
    "    last_end_time = 0.0\n",
    "\n",
    "    if split_audio_return:\n",
    "        sorted_files = sorted(\n",
    "            [file for file in files if file.startswith('audio') and file.endswith('.json')],\n",
    "            key=extract_part_number\n",
    "        )\n",
    "    else:\n",
    "        sorted_files = [file for file in files if file.endswith('.json') and not 'part' in file]\n",
    "\n",
    "    for filename in sorted_files:\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        processed_data, max_id, last_end_time = process_file(full_path, max_id, last_end_time)\n",
    "        combined_data.extend(processed_data)\n",
    "\n",
    "    # Output the combined data to a new JSON file\n",
    "    output_path = os.path.join(directory_path, 'combined_data.json')\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        json.dump(combined_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7f4599d7-9ff0-45b2-8378-49448db43236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_transcripts(f\"{path}/transcript_json\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "16799e79-8513-4e0d-a411-81a7ff3795ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_transcript(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    transcript = \"\"\n",
    "    for entry in data:\n",
    "        transcript += entry['text']\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b72133ac-0052-43a3-a857-f5a72e70634d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_transcript = full_transcript(f\"{path}/transcript_json/combined_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ec663-e438-4bf2-874d-ea2f4e95f853",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate Embeddings using sentence-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "462d2590-cd5f-41c0-913e-d38fe74ea487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef279302edac4ae5bdc8fe78ce1955d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01f08ec3d8f4bea9cdefdd87c5dde3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df7680745854b188091be4d12fed38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1cfc3d1d47421bb638597113563611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0b6077028e42238a7597b64fc6b2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a512e778104fce94dd472fef58890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "81faac64-91ac-4e31-a59c-59b637f2b9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7ad67a73-be19-4a71-8ed9-0823ca43fa63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "70de87c3-28b0-4f68-9dc0-ab06ea937528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "60ffd521-1d05-443a-ad02-285b1da4c184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_loader = JSONLoader(f\"{path}/transcript_json/combined_data.json\", jq_schema=\".[]\", text_content=False)\n",
    "json_texts = json_loader.load()\n",
    "embedding = get_embeddings(str(json_texts))\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab1de0-dd1d-4367-91c4-7ce5158abe63",
   "metadata": {},
   "source": [
    "### Train using HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3e4ce495-b053-4ec2-afd1-578d89c8dddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "eb8e7c36-5c6c-47df-87ab-99c7fb0e0451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(json_texts, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1363f483-2a6c-4c47-9190-c01ce9e0d053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed and chunk transcript\n",
    "text_splitter = SemanticChunker(hf_embeddings)\n",
    "split_docs = text_splitter.create_documents([video_transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b0da7185-6d0f-4542-84f0-d4572e7771df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_short_documents(documents, min_length=2000):\n",
    "    i = 0\n",
    "    while i < len(documents) - 1:\n",
    "        current_doc = documents[i]\n",
    "        if len(current_doc.page_content) < min_length:\n",
    "            documents[i + 1].page_content = current_doc.page_content + documents[i + 1].page_content\n",
    "            del documents[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "614a288b-752f-4f12-8b7c-728cbd3ff227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_docs = merge_short_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616916b4-6a3a-4dd3-abda-843ca4a7cf26",
   "metadata": {},
   "source": [
    "### Main Markdown File Prompting with GPT-3.5-Turbo & LangChain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0eefaa25-daaf-4323-9b24-9c1afa8d8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_prompt_template = \"\"\"\n",
    "\n",
    "Below is a script from a video that I am making into a companion guide blog post first. \\\n",
    "You are a helpful assistant made to assist in the creation I'm doing. \\\n",
    "This is a continuation of a guide so include chapters, key summaries, and incorporate visual aids and direct links to relevant parts of the video, \\\n",
    "however do not include any conclusion or overarching title. \\\n",
    "For visual aids, specific frames from the video will be identified where images can be inserted to enhance understanding. \\\n",
    "For direct links, portions of the text should be hyperlinked to their corresponding times in the video. \\\n",
    "To indicate that a sentence should be hyperlinked, insert the raw text of the transcript next to the word with the indicator <HYPERLINK: \"corresponding transcript text\">. \\\n",
    "To indicate a picture regarding the text, insert the indicator <PICTURE: \"corresponding transcript text\">. \\\n",
    "It is crucial to use the raw text from the transcript that will be used, as the additional tools that will be inserting the hyperlinks and pictures need this to know where in the video to look.\n",
    "\n",
    "In this blog post, in addition to the paragraphs: \\\n",
    "\n",
    "Create titles or headings that encapsulate main points and ideas \\\n",
    "\n",
    "Format your response in markdown, ensuring distinction and clean styling between titles and paragraphs. \\\n",
    "Be sure to include the image placeholders, and hyperlinks with enough distinguishable text WITHOUT ANY QUOTATIONS, as the placeholders will be fed into a semantic search algorithm. \\\n",
    "This structured approach will be applied to the entire transcript. \\\n",
    "The example below only shows one style, but use multiple styles including different headings, bullet points, and other markdown elements when needed. \\\n",
    "\n",
    "Here are shortened example of the input and shortened expected output:\n",
    "\n",
    "example input:\n",
    "\n",
    "Hi everyone. So in this video I'd like us to cover the process of tokenization in large language models. Now you see here that I have a sad face and that's because tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot gums to be aware of and a lot of oddness with large language models typically traces back to tokenization. So what is tokenization? Now in my previous video Let's Build GPT from Scratch we actually already did tokenization but we did a very naive simple version of tokenization. So when you go to the Google Colab for that video you see here that we loaded our training set and our training set was this Shakespeare dataset. Now in the beginning the Shakespeare dataset is just a large string in Python it's just text and so the question is how do we plug text into large language models and in this case here we created a vocabulary of 65 possible characters that we saw occur in this string. These were the possible characters and we saw that there are 65 of them and then we created a lookup table for converting from every possible character a little string piece into a token an integer. So here for example we tokenized the string hi there and we received this sequence of tokens and here we took the first 1000 characters of our dataset and we encoded it into tokens and because this is character level we received 1000 tokens in a sequence so token 18, 47, etc. Now later we saw that the way we plug these tokens into the language model is by using an embedding table and so basically if we have 65 possible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the integer associated with every single token we're using that as a lookup into this table and we're plucking out the corresponding row and this row is trainable parameters that we're going to train using backpropagation and this is the vector that then feeds into the transformer and that's how the transformer sort of perceives every single token. So here we had a very naive tokenization process that was a character level tokenizer\n",
    "\n",
    "example output:\n",
    "\n",
    "Introduction to Tokenization\n",
    "----------------------------\n",
    "\n",
    "Welcome to our comprehensive guide on tokenization in large language models (LLMs). Tokenization is a critical yet complex aspect of working with LLMs, essential for understanding how these models process text data. Despite its challenges, tokenization is foundational, as it converts strings of text into sequences of tokens, small units of text that LLMs can manage more effectively.\n",
    "\n",
    "<PICTURE: Now you see here that I have a sad face and that's because tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot gums>\n",
    "\n",
    "Understanding the Basics of Tokenization\n",
    "----------------------------------------\n",
    "\n",
    "Tokenization involves creating a vocabulary from all unique characters or words in a dataset and converting each into a corresponding integer token. This process was briefly introduced in our \"Let's Build GPT from Scratch\" video, where we tokenized a Shakespeare dataset at a character level, creating a vocabulary of 65 possible characters.\n",
    "\n",
    "<HYPERLINK: So what is tokenization? Now in my previous video Let's Build GPT from Scratch we actually already did tokenization but we did a very naive simple version of tokenization. So when you go to the Google Colab for that video you see here that we loaded>\n",
    "\n",
    "The Role of Embedding Tables in Tokenization\n",
    "--------------------------------------------\n",
    "\n",
    "After tokenization, the next step involves using an embedding table, where each token's integer is used as a lookup to extract a row of trainable parameters. These parameters, once trained, feed into the transformer model, allowing it to perceive each token effectively.\n",
    "\n",
    "<PICTURE: using backpropagation and this is the vector that then feeds into the transformer and that's how the transformer sort of perceives every single token. So here we had a very naive tokenization process that was a character level tokenizer>\n",
    "\n",
    "end examples.\n",
    "\n",
    "Here is the transcript:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo-0125\")\n",
    "guide_prompt = ChatPromptTemplate.from_template(guide_prompt_template)\n",
    "\n",
    "guide_chain = (\n",
    "    {\"transcript\": RunnablePassthrough()} \n",
    "    | guide_prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "def generate_markdown(merged_docs, path, guide_chain):\n",
    "    markdown_outputs = []\n",
    "    for doc in merged_docs:\n",
    "        print(f\"*********************************\\n{doc}\")\n",
    "        output = guide_chain.invoke(doc.page_content)\n",
    "        markdown_outputs.append(output)\n",
    "        time.sleep(1200)\n",
    "    combined_output = '\\n\\n'.join(markdown_outputs)\n",
    "    with open(f'{path}/transcript_json/llm_outline.txt', 'w') as file:\n",
    "        file.write(combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd7eb5-474d-404c-b6d2-c0c422802caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_markdown(merged_docs, path, guide_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213cc372-5d75-4030-ba03-c9f1c473c04c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dealing with the Placeholders, Grabbing Pictures & Formatting Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4d9a5ca1-18ef-4ad9-acf4-92017c8e042a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grab_frame(video, second):\n",
    "    frames_dir = f'{path}/frames'\n",
    "    if not os.path.exists(frames_dir):\n",
    "        os.makedirs(frames_dir)\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_number = round(int(second * fps))\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if frame_number >= total_frames:\n",
    "        print(f\"Error: Frame number {frame_number} exceeds total frames in video.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    \n",
    "    frame_path = os.path.join(frames_dir, f'frame_{second}.jpg')\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    cap.release()\n",
    "    \n",
    "    return frame_path\n",
    "\n",
    "def retrieve_time(segment):\n",
    "    docs = retriever.get_relevant_documents(segment)\n",
    "    docs_dict = json.loads(docs[0].page_content)\n",
    "    start_time = docs_dict[\"start\"]\n",
    "    end_time = docs_dict[\"end\"]\n",
    "    time = (start_time + end_time) / 2\n",
    "    final_time = round(time)\n",
    "    return final_time\n",
    "\n",
    "def create_hyperlink(segment, url):\n",
    "    time = retrieve_time(segment)\n",
    "    time_link = f\"{url}&t={time}s\"\n",
    "    return time_link\n",
    "\n",
    "def format_seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def process_placeholder(placeholder):\n",
    "    if placeholder.startswith(\"<PICTURE:\"):\n",
    "        description = placeholder[9:-1]\n",
    "        time = retrieve_time(description)\n",
    "        image_path = grab_frame(video_path, time)\n",
    "        # Embed the image using markdown with a specified width\n",
    "        return f'<img src=\"{image_path}\" alt=\"{description}\" width=\"450\"/>'\n",
    "    elif placeholder.startswith(\"<HYPERLINK:\"):\n",
    "        text = placeholder[11:-1]\n",
    "        time = retrieve_time(text)\n",
    "        formatted_time = format_seconds_to_hms(time)\n",
    "        hyperlink = create_hyperlink(text, url)\n",
    "        return f'[Jump to this part of the video: {formatted_time}]({hyperlink})'\n",
    "    else:\n",
    "        return placeholder\n",
    "\n",
    "def replace_placeholders(content):\n",
    "    placeholders = re.findall(r\"<[^>]+>\", content)\n",
    "    for placeholder in placeholders:\n",
    "        replacement = process_placeholder(placeholder)\n",
    "        content = content.replace(placeholder, replacement, 1)\n",
    "    return content\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def convert_txt(path, title, db):\n",
    "    txt_file_path = f'{path}/transcript_json/llm_outline.txt'\n",
    "    output_file_path = f'{path}/companion_guide.txt'\n",
    "    global video_path\n",
    "    video_path = f'{path}/original_files/video/video_file.mp4'\n",
    "    global retriever\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    \n",
    "    content = read_file(txt_file_path)\n",
    "    updated_content = replace_placeholders(content)\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "    \n",
    "    print(f\"Updated markdown content has been written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "27be7bd6-2f1b-4325-8123-32c5db782f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated markdown content has been written to /home/jovyan/work/users/pshalini/video_toBlog_transcriptor/companion_guide.txt\n"
     ]
    }
   ],
   "source": [
    "convert_txt(path, title, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cefe0e94-a4ed-409f-b85e-1798148cc8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1540\n"
     ]
    }
   ],
   "source": [
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11a9f3-da35-43fc-96b6-9aba32a09a50",
   "metadata": {},
   "source": [
    "### Generate md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "17bd5d68-6e81-4051-b0c2-452a85db0532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'companion_guide_output.md'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copyfile(\"companion_guide.txt\", \"companion_guide_output.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
