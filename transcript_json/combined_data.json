[
    {
        "id": 0,
        "start": 0.12,
        "end": 4.86,
        "text": " Hi everyone. So in this video I'd like us to cover the process of tokenization in large language models."
    },
    {
        "id": 1,
        "start": 5.6,
        "end": 10.93,
        "text": " Now you see here that I have a sad face and that's because tokenization is my least favorite part"
    },
    {
        "id": 2,
        "start": 10.93,
        "end": 15.16,
        "text": " of working with large language models but unfortunately it is necessary to understand in some detail"
    },
    {
        "id": 3,
        "start": 15.52,
        "end": 20.08,
        "text": " because it is fairly hairy, gnarly and there's a lot of hidden foot guns to be aware of"
    },
    {
        "id": 4,
        "start": 20.46,
        "end": 24.72,
        "text": " and a lot of oddness with large language models typically traces back to tokenization."
    },
    {
        "id": 5,
        "start": 25.7,
        "end": 30.26,
        "text": " So what is tokenization? Now in my previous video let's build GPT from scratch."
    },
    {
        "id": 6,
        "start": 31.54,
        "end": 36.18,
        "text": " We actually already did tokenization but we did a very naive simple version of tokenization."
    },
    {
        "id": 7,
        "start": 37.02,
        "end": 42.56,
        "text": " So when you go to the Google call lab for that video you see here that we loaded our training set"
    },
    {
        "id": 8,
        "start": 42.62,
        "end": 49.08,
        "text": " and our training set was this Shakespeare data set. Now in the beginning the Shakespeare data set is"
    },
    {
        "id": 9,
        "start": 49.08,
        "end": 55.76,
        "text": " just a large string in Python. It's just text. So the question is how do we plug text into large language models"
    },
    {
        "id": 10,
        "start": 56.78,
        "end": 64.21,
        "text": " and in this case here we created a vocabulary of 65 possible characters that we saw occur in this"
    },
    {
        "id": 11,
        "start": 64.21,
        "end": 69.58,
        "text": " string. These were the possible characters and we saw that there are 65 of them and then we created"
    },
    {
        "id": 12,
        "start": 69.68,
        "end": 76.74,
        "text": " a lookup table for converting from every possible character a little string piece into a token"
    },
    {
        "id": 13,
        "start": 77.14,
        "end": 83.66,
        "text": " and integer. So here for example we tokenized the string high there and we received this sequence"
    },
    {
        "id": 14,
        "start": 83.66,
        "end": 91.2,
        "text": " of tokens. And here we took the first 1000 characters of our data set and we encoded it into tokens"
    },
    {
        "id": 15,
        "start": 92.02,
        "end": 99.48,
        "text": " and because this is this is character level we received 1000 tokens in a sequence. So token 1847"
    },
    {
        "id": 16,
        "start": 99.52,
        "end": 107.59,
        "text": " etc. Now later we saw that the way we plug these tokens into the language model is by using an"
    },
    {
        "id": 17,
        "start": 107.59,
        "end": 114.09,
        "text": " embedding table. And so basically if we have 65 possible tokens then this embedding table is going"
    },
    {
        "id": 18,
        "start": 114.09,
        "end": 120.3,
        "text": " to have 65 rows and roughly speaking we're taking the integer associated with every single token"
    },
    {
        "id": 19,
        "start": 120.78,
        "end": 125.54,
        "text": " we're using that as a lookup into this table and we're plugging out the corresponding row"
    },
    {
        "id": 20,
        "start": 126.46,
        "end": 130.94,
        "text": " and this row is a is trainable parameters that we're going to train using back propagation"
    },
    {
        "id": 21,
        "start": 131.0,
        "end": 136.23,
        "text": " and this is the vector that then feeds into the transformer and that's how the transformer"
    },
    {
        "id": 22,
        "start": 136.23,
        "end": 143.11,
        "text": " serves perceives every single token. So here we had a very naive tokenization process that was a"
    },
    {
        "id": 23,
        "start": 143.11,
        "end": 148.59,
        "text": " character level tokenizer but in practice instead of the art language models people use a lot more"
    },
    {
        "id": 24,
        "start": 148.59,
        "end": 156.05,
        "text": " complicated schemes unfortunately for constructing these token vocabularies. So we're not dealing"
    },
    {
        "id": 25,
        "start": 156.05,
        "end": 162.64,
        "text": " on the character level we're dealing on chunk level and the way these character chunks are constructed"
    },
    {
        "id": 26,
        "start": 163.08,
        "end": 167.44,
        "text": " is using algorithms such as for example the byte pair encoding algorithm which we're going to go"
    },
    {
        "id": 27,
        "start": 167.44,
        "end": 174.6,
        "text": " into in detail and cover in this video. I'd like to briefly show you the paper that introduced a"
    },
    {
        "id": 28,
        "start": 174.76,
        "end": 180.18,
        "text": " byte level encoding as a mechanism for tokenization in the context of large language models and I would"
    },
    {
        "id": 29,
        "start": 180.18,
        "end": 186.46,
        "text": " say that that's probably the GPT2 paper and if you scroll down here to the section input representation"
    },
    {
        "id": 30,
        "start": 186.7,
        "end": 191.6,
        "text": " this is where they cover tokenization because of properties that you like the tokenization to have"
    },
    {
        "id": 31,
        "start": 192.42,
        "end": 197.48,
        "text": " and they conclude here that they're going to have a tokenizer where you have a vocabulary of 50,"
    },
    {
        "id": 32,
        "start": 197.76,
        "end": 208.18,
        "text": " 1250, 57 possible tokens and the context size is going to be 1024 tokens. So in the attention layer"
    },
    {
        "id": 33,
        "start": 208.22,
        "end": 213.5,
        "text": " of the transformer neural network every single token is attending to the previous tokens in the sequence"
    },
    {
        "id": 34,
        "start": 213.52,
        "end": 221.52,
        "text": " and it's going to see up to 1024 tokens. So tokens are this like fundamentally unit the atom of"
    },
    {
        "id": 35,
        "start": 221.58,
        "end": 226.52,
        "text": " large language models if you will and everything is in units of tokens everything is about tokens"
    },
    {
        "id": 36,
        "start": 227.1,
        "end": 232.84,
        "text": " and tokenization is the process for translating strings or text into sequences of tokens and"
    },
    {
        "id": 37,
        "start": 233.58,
        "end": 238.62,
        "text": " vice versa. When you go into the Lama2 paper as well I can show you that when you search token"
    },
    {
        "id": 38,
        "start": 239.08,
        "end": 245.04,
        "text": " you're going to get 63 hits and that's because tokens are again pervasive so here they mentioned"
    },
    {
        "id": 39,
        "start": 245.06,
        "end": 251.72,
        "text": " that they trained on two trillion tokens of data and so on. So we're going to build our own tokenizer"
    },
    {
        "id": 40,
        "start": 252.0,
        "end": 256.8,
        "text": " luckily the byte pairing coding algorithm is not that super complicated and we can build it"
    },
    {
        "id": 41,
        "start": 256.98,
        "end": 261.14,
        "text": " from scratch ourselves and we'll see exactly how this works. Before we dive into code I'd like to"
    },
    {
        "id": 42,
        "start": 261.14,
        "end": 266.05,
        "text": " give you a brief taste of some of the complexities that come from the tokenization because I just want"
    },
    {
        "id": 43,
        "start": 266.05,
        "end": 270.64,
        "text": " to make sure that we motivate it sufficiently for why we are doing all of this and why this is so gross."
    },
    {
        "id": 44,
        "start": 271.8,
        "end": 276.64,
        "text": " So tokenization is at the heart of a lot of weirdness in large language models and I would advise"
    },
    {
        "id": 45,
        "start": 276.64,
        "end": 282.12,
        "text": " that you do not brush it off. A lot of the issues that may look like just issues with the neural"
    },
    {
        "id": 46,
        "start": 282.16,
        "end": 287.36,
        "text": " work architecture or the large language model itself are actually issues with the tokenization"
    },
    {
        "id": 47,
        "start": 287.36,
        "end": 293.78,
        "text": " and fundamentally trace back to it. So if you've noticed any issues with large language models can"
    },
    {
        "id": 48,
        "start": 294.64,
        "end": 300.05,
        "text": " not able to do spelling tasks very easily that's usually due to tokenization. Simple string processing"
    },
    {
        "id": 49,
        "start": 300.05,
        "end": 306.14,
        "text": " can be difficult for the large language model to perform natively. Non-English languages can"
    },
    {
        "id": 50,
        "start": 306.16,
        "end": 311.77,
        "text": " work much worse and to a large extent this is due to tokenization. Sometimes LLMs are bad at"
    },
    {
        "id": 51,
        "start": 311.77,
        "end": 318.11,
        "text": " simple arithmetic also can trace be traced to tokenization. GPT2 specifically would have had"
    },
    {
        "id": 52,
        "start": 318.11,
        "end": 324.24,
        "text": " quite a bit more issues with Python than future versions of it due to tokenization. There's a lot"
    },
    {
        "id": 53,
        "start": 324.24,
        "end": 328.04,
        "text": " of other issues and maybe you've seen weird warnings about a trade-line white space. This is a tokenization"
    },
    {
        "id": 54,
        "start": 328.1,
        "end": 335.64,
        "text": " issue. If you had asked GPT earlier about solid gold magic harp in what it is you would see the"
    },
    {
        "id": 55,
        "start": 335.64,
        "end": 340.84,
        "text": " LLM go totally crazy and it would start going off about the completely unrelated tangent topic."
    },
    {
        "id": 56,
        "start": 341.72,
        "end": 345.68,
        "text": " Maybe you've been told to use the ammo over JSON and structure data. All of that has to do with"
    },
    {
        "id": 57,
        "start": 345.68,
        "end": 351.69,
        "text": " tokenization. So basically tokenization is at the heart of many issues. I will loop back around to"
    },
    {
        "id": 58,
        "start": 351.69,
        "end": 357.86,
        "text": " these at the end of the video but for now let me just skip over a little bit and let's go to this"
    },
    {
        "id": 59,
        "start": 357.96,
        "end": 364.84,
        "text": " web app the tick tokenizer.versal.app. So I have it loaded here and what I like about this web app is"
    },
    {
        "id": 60,
        "start": 364.86,
        "end": 370.38,
        "text": " that tokenization is running a sort of live in your browser in JavaScript. So you can just type"
    },
    {
        "id": 61,
        "start": 370.38,
        "end": 379.02,
        "text": " here stuff hello world and the whole string retokenizes. So here what we see on the left is a"
    },
    {
        "id": 62,
        "start": 379.02,
        "end": 383.95,
        "text": " string that you put in. On the right we're currently using the GPT2 tokenizer. We see that this"
    },
    {
        "id": 63,
        "start": 383.95,
        "end": 390.92,
        "text": " string that I pasted here is currently tokenizing into 300 tokens and here they are sort of shown"
    },
    {
        "id": 64,
        "start": 391.28,
        "end": 397.0,
        "text": " explicitly in different colors for every single token. So for example this word tokenization"
    },
    {
        "id": 65,
        "start": 397.34,
        "end": 411.1,
        "text": " became two tokens. The token 30,642 and 1,634. The token space is token 318. So be careful on the"
    },
    {
        "id": 66,
        "start": 411.1,
        "end": 417.36,
        "text": " bottom you can show white space and keep in mind that there are spaces and slash and new line"
    },
    {
        "id": 67,
        "start": 417.44,
        "end": 427.49,
        "text": " characters in here but you can hide them for clarity. The token space at is token 379. The token space"
    },
    {
        "id": 68,
        "start": 427.49,
        "end": 437.88,
        "text": " the is 262 etc. So you notice here that the space is part of that token chunk. Now so this is"
    },
    {
        "id": 69,
        "start": 437.88,
        "end": 444.44,
        "text": " kind of like how our English sentence broke up and that seems all along good. Now here I put in"
    },
    {
        "id": 70,
        "start": 444.48,
        "end": 453.44,
        "text": " some arithmetic. So we see that the token 127 plus and then token 6, space 6 followed by 77."
    },
    {
        "id": 71,
        "start": 453.52,
        "end": 458.74,
        "text": " So it's happening here that 127 is feeding in as a single token into the large language model"
    },
    {
        "id": 72,
        "start": 459.48,
        "end": 466.98,
        "text": " but the number 677 will actually feed in as two separate tokens. And so the large language model"
    },
    {
        "id": 73,
        "start": 467.74,
        "end": 475.28,
        "text": " has to sort of take account of that and process it correctly in its network and see here 804"
    },
    {
        "id": 74,
        "start": 475.28,
        "end": 479.68,
        "text": " will be broken up into two tokens and it is all completely arbitrary. And here I have another"
    },
    {
        "id": 75,
        "start": 479.68,
        "end": 484.29,
        "text": " example of four digit numbers and they break up in a way that they break up and it's totally"
    },
    {
        "id": 76,
        "start": 484.29,
        "end": 489.4,
        "text": " arbitrary. Sometimes you have multiple digits, single token. Sometimes you have individual"
    },
    {
        "id": 77,
        "start": 489.42,
        "end": 493.86,
        "text": " digits as many tokens and it's all kind of pretty arbitrary and coming out of the tokenizer."
    },
    {
        "id": 78,
        "start": 494.14,
        "end": 502.18,
        "text": " Here's another example. We have the string egg and you see here that this became two tokens."
    },
    {
        "id": 79,
        "start": 503.46,
        "end": 509.51,
        "text": " But for some reason when I say I have an egg you see when it's a space egg it's two token."
    },
    {
        "id": 80,
        "start": 509.51,
        "end": 514.76,
        "text": " It's sorry it's a single token. So just egg by itself in the beginning of the sentence is two"
    },
    {
        "id": 81,
        "start": 514.76,
        "end": 523.32,
        "text": " tokens but here as a space egg is suddenly a single token for the exact same string. Okay. Here"
    },
    {
        "id": 82,
        "start": 523.6,
        "end": 527.77,
        "text": " lowercase egg turns out to be a single token and in particular notice that the color is different"
    },
    {
        "id": 83,
        "start": 527.77,
        "end": 534.2,
        "text": " so this is a different token so this is case sensitive. And of course a capital egg would also be"
    },
    {
        "id": 84,
        "start": 534.2,
        "end": 541.08,
        "text": " different tokens and again this would be two tokens arbitrarily. So for the same concept egg"
    },
    {
        "id": 85,
        "start": 541.1,
        "end": 545.51,
        "text": " depending on if it's in the beginning of a sentence at the end of a sentence lowercase upper case"
    },
    {
        "id": 86,
        "start": 545.51,
        "end": 551.04,
        "text": " or mixed all this will be basically very different tokens and different IDs and the language model"
    },
    {
        "id": 87,
        "start": 551.12,
        "end": 554.62,
        "text": " has to learn from raw data from all the internet text that it's going to be training on"
    },
    {
        "id": 88,
        "start": 554.62,
        "end": 559.21,
        "text": " that these are actually all the exact same concept and it has to sort of group them in the"
    },
    {
        "id": 89,
        "start": 559.21,
        "end": 563.84,
        "text": " parameters of the neural network and understand just based on the data patterns that these are all"
    },
    {
        "id": 90,
        "start": 563.84,
        "end": 571.74,
        "text": " very similar but maybe not almost exactly similar but very very similar. After the administration"
    },
    {
        "id": 91,
        "start": 572.12,
        "end": 582.86,
        "text": " I have an introduction from OpenAI's ChatchyPity in Korean so Manaso, Pangaeo, etc. So this is in Korean"
    },
    {
        "id": 92,
        "start": 583.38,
        "end": 590.97,
        "text": " and the reason I put this here is because you'll notice that non-English languages work"
    },
    {
        "id": 93,
        "start": 590.97,
        "end": 595.85,
        "text": " slightly worse in ChatchyPity. Part of this is because of course the training data set for"
    },
    {
        "id": 94,
        "start": 595.85,
        "end": 601.04,
        "text": " ChatchyPity is much larger for English than for everything else but the same is true not just for"
    },
    {
        "id": 95,
        "start": 601.16,
        "end": 606.03,
        "text": " the large language model itself but also for the tokenizer. So when we train the tokenizer we're"
    },
    {
        "id": 96,
        "start": 606.03,
        "end": 610.06,
        "text": " going to see that there's a training set as well and there's a lot more English than non-English"
    },
    {
        "id": 97,
        "start": 610.78,
        "end": 616.24,
        "text": " and what ends up happening is that we're going to have a lot more longer tokens for English."
    },
    {
        "id": 98,
        "start": 617.64,
        "end": 622.94,
        "text": " So how do I put this? If you have a single sentence in English and you tokenize it you might see"
    },
    {
        "id": 99,
        "start": 622.94,
        "end": 627.6,
        "text": " that it's 10 tokens or something like that but if you translate that sentence into say Korean or"
    },
    {
        "id": 100,
        "start": 627.92,
        "end": 632.7,
        "text": " Japanese or something else you'll typically see that number of tokens used is much larger and"
    },
    {
        "id": 101,
        "start": 632.7,
        "end": 638.46,
        "text": " that's because the chunks here are a lot more broken up. So we're using a lot more tokens for the"
    },
    {
        "id": 102,
        "start": 638.46,
        "end": 645.2,
        "text": " exact same thing and what this does is it bloats up the sequence length of all the documents. So"
    },
    {
        "id": 103,
        "start": 645.2,
        "end": 649.58,
        "text": " you're using up more tokens and then in the attention of the transformer when these tokens"
    },
    {
        "id": 104,
        "start": 649.58,
        "end": 655.46,
        "text": " try to attend each other you are running out of context in the maximum context length of that"
    },
    {
        "id": 105,
        "start": 655.46,
        "end": 662.47,
        "text": " transformer and so basically all the non-English text is stretched out from the perspective of the"
    },
    {
        "id": 106,
        "start": 662.47,
        "end": 668.19,
        "text": " transformer and this just has to do with the trainings accused for the tokenizer and the tokenization"
    },
    {
        "id": 107,
        "start": 668.19,
        "end": 674.19,
        "text": " itself. So it will create a lot bigger tokens and a lot larger groups in English and it will have"
    },
    {
        "id": 108,
        "start": 674.19,
        "end": 681.32,
        "text": " a lot of little boundaries for all the other non-English text. So if we translated this into English"
    },
    {
        "id": 109,
        "start": 681.66,
        "end": 687.03,
        "text": " it would be significantly fewer tokens. The final example I have here is a little snippet of"
    },
    {
        "id": 110,
        "start": 687.03,
        "end": 693.73,
        "text": " Python for doing Fisbus and what I'd like you to notice is look all these individual spaces are"
    },
    {
        "id": 111,
        "start": 693.73,
        "end": 703.98,
        "text": " all separate tokens they are token 220 so 220 220 220 220 220 and then space if is a single token"
    },
    {
        "id": 112,
        "start": 704.86,
        "end": 709.0,
        "text": " and so what's going on here is that when the transformer is going to consume or try to create"
    },
    {
        "id": 113,
        "start": 709.38,
        "end": 716.24,
        "text": " this text it needs to handle all these spaces individually they all feed in one by one into the"
    },
    {
        "id": 114,
        "start": 716.24,
        "end": 722.16,
        "text": " entire transformer in the sequence and so this is being extremely wasteful tokenizing it in this way"
    },
    {
        "id": 115,
        "start": 723.02,
        "end": 728.34,
        "text": " and so as a result of that GPT-2 is not very good with Python and it's not anything to do with"
    },
    {
        "id": 116,
        "start": 728.34,
        "end": 732.84,
        "text": " coding or the language model itself it's just that if you use a lot of indentation using space in"
    },
    {
        "id": 117,
        "start": 732.84,
        "end": 738.94,
        "text": " Python like we usually do you just end up bloating out all the text and it's separated across"
    },
    {
        "id": 118,
        "start": 739.14,
        "end": 743.91,
        "text": " way too much of the sequence and we are running out of the context length in the sequence that's"
    },
    {
        "id": 119,
        "start": 743.91,
        "end": 748.2,
        "text": " roughly speaking what's happening we're being way too wasteful we're taking up way too much token space"
    },
    {
        "id": 120,
        "start": 748.74,
        "end": 753.58,
        "text": " now we can also scroll up here and we can change the tokenizer so note here that GPT-2 tokenizer"
    },
    {
        "id": 121,
        "start": 753.78,
        "end": 760.29,
        "text": " creates a token count of 300 for this string here we can change it to CL100K base which is the GPT"
    },
    {
        "id": 122,
        "start": 760.29,
        "end": 766.7,
        "text": " for tokenizer and we see that the token count drops to 185 so for the exact same string we are now"
    },
    {
        "id": 123,
        "start": 766.96,
        "end": 772.86,
        "text": " roughly having the number of tokens and roughly speaking this is because the number of tokens"
    },
    {
        "id": 124,
        "start": 773.08,
        "end": 778.26,
        "text": " in the GPT-4 tokenizer is roughly double that of the number of tokens in the GPT-2 tokenizer"
    },
    {
        "id": 125,
        "start": 778.68,
        "end": 784.18,
        "text": " so we went from roughly 50k to roughly 100k now you can imagine that this is a good thing because"
    },
    {
        "id": 126,
        "start": 784.18,
        "end": 792.1,
        "text": " the same text is now squished into half as many tokens so this is a lot denser input to the"
    },
    {
        "id": 127,
        "start": 792.1,
        "end": 797.94,
        "text": " transformer and in the transformer every single token has a finite number of tokens before it"
    },
    {
        "id": 128,
        "start": 797.94,
        "end": 803.71,
        "text": " is going to pay attention to and so what this is doing is we're roughly able to see twice as much"
    },
    {
        "id": 129,
        "start": 803.71,
        "end": 810.12,
        "text": " text as a context for what token to predict next because of this change but of course just"
    },
    {
        "id": 130,
        "start": 810.12,
        "end": 815.45,
        "text": " increasing the number of tokens is not strictly better infinitely because as you increase the number"
    },
    {
        "id": 131,
        "start": 815.45,
        "end": 821.31,
        "text": " of tokens now your embedding table is sort of getting a lot larger and also at the output we are"
    },
    {
        "id": 132,
        "start": 821.31,
        "end": 825.56,
        "text": " trying to predict next token and there's the softmax there and that grows as well we're going to"
    },
    {
        "id": 133,
        "start": 825.56,
        "end": 830.49,
        "text": " go into more detail later on this but there's some kind of a sweet spot somewhere where you have a"
    },
    {
        "id": 134,
        "start": 830.49,
        "end": 835.65,
        "text": " just right number of tokens in your vocabulary where everything is appropriately dense and still"
    },
    {
        "id": 135,
        "start": 835.65,
        "end": 841.6,
        "text": " fairly efficient now one thing I would like you to note specifically for the GPT-4 tokenizer is that"
    },
    {
        "id": 136,
        "start": 842.7,
        "end": 848.4,
        "text": " the handling of the white space for Python has improved a lot you see that here these four spaces"
    },
    {
        "id": 137,
        "start": 848.54,
        "end": 854.08,
        "text": " are represented as one single token for the three spaces here and then the token space if"
    },
    {
        "id": 138,
        "start": 854.86,
        "end": 859.85,
        "text": " and here seven spaces were all grouped into a single token so we're being a lot more efficient"
    },
    {
        "id": 139,
        "start": 859.85,
        "end": 864.3,
        "text": " in how we represent Python and this was a deliberate choice made by opening i when they designed"
    },
    {
        "id": 140,
        "start": 864.4,
        "end": 871.14,
        "text": " the GPT-4 tokenizer and they group a lot more white space into a single character what this does"
    },
    {
        "id": 141,
        "start": 871.2,
        "end": 878.5,
        "text": " is this densifies Python and therefore we can attend to more code before it when we're trying to"
    },
    {
        "id": 142,
        "start": 878.5,
        "end": 884.32,
        "text": " predict the next token in the sequence and so the improvement in the Python coding ability from GPT-2"
    },
    {
        "id": 143,
        "start": 884.32,
        "end": 889.43,
        "text": " to GPT-4 is not just a matter of the language model and the architecture and details of the"
    },
    {
        "id": 144,
        "start": 889.43,
        "end": 893.52,
        "text": " optimization but a lot of the improvement here is also coming from the design of the tokenizer"
    },
    {
        "id": 145,
        "start": 893.72,
        "end": 898.22,
        "text": " and how it groups characters into tokens okay so let's now start writing some code"
    },
    {
        "id": 146,
        "start": 899.5,
        "end": 905.43,
        "text": " so remember what we want to do we want to take strings and feed them into language models for that"
    },
    {
        "id": 147,
        "start": 905.43,
        "end": 913.07,
        "text": " we need to somehow tokenize strings into some integers in some of fixed vocabulary and then we will"
    },
    {
        "id": 148,
        "start": 913.07,
        "end": 918.03,
        "text": " use those integers to make a look up into a look up table of vectors and feed those vectors into the"
    },
    {
        "id": 149,
        "start": 918.03,
        "end": 923.22,
        "text": " transformer as an input now the reason this gets a little bit tricky of course is that we don't"
    },
    {
        "id": 150,
        "start": 923.22,
        "end": 927.44,
        "text": " just want to support the simple English alphabet we want to support different kinds of languages"
    },
    {
        "id": 151,
        "start": 928.04,
        "end": 933.4,
        "text": " so this is Anjana Seo in Korean which is hello and we also want to support many kinds of special"
    },
    {
        "id": 152,
        "start": 933.42,
        "end": 940.78,
        "text": " characters that we might find on the internet for example emoji so how do we feed this text into"
    },
    {
        "id": 153,
        "start": 941.94,
        "end": 947.16,
        "text": " transformers well how's that what is this text anyway in Python so if you go to the documentation"
    },
    {
        "id": 154,
        "start": 947.34,
        "end": 953.84,
        "text": " of a string in Python you can see that strings are immutable sequences of unicode code points"
    },
    {
        "id": 155,
        "start": 955.04,
        "end": 962.71,
        "text": " okay what are unicode code points we can go to Wikipedia so unicode code points are defined by the"
    },
    {
        "id": 156,
        "start": 962.71,
        "end": 969.01,
        "text": " unicode consortium as part of the unicode standard and what this is really is that it's just a"
    },
    {
        "id": 157,
        "start": 969.01,
        "end": 975.6,
        "text": " definition of roughly 150,000 characters right now and roughly speaking what they look like and what"
    },
    {
        "id": 158,
        "start": 975.6,
        "end": 983.06,
        "text": " integers represent those characters so this is 150,000 characters across 161 scripts as of right now"
    },
    {
        "id": 159,
        "start": 983.68,
        "end": 988.62,
        "text": " so if you scroll down here you can see that the standard is very much alive the latest standard 15.1"
    },
    {
        "id": 160,
        "start": 988.62,
        "end": 996.28,
        "text": " is September 2023 and basically this is just a way to define lots of types of characters"
    },
    {
        "id": 161,
        "start": 998.08,
        "end": 1003.72,
        "text": " like for example all these characters across different scripts so the way we can access the unicode"
    },
    {
        "id": 162,
        "start": 1003.72,
        "end": 1008.52,
        "text": " code point given a single character is by using the odd function in Python so for example I can"
    },
    {
        "id": 163,
        "start": 1008.74,
        "end": 1016.28,
        "text": " pass an order of h and I can see that for the single character h the unicode code point is 104"
    },
    {
        "id": 164,
        "start": 1017.46,
        "end": 1023.94,
        "text": " okay but this can be arbitrarily complicated so we can take for example our emoji here and we can"
    },
    {
        "id": 165,
        "start": 1023.94,
        "end": 1033.66,
        "text": " see that the code point for this one is 128,000 or we can take un and this is 50,000 now keep in"
    },
    {
        "id": 166,
        "start": 1033.68,
        "end": 1039.33,
        "text": " mind you can't plug in strings here because you this doesn't have a single code point it only"
    },
    {
        "id": 167,
        "start": 1039.33,
        "end": 1046.48,
        "text": " takes a single unicode code point character and tells you it's integer so in this way we can look up"
    },
    {
        "id": 168,
        "start": 1047.94,
        "end": 1054.78,
        "text": " all the characters of this specific string and their code points so or the x for x in this string"
    },
    {
        "id": 169,
        "start": 1055.56,
        "end": 1062.76,
        "text": " and we get this encoding here now see here we've already turned the raw code points already have"
    },
    {
        "id": 170,
        "start": 1062.96,
        "end": 1067.98,
        "text": " integers so why can't we simply just use these integers and not have any tokenization at all"
    },
    {
        "id": 171,
        "start": 1067.98,
        "end": 1073.18,
        "text": " why can't we just use this natively as is and just use the code point well one reason for that of"
    },
    {
        "id": 172,
        "start": 1073.18,
        "end": 1078.69,
        "text": " course is that the vocabulary in that case would be quite long so in this case for unicode this is"
    },
    {
        "id": 173,
        "start": 1078.69,
        "end": 1085.5,
        "text": " a vocabulary of 150,000 different code points but more worry than that I think the unicode standard"
    },
    {
        "id": 174,
        "start": 1085.64,
        "end": 1090.38,
        "text": " is very much alive and it keeps changing and so it's not kind of a stable representation"
    },
    {
        "id": 175,
        "start": 1090.54,
        "end": 1095.4,
        "text": " necessarily that we may want to use directly so for those reasons we need something a bit better"
    },
    {
        "id": 176,
        "start": 1095.4,
        "end": 1100.69,
        "text": " so to find something better we turn to encodings so if you go to the Wikipedia page here we see that"
    },
    {
        "id": 177,
        "start": 1100.69,
        "end": 1109.04,
        "text": " the unicode construction defines three types of encodings utf8 utf16 and utf32 these encodings are"
    },
    {
        "id": 178,
        "start": 1109.7,
        "end": 1115.08,
        "text": " the way by which we can take unicode text and translate it into binary data or byte streams"
    },
    {
        "id": 179,
        "start": 1116.22,
        "end": 1122.24,
        "text": " utf8 is by far the most common so this is the utf8 page now this Wikipedia page is actually quite"
    },
    {
        "id": 180,
        "start": 1122.4,
        "end": 1128.13,
        "text": " long but what's important for our purposes is that utf8 takes every single code point and it"
    },
    {
        "id": 181,
        "start": 1128.13,
        "end": 1134.06,
        "text": " translates it to a byte stream and this byte stream is between one to four bytes so it's a variable"
    },
    {
        "id": 182,
        "start": 1134.08,
        "end": 1138.65,
        "text": " length encoding so depending on the unicode point according to the schema you're going to end up"
    },
    {
        "id": 183,
        "start": 1138.65,
        "end": 1148.51,
        "text": " between one to four bytes for each code point on top of that there's utf8 utf16 and utf32 utf32 is nice"
    },
    {
        "id": 184,
        "start": 1148.51,
        "end": 1152.8,
        "text": " because it is fixed length instead of variable length but it has many other downsides as well"
    },
    {
        "id": 185,
        "start": 1153.82,
        "end": 1160.02,
        "text": " so the full kind of spectrum of pros and cons of all these different three encodings are beyond"
    },
    {
        "id": 186,
        "start": 1160.06,
        "end": 1165.5,
        "text": " the scope of this video I just like to point out that I enjoyed this block post and this block post"
    },
    {
        "id": 187,
        "start": 1165.52,
        "end": 1170.02,
        "text": " that the end of it also has a number of references that can be quite useful one of them is"
    },
    {
        "id": 188,
        "start": 1170.84,
        "end": 1177.6,
        "text": " utf8 everywhere manifesto and this manifesto describes the reason why utf8 is significantly preferred"
    },
    {
        "id": 189,
        "start": 1178.1,
        "end": 1185.32,
        "text": " and a lot nicer than the other encodings and why it is used a lot more prominently on the internet"
    },
    {
        "id": 190,
        "start": 1186.64,
        "end": 1191.34,
        "text": " one of the major advantages does just leave you a sense is that utf8 is the only one of these"
    },
    {
        "id": 191,
        "start": 1191.72,
        "end": 1197.62,
        "text": " that is backwards compatible to the much simpler ascii encoding of text but I'm not going to go into"
    },
    {
        "id": 192,
        "start": 1197.62,
        "end": 1203.59,
        "text": " the full detail in this video so suffice to say that we like the utf8 encoding and let's try to"
    },
    {
        "id": 193,
        "start": 1203.59,
        "end": 1210.77,
        "text": " take this string and see what we get if we encode it into utf8 the string class and Python actually"
    },
    {
        "id": 194,
        "start": 1210.77,
        "end": 1216.48,
        "text": " has down encode and you can give it to the encoding which is say utf8 now we get out of this is not"
    },
    {
        "id": 195,
        "start": 1216.62,
        "end": 1222.79,
        "text": " very nice because this is the bytes is a bytes object and it's not very nice in a way that it's"
    },
    {
        "id": 196,
        "start": 1222.79,
        "end": 1228.42,
        "text": " printed so I personally like to take it through a list because then we actually get the raw bytes"
    },
    {
        "id": 197,
        "start": 1228.54,
        "end": 1237.14,
        "text": " of this encoding so this is the raw bytes that represent this string according to the utf8 encoding"
    },
    {
        "id": 198,
        "start": 1238.06,
        "end": 1243.8,
        "text": " we can also look at utf16 we get a slightly different bytes stream and here we start to see one"
    },
    {
        "id": 199,
        "start": 1243.82,
        "end": 1248.82,
        "text": " of the disadvantages of utf16 you see how we have zero something zero something zero something"
    },
    {
        "id": 200,
        "start": 1248.96,
        "end": 1253.74,
        "text": " we're starting to get a sense that this is a bit of a wasteful encoding and indeed for simple"
    },
    {
        "id": 201,
        "start": 1253.74,
        "end": 1259.38,
        "text": " ascii characters or English characters here we just have the structure of zero something zero something"
    },
    {
        "id": 202,
        "start": 1259.44,
        "end": 1266.3,
        "text": " and it's not exactly nice same for utf32 when we expand this we can start to get a sense of"
    },
    {
        "id": 203,
        "start": 1266.3,
        "end": 1271.22,
        "text": " the wastefulness of this encoding for our purposes you see a lot of zeros followed by something"
    },
    {
        "id": 204,
        "start": 1272.46,
        "end": 1280.92,
        "text": " and so this is not desirable so suffice it to say that we would like to stick with utf8 for our purposes"
    },
    {
        "id": 205,
        "start": 1281.9,
        "end": 1288.12,
        "text": " however if we just use utf8 naively these are byte streams so that would imply a vocabulary"
    },
    {
        "id": 206,
        "start": 1288.18,
        "end": 1295.62,
        "text": " length of only 256 possible tokens but this vocabulary resize is very very small well this is"
    },
    {
        "id": 207,
        "start": 1295.62,
        "end": 1301.29,
        "text": " going to do if we just work to use it naively is that all of our text would be stretched out over"
    },
    {
        "id": 208,
        "start": 1301.29,
        "end": 1310.25,
        "text": " very very long sequences of bytes and so what this does is that certainly the embedding table is"
    },
    {
        "id": 209,
        "start": 1310.25,
        "end": 1314.37,
        "text": " going to be tiny and the prediction at the top of the final layer is going to be very tiny but our"
    },
    {
        "id": 210,
        "start": 1314.37,
        "end": 1320.54,
        "text": " sequences are very long and remember that we have pretty finite context length and the attention"
    },
    {
        "id": 211,
        "start": 1320.8,
        "end": 1326.95,
        "text": " that we can support in a transformer for computational reasons and so we only have as much context length"
    },
    {
        "id": 212,
        "start": 1326.95,
        "end": 1331.31,
        "text": " but now we have very very long sequences and this is just inefficient and it's not going to allow"
    },
    {
        "id": 213,
        "start": 1331.31,
        "end": 1337.04,
        "text": " us to attend to sufficiently long text before us for the purposes of the next token prediction task"
    },
    {
        "id": 214,
        "start": 1338.14,
        "end": 1344.62,
        "text": " so we don't want to use the raw bytes of the utf8 encoding we want to be able to support"
    },
    {
        "id": 215,
        "start": 1345.12,
        "end": 1350.51,
        "text": " larger vocabulary size that we can tune as a high parameter but we want to stick with the utf8"
    },
    {
        "id": 216,
        "start": 1350.51,
        "end": 1356.18,
        "text": " encoding of these streams so what do we do well the answer of course is we turn to the byte pair"
    },
    {
        "id": 217,
        "start": 1356.18,
        "end": 1362.36,
        "text": " encoding algorithm which will allow us to compress these byte sequences to a variable amount"
    },
    {
        "id": 218,
        "start": 1362.76,
        "end": 1367.98,
        "text": " so we'll get to that in a bit but I just want to briefly speak to the fact that I would love"
    },
    {
        "id": 219,
        "start": 1367.98,
        "end": 1374.69,
        "text": " nothing more than to be able to feed raw byte sequences into language models in fact there's a"
    },
    {
        "id": 220,
        "start": 1374.69,
        "end": 1379.85,
        "text": " paper about how this could potentially be done from summer last year now the problem is you"
    },
    {
        "id": 221,
        "start": 1379.85,
        "end": 1384.36,
        "text": " actually have to go in and you have to modify the transformer architecture because as I mentioned"
    },
    {
        "id": 222,
        "start": 1384.36,
        "end": 1388.95,
        "text": " you're going to have a problem where the attention will start to become extremely expensive because"
    },
    {
        "id": 223,
        "start": 1388.95,
        "end": 1395.85,
        "text": " the sequences are so long and so in this paper they propose kind of a hierarchical structuring of"
    },
    {
        "id": 224,
        "start": 1395.85,
        "end": 1401.34,
        "text": " the transformer that could allow you to just feed in raw bytes and so at the end they say together"
    },
    {
        "id": 225,
        "start": 1401.36,
        "end": 1405.76,
        "text": " these results establish the viability of tokenization free power aggressive sequence modeling at scale"
    },
    {
        "id": 226,
        "start": 1406.38,
        "end": 1411.72,
        "text": " so tokenization free would indeed be amazing we would just feed byte streams directly into our models"
    },
    {
        "id": 227,
        "start": 1412.22,
        "end": 1417.17,
        "text": " but unfortunately I don't know that this has really been proven out yet by so many groups and"
    },
    {
        "id": 228,
        "start": 1417.17,
        "end": 1421.04,
        "text": " it's efficient scale but something like this at one point would be amazing and I hope someone"
    },
    {
        "id": 229,
        "start": 1421.04,
        "end": 1425.92,
        "text": " comes up with it but for now we have to come back and we can't feed this directly into language"
    },
    {
        "id": 230,
        "start": 1425.92,
        "end": 1430.36,
        "text": " models and we have to compress it using the byte pair encoding algorithm so let's see how that works"
    },
    {
        "id": 231,
        "start": 1430.72,
        "end": 1435.1,
        "text": " so as I mentioned the byte pair encoding algorithm is not all that complicated and the Wikipedia"
    },
    {
        "id": 232,
        "start": 1435.2,
        "end": 1440.03,
        "text": " page is actually quite instructive as far as the basic idea goes we're doing is we have some kind"
    },
    {
        "id": 233,
        "start": 1440.03,
        "end": 1444.82,
        "text": " of the input sequence like for example here we have only four elements in our vocabulary"
    },
    {
        "id": 234,
        "start": 1445.28,
        "end": 1450.08,
        "text": " abc and d and we have a sequence of them so instead of bytes let's say we just had four"
    },
    {
        "id": 235,
        "start": 1450.56,
        "end": 1456.52,
        "text": " a vocab size of four this sequence is too long we'd like to compress it so we do is that we"
    },
    {
        "id": 236,
        "start": 1456.56,
        "end": 1465.64,
        "text": " iteratively find the pair of tokens that occur the most frequently and then once we've identified"
    },
    {
        "id": 237,
        "start": 1465.7,
        "end": 1472.68,
        "text": " that pair we replace that pair with just a single new token that we append to our vocabulary"
    },
    {
        "id": 238,
        "start": 1472.7,
        "end": 1479.32,
        "text": " so for example here the byte pair AA occurs most often so we mint a new token let's call it"
    },
    {
        "id": 239,
        "start": 1479.32,
        "end": 1488.68,
        "text": " capital Z and we replace every single occurs of AA by Z so now we have two Zs here so here we took"
    },
    {
        "id": 240,
        "start": 1488.68,
        "end": 1497.22,
        "text": " a sequence of 11 characters with vocabulary size four and we've converted it to a sequence of only"
    },
    {
        "id": 241,
        "start": 1497.22,
        "end": 1502.97,
        "text": " nine tokens but now with the vocabulary of five because we have a fifth vocabulary element that we"
    },
    {
        "id": 242,
        "start": 1502.97,
        "end": 1510.09,
        "text": " just created and it's Z standing for concatenation of AA and we can again repeat this process so we"
    },
    {
        "id": 243,
        "start": 1510.09,
        "end": 1517.25,
        "text": " again look at this sequence and identify the pair of tokens that are most frequent let's say that"
    },
    {
        "id": 244,
        "start": 1517.25,
        "end": 1524.41,
        "text": " that is now AB well we are going to replace AB with a new token that we mint call Y so Y becomes AB"
    },
    {
        "id": 245,
        "start": 1524.41,
        "end": 1531.68,
        "text": " and then every single occurs AB is now replaced with Y so we end up with this so now we only have"
    },
    {
        "id": 246,
        "start": 1531.86,
        "end": 1538.34,
        "text": " one two three four five six seven characters in our sequence but we have not just"
    },
    {
        "id": 247,
        "start": 1539.76,
        "end": 1546.47,
        "text": " four vocabulary elements or five but now we have six and for the final round we again look through"
    },
    {
        "id": 248,
        "start": 1546.47,
        "end": 1553.53,
        "text": " the sequence find that the phrase ZY or the pair ZY is most common and replace it one more time with"
    },
    {
        "id": 249,
        "start": 1553.53,
        "end": 1560.7,
        "text": " another um character let's say X so X is ZY and we replace all occurs of ZY and we get this"
    },
    {
        "id": 250,
        "start": 1560.74,
        "end": 1566.24,
        "text": " following sequence so basically after we have gone through this process instead of having a"
    },
    {
        "id": 251,
        "start": 1568.6,
        "end": 1578.52,
        "text": " sequence of 11 tokens whether vocabulary length of four we now have a sequence of one two three four"
    },
    {
        "id": 252,
        "start": 1578.52,
        "end": 1586.88,
        "text": " five tokens but our vocabulary length now is seven and so in this way we can iteratively compress our"
    },
    {
        "id": 253,
        "start": 1586.96,
        "end": 1593.02,
        "text": " sequence as we mint new tokens so in the in the exact same way we start we start out with bite"
    },
    {
        "id": 254,
        "start": 1593.36,
        "end": 1599.82,
        "text": " sequences so we have 256 vocabulary size but we're now going to go through these and find the bite"
    },
    {
        "id": 255,
        "start": 1599.88,
        "end": 1604.66,
        "text": " pairs that occur the most and we're going to iteratively start minting new tokens"
    },
    {
        "id": 256,
        "start": 1604.84,
        "end": 1609.64,
        "text": " depending them to our vocabulary and replacing things and in this way we're going to end up with a"
    },
    {
        "id": 257,
        "start": 1609.64,
        "end": 1616.04,
        "text": " compressed training dataset and also an algorithm for taking any arbitrary sequence and encoding it"
    },
    {
        "id": 258,
        "start": 1616.18,
        "end": 1622.06,
        "text": " using this vocabulary and also decoding it back to strings so let's now implement all that"
    },
    {
        "id": 259,
        "start": 1623.0,
        "end": 1628.05,
        "text": " so here's what I did I went to this block post that I enjoyed and I took the first paragraph and"
    },
    {
        "id": 260,
        "start": 1628.05,
        "end": 1636.05,
        "text": " I copy-pasted it here into text so this is one very long line here now to get the tokens as I"
    },
    {
        "id": 261,
        "start": 1636.05,
        "end": 1641.38,
        "text": " mentioned we just take our text and we encoded into UTF8 the tokens here at this point will be a raw"
    },
    {
        "id": 262,
        "start": 1641.62,
        "end": 1647.92,
        "text": " bytes single string of bytes and just so that it's easier to work with instead of just a bytes"
    },
    {
        "id": 263,
        "start": 1648.0,
        "end": 1653.64,
        "text": " object I'm going to convert all those bytes to integers and then create a list of it just so"
    },
    {
        "id": 264,
        "start": 1653.64,
        "end": 1658.14,
        "text": " it's easier for us to manipulate and work with and Python and visualize and here I'm printing all"
    },
    {
        "id": 265,
        "start": 1658.14,
        "end": 1667.62,
        "text": " that so this is the original this is the original paragraph and its length is 533 code points"
    },
    {
        "id": 266,
        "start": 1668.78,
        "end": 1676.4,
        "text": " and then here are the bytes encoded in UTF8 and we see that this has a length of 616 bytes at this"
    },
    {
        "id": 267,
        "start": 1676.4,
        "end": 1682.74,
        "text": " point or 616 tokens and the reason this is more is because a lot of these simple ASCII characters"
    },
    {
        "id": 268,
        "start": 1682.76,
        "end": 1687.42,
        "text": " or simple characters they just become a single byte but a lot of these unit code"
    },
    {
        "id": 269,
        "start": 1687.68,
        "end": 1692.4,
        "text": " more complex characters become multiple bytes up to four and so we are expanding that size"
    },
    {
        "id": 270,
        "start": 1693.86,
        "end": 1697.2,
        "text": " so now what we'd like to do as a first step of the algorithm is we'd like to iterate over here"
    },
    {
        "id": 271,
        "start": 1697.78,
        "end": 1703.32,
        "text": " and find the pair of bytes that occur most frequently because we're then going to merge it"
    },
    {
        "id": 272,
        "start": 1703.94,
        "end": 1708.22,
        "text": " so if you are working along on a notebook on a side then I encourage you to basically click on the"
    },
    {
        "id": 273,
        "start": 1708.22,
        "end": 1712.86,
        "text": " link find this notebook and try to write that function yourself otherwise I'm going to come here"
    },
    {
        "id": 274,
        "start": 1712.86,
        "end": 1717.57,
        "text": " and implement first the function that finds the most common pair okay so here's what I came up with"
    },
    {
        "id": 275,
        "start": 1717.57,
        "end": 1721.86,
        "text": " there are many different ways to implement this but I'm calling the function get stats it expects"
    },
    {
        "id": 276,
        "start": 1721.86,
        "end": 1727.39,
        "text": " a list of integers I'm using a dictionary to keep track of basically the counts and then this is"
    },
    {
        "id": 277,
        "start": 1727.39,
        "end": 1732.78,
        "text": " the pathonic way to iterate consecutive elements of this list which we covered in the previous video"
    },
    {
        "id": 278,
        "start": 1733.8,
        "end": 1739.79,
        "text": " and then here I'm just keeping track of just incrementing by one for all the pairs so if I call"
    },
    {
        "id": 279,
        "start": 1739.79,
        "end": 1746.8,
        "text": " this an older tokens here then the stats comes out here so this is the dictionary the keys are these"
    },
    {
        "id": 280,
        "start": 1747.26,
        "end": 1754.08,
        "text": " tuples of consecutive elements and this is the count so just to print it in a slightly better way"
    },
    {
        "id": 281,
        "start": 1754.84,
        "end": 1761.4,
        "text": " this is one way that I like to do that where you it's a little bit compound here so you can pause"
    },
    {
        "id": 282,
        "start": 1761.46,
        "end": 1768.0,
        "text": " if you like but we iterate all the items the items called on dictionary returns pairs of key value"
    },
    {
        "id": 283,
        "start": 1769.1,
        "end": 1776.8,
        "text": " and instead I create a list here of value key because if it's a value key list then I can call"
    },
    {
        "id": 284,
        "start": 1776.98,
        "end": 1783.89,
        "text": " sort on it and by default Python will use the first element which in this case it will be value"
    },
    {
        "id": 285,
        "start": 1783.89,
        "end": 1789.78,
        "text": " to sort by if it's given tuples and then reverse so it's descending and print that so basically"
    },
    {
        "id": 286,
        "start": 1790.44,
        "end": 1796.86,
        "text": " it looks like 101 comma 32 was the most commonly occurring consecutive pair and it occurred 20 times"
    },
    {
        "id": 287,
        "start": 1797.02,
        "end": 1801.9,
        "text": " we can double check that that makes reasonable sense so if I just search 101 32"
    },
    {
        "id": 288,
        "start": 1803.2,
        "end": 1811.01,
        "text": " then you see that these are the 20 occurrences of that pair and if we'd like to take a look at what"
    },
    {
        "id": 289,
        "start": 1811.01,
        "end": 1817.5,
        "text": " exactly that pair is we can use char which is the opposite of ord in Python so we give it a"
    },
    {
        "id": 290,
        "start": 1818.32,
        "end": 1825.73,
        "text": " unicode code point so 101 and of 32 and we see that this is e and space so basically there's"
    },
    {
        "id": 291,
        "start": 1825.73,
        "end": 1831.82,
        "text": " a lot of e space here meaning that a lot of these words seem to end with e so it hears e space"
    },
    {
        "id": 292,
        "start": 1831.9,
        "end": 1837.13,
        "text": " as an example so there's a lot of that going on here and this is the most common pair so now"
    },
    {
        "id": 293,
        "start": 1837.13,
        "end": 1842.24,
        "text": " that we've identified the most common pair we would like to iterate over the sequence we're going"
    },
    {
        "id": 294,
        "start": 1842.24,
        "end": 1850.09,
        "text": " to mint a new token with the ID of 256 right because these tokens currently go from 0 to 255 so when"
    },
    {
        "id": 295,
        "start": 1850.09,
        "end": 1858.02,
        "text": " we create a new token it will have an ID of 256 and we're going to iterate over this entire list"
    },
    {
        "id": 296,
        "start": 1858.54,
        "end": 1866.87,
        "text": " and every time we see 101 comma 32 we're going to swap that out for 256 so let's implement that now"
    },
    {
        "id": 297,
        "start": 1866.87,
        "end": 1872.62,
        "text": " and feel free to do that yourself as well so first I commented this just so we don't pollute"
    },
    {
        "id": 298,
        "start": 1873.46,
        "end": 1879.88,
        "text": " the notebook too much this is a nice way of in Python obtaining the highest ranking pair"
    },
    {
        "id": 299,
        "start": 1879.88,
        "end": 1887.58,
        "text": " so we're basically calling the max on this dictionary stats and this will return the maximum key"
    },
    {
        "id": 300,
        "start": 1888.72,
        "end": 1894.22,
        "text": " and then the question is how does it rank keys so you can provide it with a function that ranks keys"
    },
    {
        "id": 301,
        "start": 1894.88,
        "end": 1900.26,
        "text": " and that function is just stats that get stat that get would basically return the value"
    },
    {
        "id": 302,
        "start": 1901.22,
        "end": 1906.6,
        "text": " and so we're ranking by the value and getting the maximum key so it's 101 comma 32 as we saw"
    },
    {
        "id": 303,
        "start": 1907.64,
        "end": 1913.26,
        "text": " now to actually merge 101 32 this is the function that I wrote but again there are many different"
    },
    {
        "id": 304,
        "start": 1913.34,
        "end": 1919.04,
        "text": " versions of it so we're going to take a list of IDs and the pair that we want to replace and"
    },
    {
        "id": 305,
        "start": 1919.04,
        "end": 1926.04,
        "text": " that pair will be replaced with the new index ID X so iterating through IDs if we find the pair"
    },
    {
        "id": 306,
        "start": 1926.16,
        "end": 1932.63,
        "text": " swap it out for ID X so we create this new list and then we start at 0 and then we go through this"
    },
    {
        "id": 307,
        "start": 1932.63,
        "end": 1938.52,
        "text": " entire list sequentially from left to right and here we are checking for equality at the current"
    },
    {
        "id": 308,
        "start": 1938.56,
        "end": 1945.82,
        "text": " position with the pair so here we are checking that the pair matches now here's a bit of a tricky"
    },
    {
        "id": 309,
        "start": 1945.84,
        "end": 1950.48,
        "text": " condition that you have to append if you're trying to be careful and that is that you don't want"
    },
    {
        "id": 310,
        "start": 1950.48,
        "end": 1955.93,
        "text": " this here to be out of bounds at the very last position when you're on the right most element of"
    },
    {
        "id": 311,
        "start": 1955.93,
        "end": 1961.06,
        "text": " this list otherwise this would give you an auto-ponsor so we have to make sure that we're not at the"
    },
    {
        "id": 312,
        "start": 1961.14,
        "end": 1969.34,
        "text": " very very last element so this would be false for that so if we find a match we append to this"
    },
    {
        "id": 313,
        "start": 1969.34,
        "end": 1975.42,
        "text": " new list that replacement index and we increment the position by two so we skip over that entire pair"
    },
    {
        "id": 314,
        "start": 1976.52,
        "end": 1982.05,
        "text": " but otherwise if we haven't found a matching pair we just sort of copy over the element of that"
    },
    {
        "id": 315,
        "start": 1982.05,
        "end": 1988.18,
        "text": " position and increment by one and then return this so here's a very small toy example if we have"
    },
    {
        "id": 316,
        "start": 1988.18,
        "end": 1996.86,
        "text": " a list 566-791 and we want to replace the occurrences of 67 with 99 then calling this on that will give us"
    },
    {
        "id": 317,
        "start": 1996.86,
        "end": 2004.18,
        "text": " what we're asking for so here the 67 is replaced with 99 so now I'm going to uncommon this for our"
    },
    {
        "id": 318,
        "start": 2004.18,
        "end": 2011.64,
        "text": " actual use case where we want to take our tokens we want to take the top pair here and replace it"
    },
    {
        "id": 319,
        "start": 2011.64,
        "end": 2021.32,
        "text": " with 256 to get tokens too if we run this we get the following so recall that previously we had a"
    },
    {
        "id": 320,
        "start": 2021.32,
        "end": 2030.12,
        "text": " length 616 in this list and now we have a length 596 right so this decreased by 20 which makes sense"
    },
    {
        "id": 321,
        "start": 2030.12,
        "end": 2036.78,
        "text": " because there are 20 occurrences moreover we can try to find 256 here and we see plenty of occurrences"
    },
    {
        "id": 322,
        "start": 2037.02,
        "end": 2043.3,
        "text": " of it and moreover just double check there should be no occurrences of 10132 so this is the original"
    },
    {
        "id": 323,
        "start": 2043.34,
        "end": 2049.04,
        "text": " array plenty of them and in the second array there are no occurrences of 10132 so we've successfully"
    },
    {
        "id": 324,
        "start": 2049.04,
        "end": 2055.28,
        "text": " merged this single pair and now we just iterate this so we are going to go over the sequence again"
    },
    {
        "id": 325,
        "start": 2055.38,
        "end": 2060.06,
        "text": " find the most common pair and replace it so let me now write a while loop that uses these functions"
    },
    {
        "id": 326,
        "start": 2060.16,
        "end": 2063.46,
        "text": " to do this sort of iteratively and come"
    },
    {
        "id": 327,
        "start": 2063.46,
        "end": 2069.75,
        "text": " times do we do it for? Well, that's totally up to us as a hyperprimer. The more steps we take,"
    },
    {
        "id": 328,
        "start": 2069.75,
        "end": 2075.69,
        "text": " the larger will be our vocabulary and the shorter will be our sequence. And there is some sweet spot"
    },
    {
        "id": 329,
        "start": 2075.69,
        "end": 2081.45,
        "text": " that we usually find works the best in practice. And so this is kind of a hyperprimer and we tune it"
    },
    {
        "id": 330,
        "start": 2081.45,
        "end": 2087.96,
        "text": " and we find good vocabulary sizes. As an example, GPT-4 currently uses roughly 100,000 tokens and"
    },
    {
        "id": 331,
        "start": 2088.62,
        "end": 2092.68,
        "text": " boldpark that those are reasonable numbers currently instead of the artificial and good walls."
    },
    {
        "id": 332,
        "start": 2092.68,
        "end": 2099.51,
        "text": " So let me now write, putting it all together and iterating these steps. Okay, now before we dive"
    },
    {
        "id": 333,
        "start": 2099.51,
        "end": 2104.9900000000002,
        "text": " into the Y loop, I wanted to add one more cell here where I went to the blog post and instead of"
    },
    {
        "id": 334,
        "start": 2104.9900000000002,
        "end": 2109.4700000000003,
        "text": " grabbing just the first paragraph or two, I took the entire blog post and I stretched it out in"
    },
    {
        "id": 335,
        "start": 2109.4700000000003,
        "end": 2113.7400000000002,
        "text": " a single line. And basically just using longer text will allow us to have more representatives"
    },
    {
        "id": 336,
        "start": 2113.94,
        "end": 2119.19,
        "text": " statistics for the bite pairs and we'll just get a more sensible result out of it because it's"
    },
    {
        "id": 337,
        "start": 2119.19,
        "end": 2126.84,
        "text": " longer text. So here we have the raw text. We encode it into bytes using the UTF-8 encoding."
    },
    {
        "id": 338,
        "start": 2127.66,
        "end": 2132.67,
        "text": " And then here, as before, we are just changing it into a list of integers in Python just so it's"
    },
    {
        "id": 339,
        "start": 2132.67,
        "end": 2139.34,
        "text": " easier to work with instead of the raw bytes objects. And then this is the code that I came up with"
    },
    {
        "id": 340,
        "start": 2140.26,
        "end": 2146.34,
        "text": " to actually do the merging in loop. These two functions here are identical to what we had above."
    },
    {
        "id": 341,
        "start": 2146.34,
        "end": 2150.48,
        "text": " I only included them here just so that you have the point of reference here."
    },
    {
        "id": 342,
        "start": 2151.7400000000002,
        "end": 2157.39,
        "text": " So these are identical and then this is the new code that I added. So the first thing we want to do"
    },
    {
        "id": 343,
        "start": 2157.39,
        "end": 2162.77,
        "text": " is we want to decide on a final vocabulary size that we want our tokenizer to have. And as I"
    },
    {
        "id": 344,
        "start": 2162.77,
        "end": 2166.62,
        "text": " mentioned, this is a hyper parameter and you set it in some way depending on your best performance."
    },
    {
        "id": 345,
        "start": 2167.54,
        "end": 2172.8,
        "text": " So let's say for us, we're going to use 276 because that way we're going to be doing exactly 20 merges."
    },
    {
        "id": 346,
        "start": 2174.06,
        "end": 2182.02,
        "text": " And 20 merges because we already have 256 tokens for the raw bytes. And to reach 276, we have to"
    },
    {
        "id": 347,
        "start": 2182.02,
        "end": 2190.2200000000003,
        "text": " do 20 merges to add 20 new tokens. Here, this is one way in Python to just create a copy of the"
    },
    {
        "id": 348,
        "start": 2190.2200000000003,
        "end": 2196.59,
        "text": " list. So I'm taking the tokens list and by wrapping it in the list, Python will construct a new"
    },
    {
        "id": 349,
        "start": 2196.59,
        "end": 2202.02,
        "text": " list of all the individual elements. So this is just the copy operation. Then here, I'm creating a"
    },
    {
        "id": 350,
        "start": 2202.02,
        "end": 2208.12,
        "text": " merges dictionary. So this merges dictionary is going to maintain basically the child one,"
    },
    {
        "id": 351,
        "start": 2208.46,
        "end": 2214.64,
        "text": " child two, mapping to a new token. And so what we're going to be building up here is a binary tree"
    },
    {
        "id": 352,
        "start": 2214.86,
        "end": 2220.26,
        "text": " of merges. But actually, it's not exactly a tree because a tree would have a single root node"
    },
    {
        "id": 353,
        "start": 2220.26,
        "end": 2225.44,
        "text": " with a bunch of leaves. For us, we're starting with the leaves on the bottom which are the individual"
    },
    {
        "id": 354,
        "start": 2225.68,
        "end": 2230.67,
        "text": " bytes. Those are the starting 256 tokens. And then we're starting to like merge two of them at"
    },
    {
        "id": 355,
        "start": 2230.67,
        "end": 2241.12,
        "text": " a time. And so it's not a tree. It's more like a forest as we merge these elements. So for 20 merges,"
    },
    {
        "id": 356,
        "start": 2241.58,
        "end": 2247.25,
        "text": " we're going to find the most commonly occurring pair. We're going to mint a new token integer for"
    },
    {
        "id": 357,
        "start": 2247.25,
        "end": 2252.68,
        "text": " it. So I here will start at zero. So we're going to start at 256. We're going to print that we're"
    },
    {
        "id": 358,
        "start": 2252.7400000000002,
        "end": 2258.08,
        "text": " merging it. And we're going to replace all the occurrences of that pair with the new new"
    },
    {
        "id": 359,
        "start": 2258.08,
        "end": 2265.16,
        "text": " minted token. And we're going to record that this pair of integers merged into this new integer."
    },
    {
        "id": 360,
        "start": 2266.6,
        "end": 2275.4,
        "text": " So running this gives us the following output. So we did 20 merges. And for example, the first"
    },
    {
        "id": 361,
        "start": 2275.86,
        "end": 2283.58,
        "text": " merge was exactly as before the 101 32 tokens merging into a new token 256. Now keep in mind that"
    },
    {
        "id": 362,
        "start": 2283.58,
        "end": 2289.9,
        "text": " the individual tokens 101 and 32 can still occur in the sequence after merging. It's only when they"
    },
    {
        "id": 363,
        "start": 2289.9,
        "end": 2297.12,
        "text": " occur exactly consecutively that that becomes 256 now. And in particular, the other thing to"
    },
    {
        "id": 364,
        "start": 2297.12,
        "end": 2302.38,
        "text": " notice here is that the token 256, which is the newly minted token, is also eligible for merging."
    },
    {
        "id": 365,
        "start": 2302.94,
        "end": 2308.78,
        "text": " So here on the bottom, the 20th merge was a merge of 256 and 259 becoming 275."
    },
    {
        "id": 366,
        "start": 2309.88,
        "end": 2314.55,
        "text": " So every time we replace these tokens, they become eligible for merging in the next round of"
    },
    {
        "id": 367,
        "start": 2314.55,
        "end": 2319.04,
        "text": " the iteration. So that's why we're building up a small sort of binary forest instead of a single"
    },
    {
        "id": 368,
        "start": 2319.04,
        "end": 2324.38,
        "text": " individual tree. One thing we can take a look at as well is we can take a look at the compression"
    },
    {
        "id": 369,
        "start": 2324.44,
        "end": 2330.88,
        "text": " ratio that we've achieved. So in particular, we started off with this tokens list. So we started"
    },
    {
        "id": 370,
        "start": 2330.9,
        "end": 2341.76,
        "text": " off with 24,000 bytes. And after merging 20 times, we now have only 19,000 tokens. And so therefore,"
    },
    {
        "id": 371,
        "start": 2341.78,
        "end": 2347.14,
        "text": " the compression ratio simply just dividing the two is roughly 1.27. So that's the amount of"
    },
    {
        "id": 372,
        "start": 2347.14,
        "end": 2353.6,
        "text": " compression we were able to achieve of this text with only 20 merges. And of course, the more vocabulary"
    },
    {
        "id": 373,
        "start": 2353.64,
        "end": 2357.48,
        "text": " elements you add, the greater the compression ratio here would be."
    },
    {
        "id": 374,
        "start": 2357.82,
        "end": 2366.6800000000003,
        "text": " Finally, so that's kind of like the training of the tokenizer, if you will. Now, one point that I"
    },
    {
        "id": 375,
        "start": 2366.6800000000003,
        "end": 2372.52,
        "text": " wanted to make is that, and maybe this is a diagram that can help kind of illustrate, is that"
    },
    {
        "id": 376,
        "start": 2372.52,
        "end": 2377.44,
        "text": " tokenizer is a completely separate object from the large language model itself. So everything in"
    },
    {
        "id": 377,
        "start": 2377.44,
        "end": 2381.25,
        "text": " this lecture, we're not really touching the LLM itself. We're just training the tokenizer."
    },
    {
        "id": 378,
        "start": 2381.25,
        "end": 2386.54,
        "text": " This is a completely separate pre-processing stage usually. So the tokenizer will have its own"
    },
    {
        "id": 379,
        "start": 2386.74,
        "end": 2391.69,
        "text": " training set, just like a large language model has a potentially different training set. So the"
    },
    {
        "id": 380,
        "start": 2391.69,
        "end": 2396.42,
        "text": " tokenizer is a training set of documents on which you're going to train the tokenizer. And then"
    },
    {
        "id": 381,
        "start": 2397.46,
        "end": 2402.11,
        "text": " we're performing the byte-parent encoding algorithm as we saw above to train the vocabulary of this"
    },
    {
        "id": 382,
        "start": 2402.11,
        "end": 2407.54,
        "text": " tokenizer. So it has its own training set as a pre-processing stage that you would run a single"
    },
    {
        "id": 383,
        "start": 2407.54,
        "end": 2412.98,
        "text": " time in the beginning. And the tokenizer is trained using byte-parent encoding algorithm."
    },
    {
        "id": 384,
        "start": 2413.86,
        "end": 2417.56,
        "text": " Once you have the tokenizer, once it's trained, and you have the vocabulary and you have the"
    },
    {
        "id": 385,
        "start": 2417.56,
        "end": 2425.34,
        "text": " merges, we can do both encoding and decoding. So these two arrows here. So the tokenizer is a"
    },
    {
        "id": 386,
        "start": 2425.34,
        "end": 2431.1,
        "text": " translation layer between raw text, which is, as we saw the sequence of Unicode Code Points,"
    },
    {
        "id": 387,
        "start": 2431.84,
        "end": 2437.1800000000003,
        "text": " it can take raw text and turn it into a token sequence. And vice versa, it can take a token sequence"
    },
    {
        "id": 388,
        "start": 2438.06,
        "end": 2444.98,
        "text": " and translate it back into raw text. So now that we have trained a tokenizer and we have these merges,"
    },
    {
        "id": 389,
        "start": 2445.94,
        "end": 2450.05,
        "text": " we are going to turn to how we can do the encoding and the decoding step. If you give me text,"
    },
    {
        "id": 390,
        "start": 2450.05,
        "end": 2453.4,
        "text": " here are the tokens, and vice versa, if you give me tokens, here's a text."
    },
    {
        "id": 391,
        "start": 2454.28,
        "end": 2458.64,
        "text": " Once we have that, we can translate between these two rows. And then the language model is going"
    },
    {
        "id": 392,
        "start": 2458.64,
        "end": 2464.7200000000003,
        "text": " to be trained as a step two afterwards. And typically in a sort of state-of-the-art application,"
    },
    {
        "id": 393,
        "start": 2465.26,
        "end": 2468.49,
        "text": " you might take all of your training data for the language model and you might run it through"
    },
    {
        "id": 394,
        "start": 2468.49,
        "end": 2473.4300000000003,
        "text": " the tokenizer and sort of translate everything into a massive token sequence. And then you can"
    },
    {
        "id": 395,
        "start": 2473.4300000000003,
        "end": 2478.45,
        "text": " throw away the raw text. You're just love to let the tokens themselves. And those are stored on"
    },
    {
        "id": 396,
        "start": 2478.45,
        "end": 2482.27,
        "text": " disk, and that is what the large language model is actually reading when it's training on them."
    },
    {
        "id": 397,
        "start": 2482.27,
        "end": 2486.44,
        "text": " So this one I'm personally going to take as a single massive preprocessing stage."
    },
    {
        "id": 398,
        "start": 2488.98,
        "end": 2492.36,
        "text": " So yeah, basically, I think the most important thing I want to get across is that this is"
    },
    {
        "id": 399,
        "start": 2492.38,
        "end": 2497.16,
        "text": " a complete separate stage. It usually has its own entire training set. You may want to have those"
    },
    {
        "id": 400,
        "start": 2497.16,
        "end": 2501.15,
        "text": " training sets be different between the tokenizer and the large language model. So for example,"
    },
    {
        "id": 401,
        "start": 2501.15,
        "end": 2505.54,
        "text": " when you're training the tokenizer, as I mentioned, we don't just care about the performance of"
    },
    {
        "id": 402,
        "start": 2505.54,
        "end": 2511.2,
        "text": " English text. We care about multi-money different languages, and we also care about code or not code."
    },
    {
        "id": 403,
        "start": 2511.62,
        "end": 2516.08,
        "text": " So you may want to look into different kinds of mixtures of different kinds of languages and"
    },
    {
        "id": 404,
        "start": 2516.08,
        "end": 2521.58,
        "text": " different amounts of code and things like that, because the amount of different language that you"
    },
    {
        "id": 405,
        "start": 2521.62,
        "end": 2527.66,
        "text": " have in your tokenizer training set will determine how many merges of it there will be. And therefore"
    },
    {
        "id": 406,
        "start": 2527.7,
        "end": 2535.26,
        "text": " that determines the density with which this type of data is sort of has in the token space."
    },
    {
        "id": 407,
        "start": 2536.26,
        "end": 2540.92,
        "text": " And so roughly speaking intuitively, if you add some amount of data, like say you have a ton of"
    },
    {
        "id": 408,
        "start": 2540.92,
        "end": 2545.9300000000003,
        "text": " Japanese data in your tokenizer training set, then that means that more Japanese tokens will get"
    },
    {
        "id": 409,
        "start": 2545.9300000000003,
        "end": 2551.24,
        "text": " merged. And therefore, Japanese will have shorter sequences. And that's going to be beneficial for"
    },
    {
        "id": 410,
        "start": 2551.24,
        "end": 2556.76,
        "text": " the large language model, which has a finite context length on which it can work on in the token space."
    },
    {
        "id": 411,
        "start": 2557.7,
        "end": 2562.51,
        "text": " So hopefully that makes sense. So we're now going to turn to encoding and decoding, now that we"
    },
    {
        "id": 412,
        "start": 2562.51,
        "end": 2567.64,
        "text": " have trained a tokenizer. So we have our merges, and now how do we do encoding and decoding?"
    },
    {
        "id": 413,
        "start": 2568.3,
        "end": 2573.02,
        "text": " Okay, so let's begin with decoding, which is this arrow over here. So given a token sequence,"
    },
    {
        "id": 414,
        "start": 2573.84,
        "end": 2578.14,
        "text": " let's go through the tokenizer to get back a Python string object. So the raw text."
    },
    {
        "id": 415,
        "start": 2579.1800000000003,
        "end": 2583.07,
        "text": " So this is the function that we'd like to implement. We're given the list of integers,"
    },
    {
        "id": 416,
        "start": 2583.07,
        "end": 2587.17,
        "text": " and we want to return a Python string. If you'd like, try to implement this function yourself."
    },
    {
        "id": 417,
        "start": 2587.17,
        "end": 2590.7,
        "text": " It's a fun exercise. Otherwise, I'm going to start a basting in my own solution."
    },
    {
        "id": 418,
        "start": 2592.32,
        "end": 2598.86,
        "text": " So there are many different ways to do it. Here's one way. I will create and kind of pre-processing"
    },
    {
        "id": 419,
        "start": 2598.86,
        "end": 2606.84,
        "text": " variable that I will call vocab. And vocab is a mapping or dictionary in Python from the token"
    },
    {
        "id": 420,
        "start": 2607.76,
        "end": 2615.2200000000003,
        "text": " ID to the bytes object for that token. So we begin with the raw bytes for tokens from 0 to 255."
    },
    {
        "id": 421,
        "start": 2615.98,
        "end": 2622.26,
        "text": " And then we go in order of all the merges, and we sort of populate this vocab list by doing an"
    },
    {
        "id": 422,
        "start": 2622.26,
        "end": 2628.95,
        "text": " addition here. So this is basically the bytes representation of the first child followed by the"
    },
    {
        "id": 423,
        "start": 2628.95,
        "end": 2634.6,
        "text": " second one. And remember, these are bytes objects. So this addition here is an addition of two bytes"
    },
    {
        "id": 424,
        "start": 2634.8,
        "end": 2641.13,
        "text": " objects, just concatenation. So that's what we get here. One tricky thing to be careful with,"
    },
    {
        "id": 425,
        "start": 2641.13,
        "end": 2647.32,
        "text": " by the way, is that I'm iterating a dictionary in Python using a dot items. And it really matters"
    },
    {
        "id": 426,
        "start": 2647.32,
        "end": 2652.74,
        "text": " that this runs in the order in which we insert it items into the merges dictionary."
    },
    {
        "id": 427,
        "start": 2653.2200000000003,
        "end": 2657.94,
        "text": " Luckily, starting with Python 3.7, this is guaranteed to be the case. But before Python 3.7,"
    },
    {
        "id": 428,
        "start": 2658.26,
        "end": 2662.58,
        "text": " this iteration may have been outer order with respect to how we insert it elements into merges,"
    },
    {
        "id": 429,
        "start": 2662.94,
        "end": 2667.1,
        "text": " and this may not have worked. But we are using modern Python, so we're okay."
    },
    {
        "id": 430,
        "start": 2668.52,
        "end": 2673.46,
        "text": " And then here, given the IDs, the first thing we're going to do is get the tokens."
    },
    {
        "id": 431,
        "start": 2676.1800000000003,
        "end": 2681.05,
        "text": " So the way I implemented this here is I'm taking, I'm iterating over all the IDs. I'm using"
    },
    {
        "id": 432,
        "start": 2681.05,
        "end": 2686.64,
        "text": " vocab to look up their bytes. And then here, this is one way in Python to concatenate all these"
    },
    {
        "id": 433,
        "start": 2686.82,
        "end": 2692.36,
        "text": " bytes together to create our tokens. And then these tokens here, at this point, are raw bytes."
    },
    {
        "id": 434,
        "start": 2693.2200000000003,
        "end": 2700.1800000000003,
        "text": " So I have to decode using utf8 now, back into Python strings. So previously, we called that"
    },
    {
        "id": 435,
        "start": 2700.1800000000003,
        "end": 2705.24,
        "text": " encode on a string object to get the bytes. And now we're doing it opposite. We're taking the"
    },
    {
        "id": 436,
        "start": 2705.24,
        "end": 2711.78,
        "text": " bytes and calling a decode on the bytes object to get a string in Python. And then we can return"
    },
    {
        "id": 437,
        "start": 2712.12,
        "end": 2721.61,
        "text": " text. So this is how we can do it. Now, this actually has a issue in the way I implemented it,"
    },
    {
        "id": 438,
        "start": 2721.61,
        "end": 2726.36,
        "text": " and this could actually throw an error. So try to think, figure out why this code could actually"
    },
    {
        "id": 439,
        "start": 2726.36,
        "end": 2734.45,
        "text": " result in an error if we plug in some sequence of IDs that is unlucky. So let me demonstrate the"
    },
    {
        "id": 440,
        "start": 2734.45,
        "end": 2741.32,
        "text": " issue. When I try to decode just something like 97, I am going to get a letter A here, back. So"
    },
    {
        "id": 441,
        "start": 2741.64,
        "end": 2749.51,
        "text": " nothing too crazy happening. But when I try to decode 128 as a single element, the token 128 is"
    },
    {
        "id": 442,
        "start": 2749.51,
        "end": 2758.54,
        "text": " what in string or in Python object, unique code decoder. utf8 can't decode byte 0x80, which is this"
    },
    {
        "id": 443,
        "start": 2758.54,
        "end": 2763.82,
        "text": " in hex, in position zero, invalid start byte. What does that mean? Well, to understand what this"
    },
    {
        "id": 444,
        "start": 2763.82,
        "end": 2769.98,
        "text": " means, we have to go back to our utf8 page that I briefly showed earlier. And this is Wikipedia utf8."
    },
    {
        "id": 445,
        "start": 2770.7,
        "end": 2777.21,
        "text": " And basically, there's a specific schema that utf8 bytes take. So in particular, if you have a"
    },
    {
        "id": 446,
        "start": 2777.21,
        "end": 2782.4700000000003,
        "text": " multi byte object for some of the unique code characters, they have to have this special sort of"
    },
    {
        "id": 447,
        "start": 2782.4700000000003,
        "end": 2789.24,
        "text": " envelope in how the encoding works. And so what's happening here is that, invalid start byte,"
    },
    {
        "id": 448,
        "start": 2789.86,
        "end": 2797.2200000000003,
        "text": " that's because 128, the binary representation of it is one followed by all zeros. So we have one"
    },
    {
        "id": 449,
        "start": 2797.42,
        "end": 2801.9300000000003,
        "text": " and then all zero. And we see here that that doesn't conform to the format because one followed by"
    },
    {
        "id": 450,
        "start": 2801.9300000000003,
        "end": 2807.92,
        "text": " all zero just doesn't fit any of these rules, so to speak. So it's an invalid start byte, which is"
    },
    {
        "id": 451,
        "start": 2807.92,
        "end": 2814.3,
        "text": " byte one. This one must have a one following it and then a zero following it and then the content"
    },
    {
        "id": 452,
        "start": 2814.36,
        "end": 2820.92,
        "text": " of your unique code in excess here. So basically, we don't exactly follow the utf8 standard and this"
    },
    {
        "id": 453,
        "start": 2820.96,
        "end": 2830.82,
        "text": " cannot be decoded. And so the way to fix this is to use this errors equals in bytes.dcode"
    },
    {
        "id": 454,
        "start": 2830.84,
        "end": 2837.91,
        "text": " function of Python. And by default, errors is strict. So we will throw an error if it's not valid"
    },
    {
        "id": 455,
        "start": 2837.91,
        "end": 2843.08,
        "text": " utf8 bytes encoding. But there are many different things that you could put here on error handling."
    },
    {
        "id": 456,
        "start": 2843.7200000000003,
        "end": 2848.08,
        "text": " This is the full list of all the errors that you can use. And in particular, instead of strict,"
    },
    {
        "id": 457,
        "start": 2848.3,
        "end": 2855.5,
        "text": " let's change it to replace. And that will replace with this special marker. This is the replacement"
    },
    {
        "id": 458,
        "start": 2855.5,
        "end": 2865.55,
        "text": " character. So errors equals replace. And now we just get that character back. So basically not every"
    },
    {
        "id": 459,
        "start": 2865.55,
        "end": 2872.56,
        "text": " single byte sequence is valid utf8. And if it happens that your large language model, for example,"
    },
    {
        "id": 460,
        "start": 2872.66,
        "end": 2880.5,
        "text": " predicts your tokens in a bad manner, then they might not fall into valid utf8. And then we won't"
    },
    {
        "id": 461,
        "start": 2880.5,
        "end": 2887.35,
        "text": " be able to decode them. So the standard practice is to basically use errors equals replace. And this"
    },
    {
        "id": 462,
        "start": 2887.35,
        "end": 2893.07,
        "text": " is what you will also find in the open AI code that they released as well. But basically whenever you"
    },
    {
        "id": 463,
        "start": 2893.07,
        "end": 2898.08,
        "text": " see this kind of a character in your output, in that case, something went wrong and the LM output"
    },
    {
        "id": 464,
        "start": 2898.2200000000003,
        "end": 2905.15,
        "text": " was not valid sort of sequence of tokens. Okay, and now we're going to go the other way. So we are"
    },
    {
        "id": 465,
        "start": 2905.15,
        "end": 2909.67,
        "text": " going to implement this error right here, where we are going to be given a string and we want to"
    },
    {
        "id": 466,
        "start": 2909.67,
        "end": 2915.9,
        "text": " encode it into tokens. So this is just to ensure the function that we are interested in. And"
    },
    {
        "id": 467,
        "start": 2917.0,
        "end": 2922.46,
        "text": " this should basically bring to a list of integers of the tokens. So again, try to maybe implement"
    },
    {
        "id": 468,
        "start": 2922.46,
        "end": 2926.82,
        "text": " this yourself if you'd like a fun exercise and pause here. Otherwise, I'm going to start putting"
    },
    {
        "id": 469,
        "start": 2926.92,
        "end": 2934.58,
        "text": " in my solution. So again, there are many ways to do this. So this is one of the ways that sort of"
    },
    {
        "id": 470,
        "start": 2935.46,
        "end": 2942.8,
        "text": " I came up with. So the first thing we're going to do is we are going to take our checks encoded into"
    },
    {
        "id": 471,
        "start": 2942.8,
        "end": 2948.34,
        "text": " utf8 to get the raw bytes. And then as before, we're going to call list on the bytes object to get a"
    },
    {
        "id": 472,
        "start": 2948.34,
        "end": 2954.46,
        "text": " list of integers of those bytes. So those are the starting tokens. Those are the raw bytes of our"
    },
    {
        "id": 473,
        "start": 2954.5,
        "end": 2960.06,
        "text": " sequence. But now, of course, according to the merges dictionary above and recall, this was the"
    },
    {
        "id": 474,
        "start": 2960.06,
        "end": 2967.3,
        "text": " merges. Some of the bytes may be merged according to this lookup. In addition to that, remember that"
    },
    {
        "id": 475,
        "start": 2967.3,
        "end": 2971.35,
        "text": " the merges was built from top to bottom. And this is sort of the order in which we insert it stuff"
    },
    {
        "id": 476,
        "start": 2971.35,
        "end": 2976.94,
        "text": " into merges. And so we prefer to do all these merges in the beginning before we do these merges"
    },
    {
        "id": 477,
        "start": 2976.94,
        "end": 2983.36,
        "text": " later. Because, for example, this merge over here relies on the 256, which got merged here."
    },
    {
        "id": 478,
        "start": 2984.2,
        "end": 2988.58,
        "text": " So we have to go in the order from top to bottom, sort of, if we are going to be merging anything."
    },
    {
        "id": 479,
        "start": 2990.06,
        "end": 2993.56,
        "text": " Now, we expect to be doing a few merges. So we're going to be doing while true."
    },
    {
        "id": 480,
        "start": 2996.24,
        "end": 3001.96,
        "text": " And now we want to find a pair of bytes that is consecutive that we are allowed to merge according"
    },
    {
        "id": 481,
        "start": 3001.96,
        "end": 3006.92,
        "text": " to this. In order to reuse some of the functionality that we've already written, I'm going to reuse"
    },
    {
        "id": 482,
        "start": 3007.06,
        "end": 3014.7,
        "text": " the function get stats. So recall that get stats will give us the will basically count up how many times"
    },
    {
        "id": 483,
        "start": 3014.76,
        "end": 3021.0,
        "text": " every single pair occurs in our sequence of tokens. And return that as a dictionary. And the dictionary"
    },
    {
        "id": 484,
        "start": 3021.04,
        "end": 3028.16,
        "text": " was a mapping from all the different byte pairs to the number of times that they occur, right?"
    },
    {
        "id": 485,
        "start": 3029.6,
        "end": 3033.6,
        "text": " At this point, we don't actually care how many times they occur in the sequence. We only care"
    },
    {
        "id": 486,
        "start": 3033.94,
        "end": 3038.3,
        "text": " what the raw pairs are in that sequence. And so I'm only going to be using basically the"
    },
    {
        "id": 487,
        "start": 3038.48,
        "end": 3043.4,
        "text": " keys of this dictionary. I only care about the set of possible merge candidates. That makes sense."
    },
    {
        "id": 488,
        "start": 3044.88,
        "end": 3048.54,
        "text": " Now, we want to identify the pair that we're going to be merging at this stage of the loop."
    },
    {
        "id": 489,
        "start": 3049.44,
        "end": 3055.86,
        "text": " So what do we want? We want to find the pair or like the a key inside stats that has the lowest"
    },
    {
        "id": 490,
        "start": 3056.1800000000003,
        "end": 3062.02,
        "text": " index in the merges dictionary. Because we want to do all the early merges before we work our way"
    },
    {
        "id": 491,
        "start": 3062.02,
        "end": 3067.16,
        "text": " to the late merges. So again, there are many different ways to implement this, but I'm going to"
    },
    {
        "id": 492,
        "start": 3067.84,
        "end": 3075.14,
        "text": " do something a little bit fancy here. So I'm going to be using the min over an iterator."
    },
    {
        "id": 493,
        "start": 3076.0,
        "end": 3080.39,
        "text": " In Python, when you call min on an iterator and stats here as a dictionary, we're going to be"
    },
    {
        "id": 494,
        "start": 3080.39,
        "end": 3086.44,
        "text": " iterating the keys of this dictionary in Python. So we're looking at all the pairs inside stats,"
    },
    {
        "id": 495,
        "start": 3086.8,
        "end": 3093.34,
        "text": " which are all the consecutive pairs. And we're going to be taking the consecutive pair inside"
    },
    {
        "id": 496,
        "start": 3093.36,
        "end": 3100.54,
        "text": " tokens that has the minimum, what the min takes a key, which gives us the function that is going"
    },
    {
        "id": 497,
        "start": 3100.54,
        "end": 3106.26,
        "text": " to return a value over which we're going to do the min. And the one we care about is we care about"
    },
    {
        "id": 498,
        "start": 3106.3,
        "end": 3117.36,
        "text": " taking merges and basically getting that pairs index. So basically for any pair inside stats,"
    },
    {
        "id": 499,
        "start": 3118.2799999999997,
        "end": 3124.54,
        "text": " we are going to be looking into merges at what index it has. And we want to get the pair with the min"
    },
    {
        "id": 500,
        "start": 3124.98,
        "end": 3129.8199999999997,
        "text": " number. So as an example, if there's a pair 101 and 32, we definitely want to get that pair."
    },
    {
        "id": 501,
        "start": 3130.5,
        "end": 3135.46,
        "text": " We want to identify it here and return it and pair would become 101 32 if it occurs."
    },
    {
        "id": 502,
        "start": 3136.88,
        "end": 3142.33,
        "text": " And the reason that I'm putting a float in here as a fallback is that in the get function, when we"
    },
    {
        "id": 503,
        "start": 3142.33,
        "end": 3149.2200000000003,
        "text": " call, when we basically consider a pair that doesn't occur in the merges, then that pair is not"
    },
    {
        "id": 504,
        "start": 3149.2200000000003,
        "end": 3154.9700000000003,
        "text": " eligible to be merged. So if in the token sequence, there's some pair that is not a merging pair,"
    },
    {
        "id": 505,
        "start": 3154.9700000000003,
        "end": 3161.04,
        "text": " it cannot be merged. Then it doesn't actually occur here and it doesn't have an index. And it"
    },
    {
        "id": 506,
        "start": 3161.04,
        "end": 3165.86,
        "text": " cannot be merged, which we will denote as float in. And the reason infinity is nice here is because"
    },
    {
        "id": 507,
        "start": 3166.1400000000003,
        "end": 3170.54,
        "text": " for sure, we're guaranteed that it's not going to participate in the list of candidates when we do"
    },
    {
        "id": 508,
        "start": 3170.54,
        "end": 3177.6800000000003,
        "text": " the min. So this is one way to do it. So basically, a long story short, this returns the most eligible"
    },
    {
        "id": 509,
        "start": 3178.48,
        "end": 3185.42,
        "text": " emergent candidate pair that occurs in the tokens. Now, one thing to be careful with here is this"
    },
    {
        "id": 510,
        "start": 3186.12,
        "end": 3193.96,
        "text": " function here might fail in the following way. If there is nothing to merge, then there's nothing"
    },
    {
        "id": 511,
        "start": 3194.12,
        "end": 3200.0,
        "text": " in merges that satisfy that is satisfied anymore. There's nothing to merge. Everything just returns"
    },
    {
        "id": 512,
        "start": 3200.0,
        "end": 3206.12,
        "text": " float in and then the pair, I think, will just become the very first element of stats."
    },
    {
        "id": 513,
        "start": 3207.58,
        "end": 3211.98,
        "text": " But the spare is not actually a mergeable pair. It just becomes the first pair inside stats"
    },
    {
        "id": 514,
        "start": 3211.98,
        "end": 3218.56,
        "text": " arbitrarily because all these pairs evaluate to float in for the merging criterion. So basically,"
    },
    {
        "id": 515,
        "start": 3218.56,
        "end": 3223.6400000000003,
        "text": " it could be that this doesn't succeed because there's no more merging pairs. So if this pair is not"
    },
    {
        "id": 516,
        "start": 3223.7,
        "end": 3228.9,
        "text": " in merges, that was returned. And this is a signal for us that actually there was nothing to merge."
    },
    {
        "id": 517,
        "start": 3229.56,
        "end": 3236.08,
        "text": " No single pair can be merged anymore. In that case, we will break out. Nothing else can be merged."
    },
    {
        "id": 518,
        "start": 3239.06,
        "end": 3242.8900000000003,
        "text": " You may come up with a different implementation, by the way. This is kind of like really trying hard"
    },
    {
        "id": 519,
        "start": 3242.8900000000003,
        "end": 3249.1400000000003,
        "text": " and Python. But really, we're just trying to find a pair that can be merged with a lowest index here."
    },
    {
        "id": 520,
        "start": 3250.6000000000004,
        "end": 3256.94,
        "text": " Now, if we did find a pair that is inside merges with the lowest index, then we can merge it."
    },
    {
        "id": 521,
        "start": 3257.7799999999997,
        "end": 3264.59,
        "text": " So we're going to look into the merges dictionary for that pair to look up the index."
    },
    {
        "id": 522,
        "start": 3264.59,
        "end": 3270.66,
        "text": " And we're going to now merge that into that index. So we're going to do tokens equals and we're"
    },
    {
        "id": 523,
        "start": 3270.66,
        "end": 3277.04,
        "text": " going to replace the original tokens. We're going to be replacing the pair pair and we're going to"
    },
    {
        "id": 524,
        "start": 3277.1400000000003,
        "end": 3282.96,
        "text": " be replacing it with index IDX. And this returns a new list of tokens where every occurrence of pair"
    },
    {
        "id": 525,
        "start": 3283.08,
        "end": 3288.34,
        "text": " is replaced with IDX. So we're doing a merge. And we're going to be continuing this until eventually"
    },
    {
        "id": 526,
        "start": 3288.34,
        "end": 3292.58,
        "text": " nothing can be merged. We'll come out here and we'll break out. And here we just return tokens."
    },
    {
        "id": 527,
        "start": 3294.3199999999997,
        "end": 3298.58,
        "text": " And so that's the implementation, I think. So hopefully this runs. Okay, cool."
    },
    {
        "id": 528,
        "start": 3301.1400000000003,
        "end": 3306.26,
        "text": " Yeah, and this looks reasonable. So for example, 32 is a space in ASCII. So that's here."
    },
    {
        "id": 529,
        "start": 3308.66,
        "end": 3312.8900000000003,
        "text": " So this looks like it worked. Great. Okay, so let's wrap up this section of the video at least."
    },
    {
        "id": 530,
        "start": 3312.8900000000003,
        "end": 3318.2200000000003,
        "text": " I wanted to point out that this is not quite the right implementation just yet because we are leaving out a special case."
    },
    {
        "id": 531,
        "start": 3318.8199999999997,
        "end": 3327.16,
        "text": " So in particular, if we try to do this, this would give us an error. And the issue is that if we only have a single character or an empty string,"
    },
    {
        "id": 532,
        "start": 3327.6400000000003,
        "end": 3335.48,
        "text": " then stats is empty and that causes an issue inside min. So one way to fight this is if a line of tokens is at least two."
    },
    {
        "id": 533,
        "start": 3336.24,
        "end": 3339.46,
        "text": " Because if it's less than two is just a single token or no tokens, then let's just,"
    },
    {
        "id": 534,
        "start": 3340.16,
        "end": 3343.92,
        "text": " there's nothing to merge. So we just return. So that would fix that case."
    },
    {
        "id": 535,
        "start": 3345.56,
        "end": 3351.99,
        "text": " Okay, and then second, I have a few test cases here for us as well. So first, let's make sure about,"
    },
    {
        "id": 536,
        "start": 3351.99,
        "end": 3358.38,
        "text": " or let's note the following. If we take a string and we try to encode it and then decode it back,"
    },
    {
        "id": 537,
        "start": 3358.7799999999997,
        "end": 3362.12,
        "text": " you'd expect to get the same string back, right? Is that true for all strings?"
    },
    {
        "id": 538,
        "start": 3365.84,
        "end": 3369.4,
        "text": " So I think, so here it is the case. And I think in general, this is probably the case."
    },
    {
        "id": 539,
        "start": 3371.26,
        "end": 3376.2799999999997,
        "text": " But notice that going backwards is not, is not, you're not going to have an identity going backwards."
    },
    {
        "id": 540,
        "start": 3376.66,
        "end": 3384.04,
        "text": " Because as I mentioned, not all token sequences are valid, UTF-8, sort of, byte streams."
    },
    {
        "id": 541,
        "start": 3384.8,
        "end": 3387.34,
        "text": " And so therefore, some of them can't even be decodable."
    },
    {
        "id": 542,
        "start": 3388.9,
        "end": 3395.52,
        "text": " So this only goes in one direction. But for that one direction, we can check here. If we take the training text,"
    },
    {
        "id": 543,
        "start": 3395.7799999999997,
        "end": 3399.05,
        "text": " which is the text that we trained the tokenizer on, we can make sure that when we encode in"
    },
    {
        "id": 544,
        "start": 3399.05,
        "end": 3404.01,
        "text": " decode, we get the same thing back, which is true. And here I took some validation data. So I"
    },
    {
        "id": 545,
        "start": 3404.01,
        "end": 3408.63,
        "text": " went to, I think this webpage and I grabbed some text. So this is text that the tokenizer has not"
    },
    {
        "id": 546,
        "start": 3408.63,
        "end": 3414.05,
        "text": " seen. And we can make sure that this also works. Okay, so that gives us some confidence that this"
    },
    {
        "id": 547,
        "start": 3414.05,
        "end": 3420.02,
        "text": " was correctly implemented. So those are the basics of the byte parent coding algorithm. We saw how we"
    },
    {
        "id": 548,
        "start": 3420.84,
        "end": 3426.23,
        "text": " take some training set, train a tokenizer, the parameters of the tokenizer really are just this"
    },
    {
        "id": 549,
        "start": 3426.23,
        "end": 3430.96,
        "text": " dictionary of merges. And that basically creates the little binary forest on top of raw bytes."
    },
    {
        "id": 550,
        "start": 3432.6800000000003,
        "end": 3437.76,
        "text": " Once we have this, the merges table, we can both encode and decode between raw text and token"
    },
    {
        "id": 551,
        "start": 3437.76,
        "end": 3443.33,
        "text": " sequences. So that's the simplest setting of the tokenizer. What we're going to do now, though,"
    },
    {
        "id": 552,
        "start": 3443.33,
        "end": 3447.46,
        "text": " is we're going to look at some of the state-of-the-art launch language models and the kinds of tokenizers"
    },
    {
        "id": 553,
        "start": 3447.46,
        "end": 3451.86,
        "text": " that they use. And we're going to see that this picture complexifies very quickly. So we're"
    },
    {
        "id": 554,
        "start": 3451.86,
        "end": 3457.69,
        "text": " going to go through the details of this complex, complexification right at the time. So let's get"
    },
    {
        "id": 555,
        "start": 3457.69,
        "end": 3462.9,
        "text": " things off by looking at the GPT series. So in particular, I have the GPT2 paper here. And"
    },
    {
        "id": 556,
        "start": 3463.58,
        "end": 3470.62,
        "text": " this paper is from 2019 or so, so five years ago. And let's scroll down to input representation."
    },
    {
        "id": 557,
        "start": 3471.26,
        "end": 3476.1000000000004,
        "text": " This is where they talk about the tokenizer that they're using for GPT2. Now, this is all fairly"
    },
    {
        "id": 558,
        "start": 3476.12,
        "end": 3481.15,
        "text": " readable, so I encourage you to pause and read this yourself. But this is where they motivate the"
    },
    {
        "id": 559,
        "start": 3481.15,
        "end": 3487.8,
        "text": " use of the byte-parent coding algorithm on the byte-level representation of UTF-8 encoding."
    },
    {
        "id": 560,
        "start": 3488.7,
        "end": 3492.24,
        "text": " So this is where they motivate it, and they talk about the vocabulary sizes and everything."
    },
    {
        "id": 561,
        "start": 3493.16,
        "end": 3497.66,
        "text": " Now, everything here is exactly as we've covered it so far, but things start to depart around here."
    },
    {
        "id": 562,
        "start": 3498.74,
        "end": 3502.88,
        "text": " So what we mentioned is that they don't just apply the naive algorithm as we have done it."
    },
    {
        "id": 563,
        "start": 3503.52,
        "end": 3507.74,
        "text": " And in particular, here's a motivating example. So both of you have common words like dog."
    },
    {
        "id": 564,
        "start": 3508.5,
        "end": 3513.75,
        "text": " What will happen is that dog, of course, occurs very frequently in the text, and it occurs right next to"
    },
    {
        "id": 565,
        "start": 3513.75,
        "end": 3519.98,
        "text": " all kinds of punctuation as an example. So doc.doc.exclamation mark.doc.question mark, etc."
    },
    {
        "id": 566,
        "start": 3520.84,
        "end": 3525.26,
        "text": " And naively, you might imagine that the BP algorithm could merge these to be single tokens,"
    },
    {
        "id": 567,
        "start": 3525.38,
        "end": 3529.3199999999997,
        "text": " and then you end up with lots of tokens that are just like dog with a select of different punctuation."
    },
    {
        "id": 568,
        "start": 3530.12,
        "end": 3533.1400000000003,
        "text": " And so it feels like you're clustering things that shouldn't be clustered. You're combining"
    },
    {
        "id": 569,
        "start": 3533.16,
        "end": 3540.49,
        "text": " kind of semantics with punctuation. And this feels suboptimal. And indeed, they also say that this"
    },
    {
        "id": 570,
        "start": 3540.49,
        "end": 3545.1800000000003,
        "text": " is suboptimal, according to some of the experiments. So what they want to do is they want to top down"
    },
    {
        "id": 571,
        "start": 3545.1800000000003,
        "end": 3551.24,
        "text": " in a manual way, in force, that some types of characters should never be merged together."
    },
    {
        "id": 572,
        "start": 3552.76,
        "end": 3556.8,
        "text": " So they want to enforce these merging rules on top of the byte-parent coding algorithm."
    },
    {
        "id": 573,
        "start": 3557.76,
        "end": 3562.06,
        "text": " So let's take a look at their code and see how they actually enforce this and what kinds of"
    },
    {
        "id": 574,
        "start": 3562.06,
        "end": 3568.1000000000004,
        "text": " merging they actually do perform. So I have the top tab open here for GPT2 under open AI,"
    },
    {
        "id": 575,
        "start": 3568.1000000000004,
        "end": 3575.44,
        "text": " on GitHub. And when we go to source, there is an encoder.py. Now I don't personally love that they"
    },
    {
        "id": 576,
        "start": 3575.44,
        "end": 3580.54,
        "text": " call encoder.py because this is the tokenizer. And the tokenizer can do both encode and decode."
    },
    {
        "id": 577,
        "start": 3580.56,
        "end": 3584.6000000000004,
        "text": " So it feels kind of awkward to me that it's called encoder, but that is the tokenizer."
    },
    {
        "id": 578,
        "start": 3585.58,
        "end": 3588.74,
        "text": " And there's a lot going on here, and we're going to step through it in detail at one point."
    },
    {
        "id": 579,
        "start": 3589.3,
        "end": 3595.67,
        "text": " For now, I just want to focus on this part here. They create a regx pattern here that looks very"
    },
    {
        "id": 580,
        "start": 3595.67,
        "end": 3600.38,
        "text": " complicated, and we're going to go through it in a bit. But this is the core part that allows them"
    },
    {
        "id": 581,
        "start": 3600.38,
        "end": 3607.6400000000003,
        "text": " to enforce rules for what parts of the text will never be merged for sure. Now notice that"
    },
    {
        "id": 582,
        "start": 3607.6800000000003,
        "end": 3612.46,
        "text": " re.compile here is a little bit misleading because we're not just doing import re, which is the"
    },
    {
        "id": 583,
        "start": 3612.46,
        "end": 3618.33,
        "text": " Python RE module. We're doing import re x as re. And re x is a Python package that you can install,"
    },
    {
        "id": 584,
        "start": 3618.33,
        "end": 3622.94,
        "text": " pip install re x. And it's basically an extension of re. So it's a bit more powerful re."
    },
    {
        "id": 585,
        "start": 3626.1400000000003,
        "end": 3631.3199999999997,
        "text": " So let's take a look at this pattern and what it's doing and why this is actually doing this"
    },
    {
        "id": 586,
        "start": 3631.3199999999997,
        "end": 3636.3500000000004,
        "text": " separation that they are looking for. Okay, so I've copy-asted the pattern here to our Jupyter"
    },
    {
        "id": 587,
        "start": 3636.3500000000004,
        "end": 3642.04,
        "text": " notebook where we left off. And let's take this pattern for a spin. So in the exact same way that"
    },
    {
        "id": 588,
        "start": 3642.2799999999997,
        "end": 3648.3500000000004,
        "text": " their code does, we're going to call an re.findol for this pattern on any arbitrary string that we"
    },
    {
        "id": 589,
        "start": 3648.3500000000004,
        "end": 3654.7200000000003,
        "text": " are interested in. So this is the string that we want to encode into tokens to feed into an"
    },
    {
        "id": 590,
        "start": 3654.7200000000003,
        "end": 3661.42,
        "text": " lalm like GPT2. So what exactly is this doing? Well, re.findol will take this pattern and try to"
    },
    {
        "id": 591,
        "start": 3661.42,
        "end": 3668.04,
        "text": " match it against this string. The way this works is that you are going from left to right in the"
    },
    {
        "id": 592,
        "start": 3668.1800000000003,
        "end": 3675.2,
        "text": " string and you're trying to match the pattern. And re.findol will get all the occurrences and"
    },
    {
        "id": 593,
        "start": 3675.2,
        "end": 3680.98,
        "text": " organize them into a list. Now, when you look at the, when you look at this pattern, first of all,"
    },
    {
        "id": 594,
        "start": 3680.98,
        "end": 3687.36,
        "text": " notice that this is a raw string. And then these are three double quotes just to start the string."
    },
    {
        "id": 595,
        "start": 3687.86,
        "end": 3694.2799999999997,
        "text": " So really the string itself, this is the pattern itself, right? And notice that it's made up of a lot"
    },
    {
        "id": 596,
        "start": 3694.7799999999997,
        "end": 3701.17,
        "text": " of ors. So see this vertical bars. Those are ors in reg X. And so you go from left to right in this"
    },
    {
        "id": 597,
        "start": 3701.17,
        "end": 3706.8,
        "text": " pattern and try to match it against the string wherever you are. So we have hello and we're going"
    },
    {
        "id": 598,
        "start": 3706.8,
        "end": 3711.52,
        "text": " to try to match it. Well, it's not positive. Yes, it's not positive. If you're T or any of these,"
    },
    {
        "id": 599,
        "start": 3712.62,
        "end": 3720.61,
        "text": " but it is an optional space followed by dash p of sort of slash p of l one or more times. What is"
    },
    {
        "id": 600,
        "start": 3720.61,
        "end": 3728.0,
        "text": " slash p of l? It is coming to some documentation that I found. There might be other sources as well."
    },
    {
        "id": 601,
        "start": 3729.2799999999997,
        "end": 3735.76,
        "text": " slash p of l is a letter, any kind of letter from any language. And hello is made up of letters,"
    },
    {
        "id": 602,
        "start": 3736.16,
        "end": 3743.15,
        "text": " HE, LLO, etc. So optional space followed by a bunch of letters one or more letters is going to"
    },
    {
        "id": 603,
        "start": 3743.15,
        "end": 3750.36,
        "text": " match hello. But then the match ends because a white space is not a letter. So from their own,"
    },
    {
        "id": 604,
        "start": 3751.1400000000003,
        "end": 3757.31,
        "text": " begins a new sort of attempt to match against the string again. And starting in here, we're going"
    },
    {
        "id": 605,
        "start": 3757.31,
        "end": 3762.31,
        "text": " to skip over all these again until we get to the exact same point again. And we see that there's"
    },
    {
        "id": 606,
        "start": 3762.31,
        "end": 3766.92,
        "text": " an optional space. This is the optional space followed by a bunch of letters one or more of them."
    },
    {
        "id": 607,
        "start": 3767.24,
        "end": 3774.06,
        "text": " And so that matches. So when we run this, we get a list of two elements. Hello, and then space"
    },
    {
        "id": 608,
        "start": 3774.16,
        "end": 3781.91,
        "text": " world. So how are you? If we add more letters, we would just get them like this. Now, what is this"
    },
    {
        "id": 609,
        "start": 3781.91,
        "end": 3787.2,
        "text": " doing? And why is this important? We are taking our string. And instead of directly encoding it"
    },
    {
        "id": 610,
        "start": 3788.38,
        "end": 3794.0,
        "text": " for tokenization, we are first splitting it up. And when you actually step through the code,"
    },
    {
        "id": 611,
        "start": 3794.2200000000003,
        "end": 3799.1000000000004,
        "text": " and we'll do that in a bit more detail, what really is doing on a high level is that it first"
    },
    {
        "id": 612,
        "start": 3799.8199999999997,
        "end": 3806.27,
        "text": " splits your text into a list of texts, just like this one. And all these elements of this list"
    },
    {
        "id": 613,
        "start": 3806.27,
        "end": 3811.76,
        "text": " are processed independently by the tokenizer. And all the results of that processing are simply"
    },
    {
        "id": 614,
        "start": 3811.7799999999997,
        "end": 3820.51,
        "text": " concatenated. So hello world. Oh, I missed how? Hello world. How are you? We have five elements of"
    },
    {
        "id": 615,
        "start": 3820.51,
        "end": 3828.4300000000003,
        "text": " a list. All of these will independent independently go from text to a token sequence. And then that"
    },
    {
        "id": 616,
        "start": 3828.4300000000003,
        "end": 3834.23,
        "text": " token sequence is going to be concatenated. It's all going to be joined up. And roughly speaking,"
    },
    {
        "id": 617,
        "start": 3834.23,
        "end": 3839.65,
        "text": " what that does is you're only ever finding merges between the elements of this list. So you can"
    },
    {
        "id": 618,
        "start": 3839.65,
        "end": 3846.38,
        "text": " only ever consider merges within every one of these elements individually. And after you've done"
    },
    {
        "id": 619,
        "start": 3846.42,
        "end": 3850.8500000000004,
        "text": " all the possible merging for all these elements individually, the results of all that will be"
    },
    {
        "id": 620,
        "start": 3850.8500000000004,
        "end": 3858.95,
        "text": " joined by concatenation. And so you are basically what you're doing effectively is you are never"
    },
    {
        "id": 621,
        "start": 3858.95,
        "end": 3864.86,
        "text": " going to be merging this e with this space because they are now parts of the separate elements of"
    },
    {
        "id": 622,
        "start": 3864.86,
        "end": 3872.02,
        "text": " this list. And so you are saying we are never going to merge e space because we're breaking it up"
    },
    {
        "id": 623,
        "start": 3872.08,
        "end": 3879.2200000000003,
        "text": " in this way. So basically using this regEx pattern to chunk up the text is just one way of enforcing"
    },
    {
        "id": 624,
        "start": 3879.2200000000003,
        "end": 3885.16,
        "text": " that some merges are not to happen. And we're going to go into more of this text and we'll see that"
    },
    {
        "id": 625,
        "start": 3885.16,
        "end": 3889.9300000000003,
        "text": " what this is trying to do on a high level is we're trying to not merge across letters across numbers,"
    },
    {
        "id": 626,
        "start": 3889.9300000000003,
        "end": 3895.2799999999997,
        "text": " across punctuation, and so on. So let's see in more detail how that works. So let's continue now."
    },
    {
        "id": 627,
        "start": 3895.98,
        "end": 3902.8199999999997,
        "text": " We have slash p of n if you go to the documentation slash p of n is any kind of numeric"
    },
    {
        "id": 628,
        "start": 3902.8199999999997,
        "end": 3908.21,
        "text": " character in any script. So it's numbers. So we have an optional space followed by numbers and those"
    },
    {
        "id": 629,
        "start": 3908.21,
        "end": 3913.3500000000004,
        "text": " would be separated out. So letters and numbers are being separated. So if I do hello world one to"
    },
    {
        "id": 630,
        "start": 3913.3500000000004,
        "end": 3919.92,
        "text": " three, how are you? Then world will stop matching here because one is not a letter anymore. But one"
    },
    {
        "id": 631,
        "start": 3919.92,
        "end": 3924.12,
        "text": " is a number. So this group will match for that and we'll get it as a separate entity."
    },
    {
        "id": 632,
        "start": 3926.8199999999997,
        "end": 3929.8,
        "text": " Let's see how these apostrophe works. So here if we have"
    },
    {
        "id": 633,
        "start": 3932.74,
        "end": 3939.26,
        "text": " slash v or I mean apostrophe v as an example, then apostrophe here is not a letter or a number."
    },
    {
        "id": 634,
        "start": 3940.6000000000004,
        "end": 3947.36,
        "text": " So hello will stop matching and then we will exactly match this with that. So that will come out"
    },
    {
        "id": 635,
        "start": 3947.42,
        "end": 3952.54,
        "text": " as a separate thing. So why are they doing the apostrophes here? Honestly, I think that these are"
    },
    {
        "id": 636,
        "start": 3952.54,
        "end": 3959.49,
        "text": " just like very common apostrophes that are used typically. I don't love that they've done this"
    },
    {
        "id": 637,
        "start": 3959.49,
        "end": 3966.86,
        "text": " because let me show you what happens when you have some unicode apostrophes. Like for example,"
    },
    {
        "id": 638,
        "start": 3966.86,
        "end": 3972.02,
        "text": " you can have if you have house, then this will be separated out because of this matching."
    },
    {
        "id": 639,
        "start": 3973.16,
        "end": 3980.4,
        "text": " But if you use the unicode apostrophe like this, then suddenly this does not work. And so this"
    },
    {
        "id": 640,
        "start": 3980.4,
        "end": 3986.6800000000003,
        "text": " apostrophe will actually become its own thing now. And so it's basically hard coded for this specific"
    },
    {
        "id": 641,
        "start": 3986.7,
        "end": 3993.96,
        "text": " kind of apostrophe and otherwise they become completely separate tokens. In addition to this,"
    },
    {
        "id": 642,
        "start": 3994.0,
        "end": 4000.66,
        "text": " you can go to the GPT2 docs and here when they define the pattern they say should have added"
    },
    {
        "id": 643,
        "start": 4000.8199999999997,
        "end": 4006.27,
        "text": " RE.ignore case. So BP merges can happen for capitalized versions of contractions. So what they're"
    },
    {
        "id": 644,
        "start": 4006.27,
        "end": 4011.7799999999997,
        "text": " pointing out is that you see how this is a apostrophe and then lowercase letters. Well, because they"
    },
    {
        "id": 645,
        "start": 4011.7799999999997,
        "end": 4018.6800000000003,
        "text": " didn't do RE.ignore case, then these rules will not separate out the apostrophes if it's uppercase."
    },
    {
        "id": 646,
        "start": 4019.9,
        "end": 4030.84,
        "text": " So house would be like this. But if I did house, if I'm uppercase, then notice something the"
    },
    {
        "id": 647,
        "start": 4030.84,
        "end": 4037.2200000000003,
        "text": " apostrophe comes by itself. So the tokenization will work differently in uppercase and lowercase"
    },
    {
        "id": 648,
        "start": 4037.26,
        "end": 4041.92,
        "text": " inconsistently separating out these apostrophes. So it feels extremely gnarly and slightly gross."
    },
    {
        "id": 649,
        "start": 4042.3199999999997,
        "end": 4048.95,
        "text": " But that's how that works. Okay, so let's come back. After trying to match a bunch of apostrophe"
    },
    {
        "id": 650,
        "start": 4048.95,
        "end": 4053.52,
        "text": " expressions, by the way, the other she hears that these are quite language-specific probably. So"
    },
    {
        "id": 651,
        "start": 4054.46,
        "end": 4057.84,
        "text": " I don't know that all the languages, for example, user don't use apostrophes, but that would be"
    },
    {
        "id": 652,
        "start": 4057.84,
        "end": 4063.42,
        "text": " inconsistently tokenized as a result. Then we try to match letters, then we try to match numbers."
    },
    {
        "id": 653,
        "start": 4064.4,
        "end": 4068.56,
        "text": " And then if that doesn't work, we fall back to here. And what this is saying is, again,"
    },
    {
        "id": 654,
        "start": 4068.86,
        "end": 4072.26,
        "text": " optional space, followed by something that is not a letter, number, or a space,"
    },
    {
        "id": 655,
        "start": 4073.62,
        "end": 4078.08,
        "text": " and one or more of that. So what this is doing effectively is this is trying to match punctuation,"
    },
    {
        "id": 656,
        "start": 4078.08,
        "end": 4083.8199999999997,
        "text": " roughly speaking, not letters and not numbers. So this group will try to trigger for that. So if I do"
    },
    {
        "id": 657,
        "start": 4083.8199999999997,
        "end": 4090.2200000000003,
        "text": " something like this, then these parts here are not letters or numbers, but they will actually"
    },
    {
        "id": 658,
        "start": 4091.42,
        "end": 4096.6,
        "text": " they will actually get caught here. And so they become its own group. So we've separated out the"
    },
    {
        "id": 659,
        "start": 4096.6,
        "end": 4103.38,
        "text": " punctuation. And finally, this is also a little bit confusing. So this is matching white space,"
    },
    {
        "id": 660,
        "start": 4104.22,
        "end": 4110.9,
        "text": " but this is using a negative look ahead assertion in reg X. So what this is doing is it's matching"
    },
    {
        "id": 661,
        "start": 4110.9400000000005,
        "end": 4116.76,
        "text": " white space up to, but not including the last white space character. Why is this important?"
    },
    {
        "id": 662,
        "start": 4117.92,
        "end": 4122.2,
        "text": " This is pretty subtle, I think. So you see how the white space is always included at the beginning"
    },
    {
        "id": 663,
        "start": 4122.4400000000005,
        "end": 4126.62,
        "text": " of the word. So space are space you etc."
    },
    {
        "id": 664,
        "start": 4126.62,
        "end": 4127.98,
        "text": " Suppose we have a lot of spaces here."
    },
    {
        "id": 665,
        "start": 4129.78,
        "end": 4134.48,
        "text": " What's going to happen here is that these spaces up to and not including the last character"
    },
    {
        "id": 666,
        "start": 4134.9,
        "end": 4136.34,
        "text": " will get caught by this."
    },
    {
        "id": 667,
        "start": 4137.28,
        "end": 4141.84,
        "text": " And what that will do is it will separate out the spaces up to but not including the last"
    },
    {
        "id": 668,
        "start": 4141.84,
        "end": 4147.0599999999995,
        "text": " character so that the last character can come here and join with the space you."
    },
    {
        "id": 669,
        "start": 4147.88,
        "end": 4151.4,
        "text": " And the reason that's nice is because space you is the common token."
    },
    {
        "id": 670,
        "start": 4151.96,
        "end": 4155.22,
        "text": " So if I didn't have these extra spaces here, you would just have space you."
    },
    {
        "id": 671,
        "start": 4155.76,
        "end": 4160.49,
        "text": " And if I add tokens, if I add spaces, we still have a space you, but now we have all"
    },
    {
        "id": 672,
        "start": 4160.49,
        "end": 4161.5,
        "text": " this extra white space."
    },
    {
        "id": 673,
        "start": 4162.34,
        "end": 4166.28,
        "text": " So basically the GPT2 tokenizer really likes to have a space letter or numbers."
    },
    {
        "id": 674,
        "start": 4167.08,
        "end": 4169.04,
        "text": " And it it pre-pens these spaces."
    },
    {
        "id": 675,
        "start": 4169.58,
        "end": 4171.34,
        "text": " And this is just something that it is consistent about."
    },
    {
        "id": 676,
        "start": 4172.28,
        "end": 4173.16,
        "text": " So that's what that is for."
    },
    {
        "id": 677,
        "start": 4173.72,
        "end": 4178.26,
        "text": " And then finally we have all the the last fallback is white space characters."
    },
    {
        "id": 678,
        "start": 4179.12,
        "end": 4187.72,
        "text": " So that would be just if that doesn't get caught, then this thing will catch any trail"
    },
    {
        "id": 679,
        "start": 4187.72,
        "end": 4188.66,
        "text": " and spaces and so on."
    },
    {
        "id": 680,
        "start": 4189.32,
        "end": 4191.5199999999995,
        "text": " I wanted to show one more real world example here."
    },
    {
        "id": 681,
        "start": 4191.98,
        "end": 4195.61,
        "text": " So if we have this string, which is a piece of Python code, and then we try to split it"
    },
    {
        "id": 682,
        "start": 4195.61,
        "end": 4197.82,
        "text": " up, then this to kind of up that we get."
    },
    {
        "id": 683,
        "start": 4198.74,
        "end": 4200.69,
        "text": " So you'll notice that the list has many elements here."
    },
    {
        "id": 684,
        "start": 4200.69,
        "end": 4206.24,
        "text": " And that's because we are splitting up fairly often every time sort of category changes."
    },
    {
        "id": 685,
        "start": 4207.96,
        "end": 4210.0599999999995,
        "text": " So there will never be any mergers within these elements."
    },
    {
        "id": 686,
        "start": 4211.16,
        "end": 4213.28,
        "text": " And that's what you are seeing here."
    },
    {
        "id": 687,
        "start": 4214.0199999999995,
        "end": 4220.72,
        "text": " Now you might think that in order to train the tokenizer, open AI has used this to split"
    },
    {
        "id": 688,
        "start": 4220.74,
        "end": 4225.26,
        "text": " up text into chunks and then run just a BPE algorithm within all the chunks."
    },
    {
        "id": 689,
        "start": 4226.08,
        "end": 4229.0599999999995,
        "text": " But there's not exactly what happened and the reason is the following."
    },
    {
        "id": 690,
        "start": 4229.639999999999,
        "end": 4231.639999999999,
        "text": " Notice that we have the spaces here."
    },
    {
        "id": 691,
        "start": 4232.44,
        "end": 4234.62,
        "text": " Those spaces end up being entire elements."
    },
    {
        "id": 692,
        "start": 4236.38,
        "end": 4239.5599999999995,
        "text": " But these spaces never actually end up being merged by opening AI."
    },
    {
        "id": 693,
        "start": 4239.88,
        "end": 4243.5199999999995,
        "text": " And the way you can tell is that if you copy-based the exact same chunk here into a Tiktok"
    },
    {
        "id": 694,
        "start": 4244.3,
        "end": 4248.889999999999,
        "text": " and Tiktok and Azure, you see that all the spaces are kept independent and they're all"
    },
    {
        "id": 695,
        "start": 4248.889999999999,
        "end": 4249.82,
        "text": " token to 20."
    },
    {
        "id": 696,
        "start": 4251.179999999999,
        "end": 4255.92,
        "text": " So I think opening at some point and force some rule that these spaces would never be"
    },
    {
        "id": 697,
        "start": 4255.92,
        "end": 4256.3,
        "text": " merged."
    },
    {
        "id": 698,
        "start": 4256.82,
        "end": 4263.11,
        "text": " And so there's some additional rules on top of just chunking and BPE that open AI is not"
    },
    {
        "id": 699,
        "start": 4263.11,
        "end": 4264.0,
        "text": " clear about."
    },
    {
        "id": 700,
        "start": 4264.7,
        "end": 4267.36,
        "text": " Now the training code for the GPT2 tokenizer was never released."
    },
    {
        "id": 701,
        "start": 4267.82,
        "end": 4271.179999999999,
        "text": " So all we have is the code that I've already shown you."
    },
    {
        "id": 702,
        "start": 4271.5199999999995,
        "end": 4275.98,
        "text": " But this code here that they've released is only the inference code for the tokens."
    },
    {
        "id": 703,
        "start": 4276.5199999999995,
        "end": 4277.82,
        "text": " So this is not the training code."
    },
    {
        "id": 704,
        "start": 4277.88,
        "end": 4280.28,
        "text": " You can't give it a piece of text and train tokenizer."
    },
    {
        "id": 705,
        "start": 4280.86,
        "end": 4286.0599999999995,
        "text": " This is just the inference code which takes the mergers that we have up above and applies"
    },
    {
        "id": 706,
        "start": 4286.139999999999,
        "end": 4287.84,
        "text": " them to a new piece of text."
    },
    {
        "id": 707,
        "start": 4288.54,
        "end": 4292.22,
        "text": " And so we don't know exactly how opening AI trained the tokenizer."
    },
    {
        "id": 708,
        "start": 4292.44,
        "end": 4296.66,
        "text": " But it wasn't as simple as chunk it up and BPE it whenever it was."
    },
    {
        "id": 709,
        "start": 4297.5599999999995,
        "end": 4302.44,
        "text": " Next, I wanted to introduce you to the Tiktokin library for open AI, which is the official"
    },
    {
        "id": 710,
        "start": 4302.8,
        "end": 4304.8,
        "text": " library for tokenization from open AI."
    },
    {
        "id": 711,
        "start": 4305.46,
        "end": 4313.0599999999995,
        "text": " So this is Tiktokin, pip install Tiktokin, and then you can do the tokenization inference."
    },
    {
        "id": 712,
        "start": 4313.54,
        "end": 4314.91,
        "text": " This is again not training code."
    },
    {
        "id": 713,
        "start": 4314.91,
        "end": 4316.72,
        "text": " This is only inference code for tokenization."
    },
    {
        "id": 714,
        "start": 4318.48,
        "end": 4322.87,
        "text": " I wanted to show you how you would use it quite simple and running this just gives us"
    },
    {
        "id": 715,
        "start": 4322.87,
        "end": 4325.36,
        "text": " the GPT2 tokens or the GPT4 tokens."
    },
    {
        "id": 716,
        "start": 4325.82,
        "end": 4327.7,
        "text": " So this is the tokenizer you use for GPT4."
    },
    {
        "id": 717,
        "start": 4328.679999999999,
        "end": 4333.12,
        "text": " As we in particular we see that the white space in GPT2 remains unmerged, but in GPT4"
    },
    {
        "id": 718,
        "start": 4333.54,
        "end": 4337.46,
        "text": " these white spaces merge as we also saw in this one."
    },
    {
        "id": 719,
        "start": 4337.66,
        "end": 4342.7,
        "text": " Where here they are all unmerged, but if we go down to GPT4 they become merged."
    },
    {
        "id": 720,
        "start": 4345.599999999999,
        "end": 4352.72,
        "text": " Now in the GPT4 tokenizer they changed the regular expression that they use to chunk"
    },
    {
        "id": 721,
        "start": 4352.74,
        "end": 4353.3,
        "text": " up text."
    },
    {
        "id": 722,
        "start": 4353.98,
        "end": 4359.55,
        "text": " So the way to see this is that if you come to your the Tiktokin library and then you go"
    },
    {
        "id": 723,
        "start": 4359.55,
        "end": 4364.8099999999995,
        "text": " to this file TiktokinX open AI public, this is where sort of like the definition of all"
    },
    {
        "id": 724,
        "start": 4364.8099999999995,
        "end": 4367.32,
        "text": " these different tokenizers that open AI maintains is."
    },
    {
        "id": 725,
        "start": 4367.98,
        "end": 4371.8099999999995,
        "text": " And so necessarily to do the inference they had to publish some of the details about"
    },
    {
        "id": 726,
        "start": 4371.8099999999995,
        "end": 4372.4,
        "text": " the strings."
    },
    {
        "id": 727,
        "start": 4373.48,
        "end": 4375.9,
        "text": " So this is the string that we already saw for GPT2."
    },
    {
        "id": 728,
        "start": 4376.4,
        "end": 4380.8,
        "text": " It is slightly different, but it is actually equivalent to what we discussed here."
    },
    {
        "id": 729,
        "start": 4381.34,
        "end": 4384.54,
        "text": " So this pattern that we discussed is equivalent to this pattern."
    },
    {
        "id": 730,
        "start": 4384.92,
        "end": 4386.74,
        "text": " This one just executes a little bit faster."
    },
    {
        "id": 731,
        "start": 4387.599999999999,
        "end": 4390.59,
        "text": " So here you see a little bit of a slightly different definition, but otherwise it's the"
    },
    {
        "id": 732,
        "start": 4390.59,
        "end": 4390.84,
        "text": " same."
    },
    {
        "id": 733,
        "start": 4391.8,
        "end": 4393.24,
        "text": " We're going to go into special tokens in a bit."
    },
    {
        "id": 734,
        "start": 4394.46,
        "end": 4399.08,
        "text": " And then if you scroll down to CL100K, this is the GPT4 tokenizer."
    },
    {
        "id": 735,
        "start": 4399.719999999999,
        "end": 4401.2,
        "text": " You see that the pattern has changed."
    },
    {
        "id": 736,
        "start": 4402.8,
        "end": 4406.599999999999,
        "text": " And this is kind of like the main the major change in addition to a bunch of other special"
    },
    {
        "id": 737,
        "start": 4406.639999999999,
        "end": 4408.28,
        "text": " tokens, which we'll go into again."
    },
    {
        "id": 738,
        "start": 4409.719999999999,
        "end": 4413.2,
        "text": " Now some I'm not going to actually go into the full detail of the pattern change because"
    },
    {
        "id": 739,
        "start": 4413.9,
        "end": 4415.099999999999,
        "text": " honestly this is a mind numbing."
    },
    {
        "id": 740,
        "start": 4415.24,
        "end": 4420.08,
        "text": " I would just advise that you pull out chat GPT and the reg X documentation and just"
    },
    {
        "id": 741,
        "start": 4420.08,
        "end": 4420.92,
        "text": " step through it."
    },
    {
        "id": 742,
        "start": 4421.48,
        "end": 4424.12,
        "text": " But really the major changes are number one."
    },
    {
        "id": 743,
        "start": 4424.68,
        "end": 4431.5599999999995,
        "text": " You see this I here, that means that the case sensitivity, this is case insensitive match."
    },
    {
        "id": 744,
        "start": 4431.94,
        "end": 4436.96,
        "text": " And so the comment that we saw earlier on, oh, we should have used RE.uppercase."
    },
    {
        "id": 745,
        "start": 4437.5199999999995,
        "end": 4444.61,
        "text": " Basically, we're now going to be matching these apostrophe S, apostrophe D, apostrophe M,"
    },
    {
        "id": 746,
        "start": 4444.61,
        "end": 4445.0599999999995,
        "text": " et cetera."
    },
    {
        "id": 747,
        "start": 4445.66,
        "end": 4448.18,
        "text": " We're going to be matching them both in lower case and in uppercase."
    },
    {
        "id": 748,
        "start": 4448.94,
        "end": 4449.58,
        "text": " So that's fixed."
    },
    {
        "id": 749,
        "start": 4450.34,
        "end": 4453.599999999999,
        "text": " There's a bunch of different handling of the white space that I'm not going to go into"
    },
    {
        "id": 750,
        "start": 4453.599999999999,
        "end": 4454.38,
        "text": " the full details of."
    },
    {
        "id": 751,
        "start": 4455.099999999999,
        "end": 4459.94,
        "text": " And then one more thing here is you will notice that when they match the numbers, they only"
    },
    {
        "id": 752,
        "start": 4459.94,
        "end": 4461.68,
        "text": " match one to three numbers."
    },
    {
        "id": 753,
        "start": 4462.48,
        "end": 4468.7,
        "text": " So they will never merge numbers that are in more than three digits."
    },
    {
        "id": 754,
        "start": 4469.08,
        "end": 4472.34,
        "text": " Only up to three digits of numbers will ever be merged."
    },
    {
        "id": 755,
        "start": 4473.219999999999,
        "end": 4478.2,
        "text": " And that's one change that they made as well to prevent tokens that are very, very long"
    },
    {
        "id": 756,
        "start": 4478.32,
        "end": 4479.0,
        "text": " number sequences."
    },
    {
        "id": 757,
        "start": 4480.599999999999,
        "end": 4484.17,
        "text": " But again, we don't really know why they do any of this stuff because none of this is"
    },
    {
        "id": 758,
        "start": 4484.17,
        "end": 4484.58,
        "text": " documented."
    },
    {
        "id": 759,
        "start": 4485.16,
        "end": 4487.0599999999995,
        "text": " And it's just we just get the pattern."
    },
    {
        "id": 760,
        "start": 4487.48,
        "end": 4490.62,
        "text": " So yeah, it is what it is."
    },
    {
        "id": 761,
        "start": 4490.719999999999,
        "end": 4493.32,
        "text": " But those are some of the changes that GPT-4 has made."
    },
    {
        "id": 762,
        "start": 4493.38,
        "end": 4497.42,
        "text": " And of course, the vocabulary size went from roughly 50k to roughly 100k."
    },
    {
        "id": 763,
        "start": 4498.66,
        "end": 4503.0599999999995,
        "text": " The next thing I would like to do very briefly is to take you through the GPT-2 encoder.py"
    },
    {
        "id": 764,
        "start": 4503.099999999999,
        "end": 4504.24,
        "text": " that opening I has released."
    },
    {
        "id": 765,
        "start": 4505.68,
        "end": 4507.88,
        "text": " This is the file, the alabery machine, to briefly."
    },
    {
        "id": 766,
        "start": 4508.719999999999,
        "end": 4513.99,
        "text": " Now this file is fairly short and should be relatively understandable to you at this"
    },
    {
        "id": 767,
        "start": 4513.99,
        "end": 4514.3,
        "text": " point."
    },
    {
        "id": 768,
        "start": 4516.36,
        "end": 4522.36,
        "text": " Starting at the bottom here, they are loading two files encoder.json and vocab.bpe."
    },
    {
        "id": 769,
        "start": 4523.0,
        "end": 4526.38,
        "text": " And they do some like processing on it and then they call this encoder object, which is"
    },
    {
        "id": 770,
        "start": 4526.38,
        "end": 4527.0,
        "text": " the tokenizer."
    },
    {
        "id": 771,
        "start": 4527.92,
        "end": 4533.0199999999995,
        "text": " Now if you'd like to inspect these two files, which together constitute their saved tokenizer,"
    },
    {
        "id": 772,
        "start": 4533.76,
        "end": 4535.78,
        "text": " then you can do that with a piece of code like this."
    },
    {
        "id": 773,
        "start": 4537.88,
        "end": 4540.5199999999995,
        "text": " This very can download these two files and you can inspect them if you'd like."
    },
    {
        "id": 774,
        "start": 4541.2,
        "end": 4545.719999999999,
        "text": " And what you will find is that this encoder, as they call it in their code, is exactly"
    },
    {
        "id": 775,
        "start": 4545.74,
        "end": 4547.0599999999995,
        "text": " equal to our vocab."
    },
    {
        "id": 776,
        "start": 4548.0,
        "end": 4553.78,
        "text": " So remember here where we have this vocab object, which allowed us to decode very efficiently."
    },
    {
        "id": 777,
        "start": 4554.3,
        "end": 4560.139999999999,
        "text": " And basically it took us from the integer to the bytes for that integer."
    },
    {
        "id": 778,
        "start": 4560.88,
        "end": 4563.54,
        "text": " So our vocab is exactly their encoder."
    },
    {
        "id": 779,
        "start": 4564.5199999999995,
        "end": 4569.58,
        "text": " And then their vocab.bpe, confusingly, is actually our merges."
    },
    {
        "id": 780,
        "start": 4570.32,
        "end": 4576.32,
        "text": " So their ppe merges, which is based on the data inside vocab.bpe, ends up being equivalent"
    },
    {
        "id": 781,
        "start": 4576.34,
        "end": 4577.36,
        "text": " to our merges."
    },
    {
        "id": 782,
        "start": 4578.219999999999,
        "end": 4585.4,
        "text": " So basically they are saving and loading the two variables that for us are also critical."
    },
    {
        "id": 783,
        "start": 4585.62,
        "end": 4587.98,
        "text": " The merges variable and the vocab variable."
    },
    {
        "id": 784,
        "start": 4588.32,
        "end": 4592.7,
        "text": " Using just these two variables, you can represent a tokenizer and you can both do encoding"
    },
    {
        "id": 785,
        "start": 4592.84,
        "end": 4595.2,
        "text": " and decoding once you've trained this tokenizer."
    },
    {
        "id": 786,
        "start": 4596.32,
        "end": 4602.76,
        "text": " Now the only thing that is actually slightly confusing inside what opening I does here"
    },
    {
        "id": 787,
        "start": 4603.16,
        "end": 4606.92,
        "text": " is that in addition to this encoder and the decoder, they also have something called the"
    },
    {
        "id": 788,
        "start": 4607.0199999999995,
        "end": 4608.8,
        "text": " byte encoder and the byte decoder."
    },
    {
        "id": 789,
        "start": 4609.62,
        "end": 4615.12,
        "text": " And this is actually unfortunately just kind of a spurious implementation detail."
    },
    {
        "id": 790,
        "start": 4615.12,
        "end": 4617.469999999999,
        "text": " And it isn't actually deep or interesting in any way."
    },
    {
        "id": 791,
        "start": 4617.469999999999,
        "end": 4619.0599999999995,
        "text": " So I'm going to skip the discussion of it."
    },
    {
        "id": 792,
        "start": 4619.719999999999,
        "end": 4623.69,
        "text": " So what opening I does here for reasons that I don't fully understand is that not only"
    },
    {
        "id": 793,
        "start": 4623.69,
        "end": 4627.42,
        "text": " have they this tokenizer which can encode and decode, but they have a whole separate"
    },
    {
        "id": 794,
        "start": 4627.46,
        "end": 4630.5599999999995,
        "text": " layer here in addition that is used serially with the tokenizer."
    },
    {
        "id": 795,
        "start": 4631.3,
        "end": 4637.5199999999995,
        "text": " And so you first do byte encode and then encode and then you do decode and then byte decode."
    },
    {
        "id": 796,
        "start": 4637.9,
        "end": 4641.36,
        "text": " So that's the loop and they are just stacked serial on top of each other."
    },
    {
        "id": 797,
        "start": 4642.3,
        "end": 4645.8,
        "text": " And it's not that interesting so I won't cover it and you can substitute it if you'd like."
    },
    {
        "id": 798,
        "start": 4646.88,
        "end": 4650.94,
        "text": " Otherwise this file, if you ignore the byte encoder and the byte decoder will be algorithmically"
    },
    {
        "id": 799,
        "start": 4650.94,
        "end": 4651.92,
        "text": " very familiar with you."
    },
    {
        "id": 800,
        "start": 4652.48,
        "end": 4657.58,
        "text": " And the meat of it here is the what they call BPE function and you should recognize this"
    },
    {
        "id": 801,
        "start": 4657.88,
        "end": 4662.61,
        "text": " loop here which is very similar to our own Y loop where they're trying to identify the"
    },
    {
        "id": 802,
        "start": 4662.61,
        "end": 4666.98,
        "text": " by gram a pair that they should be merging next."
    },
    {
        "id": 803,
        "start": 4667.34,
        "end": 4670.719999999999,
        "text": " And then here just like we had, they have a for loop trying to merge this pair."
    },
    {
        "id": 804,
        "start": 4671.48,
        "end": 4675.36,
        "text": " So they will go over all the sequence and they will merge the pair whenever they find"
    },
    {
        "id": 805,
        "start": 4675.36,
        "end": 4675.639999999999,
        "text": " it."
    },
    {
        "id": 806,
        "start": 4676.76,
        "end": 4681.099999999999,
        "text": " And they keep repeating that until they run out of possible merges in the in the text."
    },
    {
        "id": 807,
        "start": 4681.58,
        "end": 4685.79,
        "text": " So that's the meat of this file and there's an encode and decode function just like we"
    },
    {
        "id": 808,
        "start": 4685.79,
        "end": 4686.5,
        "text": " have implemented it."
    },
    {
        "id": 809,
        "start": 4687.2,
        "end": 4690.719999999999,
        "text": " So long story short, what I want you to take away at this point is that unfortunately"
    },
    {
        "id": 810,
        "start": 4690.74,
        "end": 4694.26,
        "text": " it's a little bit of a messy code that they have, but algorithmically it is identical"
    },
    {
        "id": 811,
        "start": 4694.38,
        "end": 4695.84,
        "text": " to what we built up above."
    },
    {
        "id": 812,
        "start": 4696.42,
        "end": 4700.8099999999995,
        "text": " And what we built up above if you understand it is algorithmically what is necessary to"
    },
    {
        "id": 813,
        "start": 4700.8099999999995,
        "end": 4705.16,
        "text": " actually build a BPE tokenizer, train it and then both encode and decode."
    },
    {
        "id": 814,
        "start": 4706.28,
        "end": 4708.719999999999,
        "text": " The next topic I would like to turn to is that of special tokens."
    },
    {
        "id": 815,
        "start": 4709.599999999999,
        "end": 4714.82,
        "text": " So in addition to tokens that are coming from raw bytes and the BPE merges, we can insert"
    },
    {
        "id": 816,
        "start": 4714.86,
        "end": 4718.8,
        "text": " all kinds of tokens that we are going to use to delimit different parts of the data"
    },
    {
        "id": 817,
        "start": 4719.099999999999,
        "end": 4723.08,
        "text": " or introduce to create a special structure of the token streams."
    },
    {
        "id": 818,
        "start": 4724.0199999999995,
        "end": 4730.599999999999,
        "text": " So if you look at this encoder object from OpenAI's GPT2 right here, we mentioned this"
    },
    {
        "id": 819,
        "start": 4730.599999999999,
        "end": 4731.88,
        "text": " is very similar to our vocab."
    },
    {
        "id": 820,
        "start": 4732.68,
        "end": 4736.5599999999995,
        "text": " So notice that the length of this is 50,257."
    },
    {
        "id": 821,
        "start": 4739.28,
        "end": 4743.24,
        "text": " And as I mentioned it's mapping and it's inverted from the mapping of our vocab."
    },
    {
        "id": 822,
        "start": 4743.32,
        "end": 4749.0,
        "text": " Our vocab goes from integer to string and they go the other way around for no amazing"
    },
    {
        "id": 823,
        "start": 4749.0599999999995,
        "end": 4749.34,
        "text": " reason."
    },
    {
        "id": 824,
        "start": 4750.88,
        "end": 4754.7,
        "text": " But the thing to note here is that this, the mapping table here is 50,257."
    },
    {
        "id": 825,
        "start": 4755.34,
        "end": 4756.48,
        "text": " Where does that number come from?"
    },
    {
        "id": 826,
        "start": 4758.9,
        "end": 4759.54,
        "text": " Where are the tokens?"
    },
    {
        "id": 827,
        "start": 4759.54,
        "end": 4767.62,
        "text": " As I mentioned there are 256 raw bytes tokens and then OpenAI actually bit 50,000 merges."
    },
    {
        "id": 828,
        "start": 4768.94,
        "end": 4773.4,
        "text": " So those become the other tokens but this would have been 50,256."
    },
    {
        "id": 829,
        "start": 4774.3,
        "end": 4776.38,
        "text": " So what is the 57th token?"
    },
    {
        "id": 830,
        "start": 4776.88,
        "end": 4778.68,
        "text": " And there is basically one special token."
    },
    {
        "id": 831,
        "start": 4780.86,
        "end": 4784.719999999999,
        "text": " And that one special token you can see is called End of Text."
    },
    {
        "id": 832,
        "start": 4786.0,
        "end": 4788.82,
        "text": " So this is a special token and it's the very last token."
    },
    {
        "id": 833,
        "start": 4789.86,
        "end": 4793.32,
        "text": " And this token is used to delimits documents in the training set."
    },
    {
        "id": 834,
        "start": 4794.32,
        "end": 4798.45,
        "text": " So when we're creating the training data, we have all these documents and we tokenize"
    },
    {
        "id": 835,
        "start": 4798.45,
        "end": 4799.74,
        "text": " them and get a stream of tokens."
    },
    {
        "id": 836,
        "start": 4800.88,
        "end": 4804.78,
        "text": " Those tokens only range from 0 to 50,256."
    },
    {
        "id": 837,
        "start": 4805.58,
        "end": 4809.599999999999,
        "text": " And then in between those documents we put special end of text token."
    },
    {
        "id": 838,
        "start": 4810.7,
        "end": 4812.84,
        "text": " And we insert that token in between documents."
    },
    {
        "id": 839,
        "start": 4813.96,
        "end": 4819.38,
        "text": " And we are using this as a signal to the language model that the document has ended and what"
    },
    {
        "id": 840,
        "start": 4819.38,
        "end": 4822.599999999999,
        "text": " follows is going to be unrelated to the document previously."
    },
    {
        "id": 841,
        "start": 4823.599999999999,
        "end": 4825.599999999999,
        "text": " That said the language model has to learn this from data."
    },
    {
        "id": 842,
        "start": 4825.8,
        "end": 4831.12,
        "text": " It needs to learn that this token usually means that it should wipe its sort of memory"
    },
    {
        "id": 843,
        "start": 4831.219999999999,
        "end": 4835.18,
        "text": " of what came before and what came before this token is not actually informative to what"
    },
    {
        "id": 844,
        "start": 4835.18,
        "end": 4835.76,
        "text": " comes next."
    },
    {
        "id": 845,
        "start": 4835.9,
        "end": 4838.5199999999995,
        "text": " But we are expecting the language model to just like learn this."
    },
    {
        "id": 846,
        "start": 4838.62,
        "end": 4842.3,
        "text": " But we're giving it the special sort of the limiter of these documents."
    },
    {
        "id": 847,
        "start": 4843.599999999999,
        "end": 4849.08,
        "text": " We can go here to tokenizer and this is the GPT to tokenizer, our code that we've been"
    },
    {
        "id": 848,
        "start": 4849.08,
        "end": 4849.8,
        "text": " playing with before."
    },
    {
        "id": 849,
        "start": 4850.4,
        "end": 4851.34,
        "text": " So we can add here, right?"
    },
    {
        "id": 850,
        "start": 4851.36,
        "end": 4853.219999999999,
        "text": " Hello world, how are you?"
    },
    {
        "id": 851,
        "start": 4853.639999999999,
        "end": 4854.58,
        "text": " And we're getting different tokens."
    },
    {
        "id": 852,
        "start": 4855.18,
        "end": 4858.599999999999,
        "text": " But now you can see what happens if I put end of text."
    },
    {
        "id": 853,
        "start": 4859.84,
        "end": 4867.3,
        "text": " You see how until I finished it, these are all different tokens end of text still for"
    },
    {
        "id": 854,
        "start": 4867.4,
        "end": 4867.68,
        "text": " tokens."
    },
    {
        "id": 855,
        "start": 4867.9,
        "end": 4872.38,
        "text": " And now when I finish it, suddenly we get token 50,256."
    },
    {
        "id": 856,
        "start": 4873.62,
        "end": 4878.76,
        "text": " And the reason this works is because this didn't actually go through the BPE merges."
    },
    {
        "id": 857,
        "start": 4879.219999999999,
        "end": 4885.84,
        "text": " Instead, the code that actually outposts the tokens has special case instructions for"
    },
    {
        "id": 858,
        "start": 4885.84,
        "end": 4887.34,
        "text": " handling special tokens."
    },
    {
        "id": 859,
        "start": 4888.34,
        "end": 4893.92,
        "text": " We did not see these special instructions for handling special tokens in the encoder.py."
    },
    {
        "id": 860,
        "start": 4894.28,
        "end": 4895.0199999999995,
        "text": " It's absent there."
    },
    {
        "id": 861,
        "start": 4895.7,
        "end": 4900.84,
        "text": " But if you go to tick token library, which is implemented in Rust, you will find all kinds"
    },
    {
        "id": 862,
        "start": 4900.84,
        "end": 4906.36,
        "text": " of special case handling for these special tokens that you can register, create, ask the"
    },
    {
        "id": 863,
        "start": 4906.36,
        "end": 4908.2,
        "text": " vocabulary, and then it looks for them."
    },
    {
        "id": 864,
        "start": 4908.2,
        "end": 4913.44,
        "text": " And it whenever it sees these special tokens like this, it will actually come in and swap"
    },
    {
        "id": 865,
        "start": 4913.5,
        "end": 4914.36,
        "text": " in that special token."
    },
    {
        "id": 866,
        "start": 4915.2,
        "end": 4919.28,
        "text": " So these things are outside of the typical algorithm of BPE encoding."
    },
    {
        "id": 867,
        "start": 4921.04,
        "end": 4926.04,
        "text": " So these special tokens are used pervasively, not just in basically base language modeling"
    },
    {
        "id": 868,
        "start": 4926.099999999999,
        "end": 4930.0199999999995,
        "text": " of predicting the next token in the sequence, but especially when it gets to later to the"
    },
    {
        "id": 869,
        "start": 4930.0199999999995,
        "end": 4934.099999999999,
        "text": " fine tuning stage and all of the chat, GBT sort of aspects of it."
    },
    {
        "id": 870,
        "start": 4934.68,
        "end": 4938.25,
        "text": " Because we don't just want to limit documents, we want to limit entire conversations between"
    },
    {
        "id": 871,
        "start": 4938.25,
        "end": 4939.719999999999,
        "text": " an assistant and a user."
    },
    {
        "id": 872,
        "start": 4940.36,
        "end": 4946.16,
        "text": " So if I refresh this tick token as a page, the default example that they have here is using"
    },
    {
        "id": 873,
        "start": 4946.74,
        "end": 4952.12,
        "text": " not sort of base model encoders, but fine-tuned model sort of tokenizers."
    },
    {
        "id": 874,
        "start": 4953.42,
        "end": 4958.5599999999995,
        "text": " So for example, using the GBT 3.5 turbo scheme, these here are all special tokens."
    },
    {
        "id": 875,
        "start": 4958.86,
        "end": 4961.38,
        "text": " I am start, I am end, etc."
    },
    {
        "id": 876,
        "start": 4962.219999999999,
        "end": 4965.92,
        "text": " This is short for imagining my log underscore start, by the way."
    },
    {
        "id": 877,
        "start": 4967.18,
        "end": 4970.78,
        "text": " But you can see here that there's a sort of start and end of every single message."
    },
    {
        "id": 878,
        "start": 4971.28,
        "end": 4977.099999999999,
        "text": " There can be many other tokens, lots of tokens, in use to delimit these conversations"
    },
    {
        "id": 879,
        "start": 4977.58,
        "end": 4980.12,
        "text": " and kind of keep track of the flow of the messages here."
    },
    {
        "id": 880,
        "start": 4981.18,
        "end": 4983.28,
        "text": " Now we can go back to the tick token library."
    },
    {
        "id": 881,
        "start": 4984.08,
        "end": 4988.48,
        "text": " And here when you scroll to the bottom, they talk about how you can extend tick token."
    },
    {
        "id": 882,
        "start": 4988.599999999999,
        "end": 4995.96,
        "text": " And I can, you can create basically, you can fork the CLK base tokenizer is in GBT 4."
    },
    {
        "id": 883,
        "start": 4996.0199999999995,
        "end": 4998.86,
        "text": " And for example, you can extend it by adding more special tokens."
    },
    {
        "id": 884,
        "start": 4999.08,
        "end": 4999.91,
        "text": " And these are totally up to you."
    },
    {
        "id": 885,
        "start": 4999.91,
        "end": 5004.18,
        "text": " You can come up with any arbitrary tokens and add them with the new ID afterwards."
    },
    {
        "id": 886,
        "start": 5004.78,
        "end": 5010.7,
        "text": " And the tick token library will correctly swap them out when it sees this in the strings."
    },
    {
        "id": 887,
        "start": 5011.92,
        "end": 5015.48,
        "text": " Now we can also go back to this file, which we looked at previously."
    },
    {
        "id": 888,
        "start": 5016.46,
        "end": 5020.38,
        "text": " And I mentioned that the GPT 2 in tick token opening I public that pie."
    },
    {
        "id": 889,
        "start": 5021.8,
        "end": 5024.62,
        "text": " We have the vocabulary, we have the pattern for splitting."
    },
    {
        "id": 890,
        "start": 5025.24,
        "end": 5028.67,
        "text": " And then here we are registering the single special token in GPT 2, which was the end"
    },
    {
        "id": 891,
        "start": 5028.67,
        "end": 5029.32,
        "text": " of text token."
    },
    {
        "id": 892,
        "start": 5029.62,
        "end": 5031.12,
        "text": " And we saw that it has this ID."
    },
    {
        "id": 893,
        "start": 5032.46,
        "end": 5037.099999999999,
        "text": " In GPT 4, when they defined this here, you see that the pattern has changed as we've"
    },
    {
        "id": 894,
        "start": 5037.099999999999,
        "end": 5037.42,
        "text": " discussed."
    },
    {
        "id": 895,
        "start": 5037.92,
        "end": 5040.16,
        "text": " But also the special tokens have changed in this token either."
    },
    {
        "id": 896,
        "start": 5040.5599999999995,
        "end": 5043.9,
        "text": " So we of course have the end of text, just like in GPT 2."
    },
    {
        "id": 897,
        "start": 5044.32,
        "end": 5047.46,
        "text": " But we also see three, sorry, four additional tokens here."
    },
    {
        "id": 898,
        "start": 5048.04,
        "end": 5049.66,
        "text": " Thin prefix, middle and suffix."
    },
    {
        "id": 899,
        "start": 5050.34,
        "end": 5050.86,
        "text": " What is thin?"
    },
    {
        "id": 900,
        "start": 5051.139999999999,
        "end": 5053.099999999999,
        "text": " Thin is short for fill in the middle."
    },
    {
        "id": 901,
        "start": 5054.16,
        "end": 5057.18,
        "text": " And if you'd like to learn more about this ID, it comes from this paper."
    },
    {
        "id": 902,
        "start": 5059.0599999999995,
        "end": 5060.5,
        "text": " And I'm not going to go into detail in this video."
    },
    {
        "id": 903,
        "start": 5060.5,
        "end": 5061.42,
        "text": " It's beyond this video."
    },
    {
        "id": 904,
        "start": 5062.04,
        "end": 5065.139999999999,
        "text": " And then there's one additional Serb token here."
    },
    {
        "id": 905,
        "start": 5066.08,
        "end": 5067.68,
        "text": " So that's that encoding as well."
    },
    {
        "id": 906,
        "start": 5068.82,
        "end": 5071.48,
        "text": " So it's very common basically to train a language model."
    },
    {
        "id": 907,
        "start": 5072.219999999999,
        "end": 5075.32,
        "text": " And then if you'd like, you can add special tokens."
    },
    {
        "id": 908,
        "start": 5076.219999999999,
        "end": 5081.54,
        "text": " Now when you add special tokens, you of course have to do some model surgery to the transformer"
    },
    {
        "id": 909,
        "start": 5081.94,
        "end": 5083.8,
        "text": " and all the primators involved in that transformer."
    },
    {
        "id": 910,
        "start": 5084.28,
        "end": 5085.74,
        "text": " Because you are basically adding an integer."
    },
    {
        "id": 911,
        "start": 5086.36,
        "end": 5090.28,
        "text": " And you want to make sure that for example, your embedding matrix for the vocabulary tokens"
    },
    {
        "id": 912,
        "start": 5090.7,
        "end": 5092.66,
        "text": " has to be extended by adding a row."
    },
    {
        "id": 913,
        "start": 5093.12,
        "end": 5096.76,
        "text": " And typically this row would be initialized with small random numbers for something like"
    },
    {
        "id": 914,
        "start": 5096.8,
        "end": 5096.98,
        "text": " that."
    },
    {
        "id": 915,
        "start": 5097.54,
        "end": 5101.08,
        "text": " Because we need to have a vector that now stands for that token."
    },
    {
        "id": 916,
        "start": 5101.78,
        "end": 5104.0,
        "text": " In addition to that, you have to go to the final layer of the transformer."
    },
    {
        "id": 917,
        "start": 5104.0,
        "end": 5108.74,
        "text": " And you have to make sure that that projection at the very end into the classifier is extended"
    },
    {
        "id": 918,
        "start": 5108.76,
        "end": 5109.719999999999,
        "text": " by one as well."
    },
    {
        "id": 919,
        "start": 5110.4,
        "end": 5114.76,
        "text": " And basically there's some model surgery involved that you have to couple with the tokenization"
    },
    {
        "id": 920,
        "start": 5114.84,
        "end": 5117.42,
        "text": " changes if you are going to add special tokens."
    },
    {
        "id": 921,
        "start": 5118.12,
        "end": 5121.67,
        "text": " But this is a very common operation that people do, especially if they'd like to fine tune"
    },
    {
        "id": 922,
        "start": 5121.67,
        "end": 5122.139999999999,
        "text": " the model."
    },
    {
        "id": 923,
        "start": 5122.42,
        "end": 5126.34,
        "text": " For example, taking it from a base model to a chat model like chat GPT."
    },
    {
        "id": 924,
        "start": 5128.12,
        "end": 5131.15,
        "text": " Okay, so at this point, you should have everything you need in order to build your own GPT"
    },
    {
        "id": 925,
        "start": 5131.15,
        "end": 5131.8,
        "text": " for tokenizer."
    },
    {
        "id": 926,
        "start": 5132.58,
        "end": 5136.219999999999,
        "text": " Now, in the process of developing this lecture, I've done that and I published the code"
    },
    {
        "id": 927,
        "start": 5136.28,
        "end": 5138.0599999999995,
        "text": " under this repository, minbpe."
    },
    {
        "id": 928,
        "start": 5139.36,
        "end": 5142.34,
        "text": " So minbpe looks like this right now is on recording."
    },
    {
        "id": 929,
        "start": 5142.68,
        "end": 5147.0599999999995,
        "text": " But the minbpe repository will probably change quite a bit because I intend to continue working"
    },
    {
        "id": 930,
        "start": 5147.139999999999,
        "end": 5147.44,
        "text": " on it."
    },
    {
        "id": 931,
        "start": 5148.7,
        "end": 5153.469999999999,
        "text": " In addition to the minbpe repository, I've published this exercise progression that you"
    },
    {
        "id": 932,
        "start": 5153.469999999999,
        "end": 5153.98,
        "text": " can follow."
    },
    {
        "id": 933,
        "start": 5154.5599999999995,
        "end": 5160.62,
        "text": " So if you go to exercise.md here, this is sort of me breaking up the task ahead of you"
    },
    {
        "id": 934,
        "start": 5160.62,
        "end": 5165.48,
        "text": " into four steps that sort of build up to what can be a GPT for tokenizer."
    },
    {
        "id": 935,
        "start": 5166.12,
        "end": 5170.16,
        "text": " And so feel free to follow these steps exactly and follow a little bit of the guidance that"
    },
    {
        "id": 936,
        "start": 5170.16,
        "end": 5170.82,
        "text": " I've laid out here."
    },
    {
        "id": 937,
        "start": 5171.5199999999995,
        "end": 5175.86,
        "text": " And anytime you feel stuck, just reference the minbpe repository here."
    },
    {
        "id": 938,
        "start": 5176.34,
        "end": 5180.24,
        "text": " So either the tests could be useful or the minbpe repository itself."
    },
    {
        "id": 939,
        "start": 5180.799999999999,
        "end": 5183.799999999999,
        "text": " I try to keep the code fairly clean and understandable."
    },
    {
        "id": 940,
        "start": 5184.34,
        "end": 5188.88,
        "text": " And so feel free to reference it whenever you get stuck."
    },
    {
        "id": 941,
        "start": 5190.28,
        "end": 5194.58,
        "text": " In addition to that, basically, once you write it, you should be able to reproduce this"
    },
    {
        "id": 942,
        "start": 5194.719999999999,
        "end": 5195.6,
        "text": " behavior from tectokin."
    },
    {
        "id": 943,
        "start": 5196.12,
        "end": 5200.84,
        "text": " So getting the GPT for tokenizer, you can take, you can encode this train and you should"
    },
    {
        "id": 944,
        "start": 5200.84,
        "end": 5201.5,
        "text": " get these tokens."
    },
    {
        "id": 945,
        "start": 5202.24,
        "end": 5205.0,
        "text": " And then you can encode and decode these at the same string to recover it."
    },
    {
        "id": 946,
        "start": 5205.74,
        "end": 5208.76,
        "text": " And in addition to all that, you should be able to implement your own train function,"
    },
    {
        "id": 947,
        "start": 5209.36,
        "end": 5213.0,
        "text": " which tectokin library does not provide it's again, only inference code."
    },
    {
        "id": 948,
        "start": 5213.299999999999,
        "end": 5216.34,
        "text": " But you should write your own train minbpe does it as well."
    },
    {
        "id": 949,
        "start": 5217.1,
        "end": 5219.5,
        "text": " And that will allow you to train your own token vocabulary."
    },
    {
        "id": 950,
        "start": 5221.2,
        "end": 5226.92,
        "text": " So here some of the code inside minbpe, minbpe, shows the token vocabulary that you might"
    },
    {
        "id": 951,
        "start": 5226.94,
        "end": 5227.299999999999,
        "text": " obtain."
    },
    {
        "id": 952,
        "start": 5228.0199999999995,
        "end": 5231.719999999999,
        "text": " So on the left here, we have the GPT for merges."
    },
    {
        "id": 953,
        "start": 5232.78,
        "end": 5235.98,
        "text": " So the first 256 are raw individual bytes."
    },
    {
        "id": 954,
        "start": 5236.46,
        "end": 5240.32,
        "text": " And then here I am visualizing the merges that GPT for performed during its training."
    },
    {
        "id": 955,
        "start": 5240.9,
        "end": 5246.38,
        "text": " So the very first merge that GPT for did was merge two spaces into a single token for"
    },
    {
        "id": 956,
        "start": 5246.86,
        "end": 5247.58,
        "text": " two spaces."
    },
    {
        "id": 957,
        "start": 5248.0599999999995,
        "end": 5249.219999999999,
        "text": " And that is the token 256."
    },
    {
        "id": 958,
        "start": 5250.32,
        "end": 5252.92,
        "text": " And so this is the order in which things merged during GPT for training."
    },
    {
        "id": 959,
        "start": 5253.5199999999995,
        "end": 5259.46,
        "text": " And this is the merge order that we obtain in minbpe by training a tokenizer."
    },
    {
        "id": 960,
        "start": 5259.98,
        "end": 5264.799999999999,
        "text": " And in this case, I trained it on Wikipedia page of Taylor Swift, not because I'm a Swiftie,"
    },
    {
        "id": 961,
        "start": 5264.799999999999,
        "end": 5269.219999999999,
        "text": " but because that is one of the longest Wikipedia pages apparently that's available."
    },
    {
        "id": 962,
        "start": 5269.82,
        "end": 5270.62,
        "text": " But she is pretty cool."
    },
    {
        "id": 963,
        "start": 5271.639999999999,
        "end": 5275.08,
        "text": " And what was I going to say?"
    },
    {
        "id": 964,
        "start": 5275.4,
        "end": 5275.6,
        "text": " Yeah."
    },
    {
        "id": 965,
        "start": 5276.5599999999995,
        "end": 5278.0599999999995,
        "text": " I've got these two vocabularies."
    },
    {
        "id": 966,
        "start": 5278.24,
        "end": 5284.84,
        "text": " And so as an example, here GPT for merges, I end to become in."
    },
    {
        "id": 967,
        "start": 5285.42,
        "end": 5288.68,
        "text": " And we've done the exact same thing on this token 259."
    },
    {
        "id": 968,
        "start": 5289.24,
        "end": 5291.32,
        "text": " Here space t becomes space t."
    },
    {
        "id": 969,
        "start": 5291.88,
        "end": 5294.0599999999995,
        "text": " And that happened for us a little bit later as well."
    },
    {
        "id": 970,
        "start": 5294.46,
        "end": 5298.36,
        "text": " So the difference here is again, to my understanding, only a difference of the training set."
    },
    {
        "id": 971,
        "start": 5298.78,
        "end": 5302.78,
        "text": " So as an example, because I see a lot of white space, I expect that GPT for probably had"
    },
    {
        "id": 972,
        "start": 5302.799999999999,
        "end": 5304.42,
        "text": " a lot of Python code in its training set."
    },
    {
        "id": 973,
        "start": 5304.48,
        "end": 5306.68,
        "text": " I'm not sure for the tokenizer."
    },
    {
        "id": 974,
        "start": 5307.82,
        "end": 5311.38,
        "text": " And here we see much plus of that, of course, in the Wikipedia page."
    },
    {
        "id": 975,
        "start": 5312.28,
        "end": 5313.76,
        "text": " So roughly speaking, they look the same."
    },
    {
        "id": 976,
        "start": 5314.08,
        "end": 5316.219999999999,
        "text": " And they look the same because they're running the same algorithm."
    },
    {
        "id": 977,
        "start": 5316.639999999999,
        "end": 5320.46,
        "text": " And when you train your own, you're probably going to get something similar, depending on"
    },
    {
        "id": 978,
        "start": 5320.46,
        "end": 5321.139999999999,
        "text": " what you train it on."
    },
    {
        "id": 979,
        "start": 5321.74,
        "end": 5325.7,
        "text": " Okay, so we are now going to move on from Tiktokin and the way that OpenAI tokenizes"
    },
    {
        "id": 980,
        "start": 5325.7,
        "end": 5326.299999999999,
        "text": " its strings."
    },
    {
        "id": 981,
        "start": 5326.94,
        "end": 5330.88,
        "text": " We're going to discuss one more very commonly used library for working with tokenization"
    },
    {
        "id": 982,
        "start": 5331.08,
        "end": 5331.5599999999995,
        "text": " on lambs."
    },
    {
        "id": 983,
        "start": 5332.0199999999995,
        "end": 5333.16,
        "text": " And that is sentence piece."
    },
    {
        "id": 984,
        "start": 5334.12,
        "end": 5339.44,
        "text": " So sentence piece is very commonly used in language models because on Tiktokin, it can"
    },
    {
        "id": 985,
        "start": 5339.44,
        "end": 5342.82,
        "text": " do both training and inference and is quite efficient at both."
    },
    {
        "id": 986,
        "start": 5343.639999999999,
        "end": 5348.54,
        "text": " It supports a number of algorithms for training vocabularies, but one of them is the Mike"
    },
    {
        "id": 987,
        "start": 5348.54,
        "end": 5350.16,
        "text": " Bering coding algorithm that we've been looking at."
    },
    {
        "id": 988,
        "start": 5350.5,
        "end": 5351.639999999999,
        "text": " So it supports it."
    },
    {
        "id": 989,
        "start": 5352.5,
        "end": 5357.68,
        "text": " Now sentence piece is used both by Lama and Mral series and many other models as well."
    },
    {
        "id": 990,
        "start": 5358.5599999999995,
        "end": 5360.639999999999,
        "text": " It is on GitHub under Google slash sentence piece."
    },
    {
        "id": 991,
        "start": 5362.0,
        "end": 5365.68,
        "text": " And the big difference with sentence piece, and we're going to look at example because"
    },
    {
        "id": 992,
        "start": 5365.719999999999,
        "end": 5371.08,
        "text": " this is kind of hard and subtle to explain, is that they think different about the order"
    },
    {
        "id": 993,
        "start": 5371.74,
        "end": 5373.36,
        "text": " of operations here."
    },
    {
        "id": 994,
        "start": 5373.78,
        "end": 5379.299999999999,
        "text": " So in the case of Tiktokin, we first take our code points in a string."
    },
    {
        "id": 995,
        "start": 5379.639999999999,
        "end": 5382.86,
        "text": " We encode them using ETF8 to bytes and then we're merging bytes."
    },
    {
        "id": 996,
        "start": 5383.139999999999,
        "end": 5383.98,
        "text": " It's fairly straightforward."
    },
    {
        "id": 997,
        "start": 5385.5199999999995,
        "end": 5390.32,
        "text": " For sentence piece, it works directly on the level of the code points themselves."
    },
    {
        "id": 998,
        "start": 5391.04,
        "end": 5393.94,
        "text": " So it looks at whatever code points are available in your training set."
    },
    {
        "id": 999,
        "start": 5394.66,
        "end": 5396.26,
        "text": " And then it starts merging those code points."
    },
    {
        "id": 1000,
        "start": 5397.0,
        "end": 5400.5599999999995,
        "text": " And the BPE is running on the level of code points."
    },
    {
        "id": 1001,
        "start": 5401.86,
        "end": 5406.62,
        "text": " And if you happen to run out of code points, so there are maybe some rare code points"
    },
    {
        "id": 1002,
        "start": 5406.66,
        "end": 5407.799999999999,
        "text": " that just don't come up too often."
    },
    {
        "id": 1003,
        "start": 5407.96,
        "end": 5410.5199999999995,
        "text": " And the rarity is determined by this character coverage of primer."
    },
    {
        "id": 1004,
        "start": 5411.719999999999,
        "end": 5418.35,
        "text": " Then these code points will either get maps to a special unknown token like UNC, or if"
    },
    {
        "id": 1005,
        "start": 5418.35,
        "end": 5422.57,
        "text": " you have the byte foldback option turned on, then they will take those rare code points."
    },
    {
        "id": 1006,
        "start": 5422.57,
        "end": 5424.74,
        "text": " It will encode them using ETF8."
    },
    {
        "id": 1007,
        "start": 5425.08,
        "end": 5428.44,
        "text": " And then the individual bytes of that encoding will be translated into tokens."
    },
    {
        "id": 1008,
        "start": 5428.74,
        "end": 5432.58,
        "text": " And there are these special byte tokens that basically get added to the vocabulary."
    },
    {
        "id": 1009,
        "start": 5433.299999999999,
        "end": 5440.26,
        "text": " So it uses BPE on the code points and then it falls back to bytes for rare code points."
    },
    {
        "id": 1010,
        "start": 5442.08,
        "end": 5443.19,
        "text": " And so that's kind of like the difference."
    },
    {
        "id": 1011,
        "start": 5443.19,
        "end": 5447.38,
        "text": " Personally, I find the token way significantly cleaner, but it's kind of like a subtle,"
    },
    {
        "id": 1012,
        "start": 5447.42,
        "end": 5450.1,
        "text": " but pretty major difference between the way they approach tokenization."
    },
    {
        "id": 1013,
        "start": 5450.7,
        "end": 5455.73,
        "text": " Let's work with a concrete example because otherwise, this is kind of hard to get your"
    },
    {
        "id": 1014,
        "start": 5455.73,
        "end": 5456.08,
        "text": " head around."
    },
    {
        "id": 1015,
        "start": 5457.1,
        "end": 5458.44,
        "text": " So let's work with a concrete example."
    },
    {
        "id": 1016,
        "start": 5459.42,
        "end": 5460.96,
        "text": " This is how we can import sentence piece."
    },
    {
        "id": 1017,
        "start": 5462.0599999999995,
        "end": 5465.85,
        "text": " And then here we're going to take, I think I took the description of sentence piece and"
    },
    {
        "id": 1018,
        "start": 5465.85,
        "end": 5467.46,
        "text": " I just created like a little toy dataset."
    },
    {
        "id": 1019,
        "start": 5468.0,
        "end": 5469.08,
        "text": " It really likes to have a file."
    },
    {
        "id": 1020,
        "start": 5469.24,
        "end": 5471.96,
        "text": " So I created a toy.txt file with this content."
    },
    {
        "id": 1021,
        "start": 5473.299999999999,
        "end": 5477.42,
        "text": " Now what's kind of a little bit crazy about sentence piece is that there's a ton of options"
    },
    {
        "id": 1022,
        "start": 5477.44,
        "end": 5478.12,
        "text": " and configurations."
    },
    {
        "id": 1023,
        "start": 5479.0599999999995,
        "end": 5482.46,
        "text": " And the reason this is so is because sentence piece has been around, I think, for a while."
    },
    {
        "id": 1024,
        "start": 5482.84,
        "end": 5485.08,
        "text": " And it really tries to handle a large diversity of things."
    },
    {
        "id": 1025,
        "start": 5485.96,
        "end": 5490.5599999999995,
        "text": " And because it's been around, I think it has quite a bit of accumulated historical baggage"
    },
    {
        "id": 1026,
        "start": 5490.9,
        "end": 5491.4,
        "text": " as well."
    },
    {
        "id": 1027,
        "start": 5492.34,
        "end": 5495.62,
        "text": " And so in particular, there's like a ton of configuration arguments."
    },
    {
        "id": 1028,
        "start": 5495.62,
        "end": 5496.74,
        "text": " This is not even all of it."
    },
    {
        "id": 1029,
        "start": 5497.639999999999,
        "end": 5499.9,
        "text": " You can go to here to see all the training options."
    },
    {
        "id": 1030,
        "start": 5501.84,
        "end": 5507.46,
        "text": " And there's also quite useful documentation when you look at the raw protobuff that is used"
    },
    {
        "id": 1031,
        "start": 5507.6,
        "end": 5509.78,
        "text": " to represent the trainer spec and so on."
    },
    {
        "id": 1032,
        "start": 5511.7,
        "end": 5513.98,
        "text": " Many of these options are irrelevant to us."
    },
    {
        "id": 1033,
        "start": 5514.0,
        "end": 5515.5,
        "text": " So maybe 2.0 on one example."
    },
    {
        "id": 1034,
        "start": 5516.0,
        "end": 5517.08,
        "text": " Dash dash shrinking factor."
    },
    {
        "id": 1035,
        "start": 5517.76,
        "end": 5521.16,
        "text": " This shrinking factor is not used in the bike bearing coding algorithm."
    },
    {
        "id": 1036,
        "start": 5521.5,
        "end": 5524.38,
        "text": " So this is just an argument that is irrelevant to us."
    },
    {
        "id": 1037,
        "start": 5525.28,
        "end": 5526.639999999999,
        "text": " It applies to a different training algorithm."
    },
    {
        "id": 1038,
        "start": 5529.74,
        "end": 5534.1,
        "text": " Now what I tried to do here is I tried to set up sentence piece in a way that is very,"
    },
    {
        "id": 1039,
        "start": 5534.1,
        "end": 5540.46,
        "text": " very similar as far as I can tell to maybe identical, hopefully, to the way that Lama 2 was trained."
    },
    {
        "id": 1040,
        "start": 5540.94,
        "end": 5545.0599999999995,
        "text": " So the way they trained their own tokenizer."
    },
    {
        "id": 1041,
        "start": 5545.5199999999995,
        "end": 5549.88,
        "text": " And the way I did this was basically can take the tokenizer.model file that met our release."
    },
    {
        "id": 1042,
        "start": 5550.639999999999,
        "end": 5556.98,
        "text": " And you can open it using the protobuff file that you can generate."
    },
    {
        "id": 1043,
        "start": 5557.719999999999,
        "end": 5559.03,
        "text": " And then you can inspect all the options."
    },
    {
        "id": 1044,
        "start": 5559.03,
        "end": 5561.32,
        "text": " And I tried to copy over all the options that looked relevant."
    },
    {
        "id": 1045,
        "start": 5562.219999999999,
        "end": 5563.34,
        "text": " So here we set up the input."
    },
    {
        "id": 1046,
        "start": 5563.82,
        "end": 5565.46,
        "text": " It's raw text in this file."
    },
    {
        "id": 1047,
        "start": 5566.0,
        "end": 5566.94,
        "text": " Here's going to be the output."
    },
    {
        "id": 1048,
        "start": 5567.18,
        "end": 5570.48,
        "text": " So it's going to be for to talk 400.model and .vo cap."
    },
    {
        "id": 1049,
        "start": 5571.86,
        "end": 5574.94,
        "text": " We're saying that we're going to use the BP algorithm and we want to voc up size of 400."
    },
    {
        "id": 1050,
        "start": 5576.5599999999995,
        "end": 5585.0,
        "text": " And there's a ton of configurations here for basically pre-processing and normalization"
    },
    {
        "id": 1051,
        "start": 5585.08,
        "end": 5586.0,
        "text": " rules as they're called."
    },
    {
        "id": 1052,
        "start": 5586.62,
        "end": 5591.18,
        "text": " Normalization used to be very prevalent, I would say, before LLM's in natural language processing."
    },
    {
        "id": 1053,
        "start": 5591.26,
        "end": 5596.4,
        "text": " So in machine translation and text classification and so on, you want to normalize and simplify"
    },
    {
        "id": 1054,
        "start": 5596.46,
        "end": 5600.36,
        "text": " the text and you want to turn it all lowercase and you want to remove all double whitespace,"
    },
    {
        "id": 1055,
        "start": 5600.54,
        "end": 5600.82,
        "text": " et cetera."
    },
    {
        "id": 1056,
        "start": 5601.54,
        "end": 5603.76,
        "text": " And in language models, we're not to do any of it."
    },
    {
        "id": 1057,
        "start": 5604.04,
        "end": 5605.88,
        "text": " And at least that is my preference as a deep learning person."
    },
    {
        "id": 1058,
        "start": 5606.219999999999,
        "end": 5607.53,
        "text": " You want to not touch your data."
    },
    {
        "id": 1059,
        "start": 5607.53,
        "end": 5612.299999999999,
        "text": " You want to keep the raw data as much as possible in a raw form."
    },
    {
        "id": 1060,
        "start": 5613.5599999999995,
        "end": 5616.04,
        "text": " So you're basically trying to turn off a lot of this if you can."
    },
    {
        "id": 1061,
        "start": 5617.04,
        "end": 5620.24,
        "text": " The other thing that sentence piece does is that it has this concept of sentences."
    },
    {
        "id": 1062,
        "start": 5621.0,
        "end": 5623.08,
        "text": " So sentence piece."
    },
    {
        "id": 1063,
        "start": 5623.639999999999,
        "end": 5629.24,
        "text": " It's back, it kind of was developed, I think, early in the days where there was an idea"
    },
    {
        "id": 1064,
        "start": 5629.24,
        "end": 5632.5599999999995,
        "text": " that you're training a tokenizer on a bunch of independent sentences."
    },
    {
        "id": 1065,
        "start": 5633.2,
        "end": 5636.36,
        "text": " So it has a lot of like how many sentences you're going to train on."
    },
    {
        "id": 1066,
        "start": 5636.639999999999,
        "end": 5638.0199999999995,
        "text": " What is the maximum sentence length?"
    },
    {
        "id": 1067,
        "start": 5641.08,
        "end": 5642.0199999999995,
        "text": " Shuffling sentences."
    },
    {
        "id": 1068,
        "start": 5642.46,
        "end": 5645.219999999999,
        "text": " So for it, sentences are kind of like the individual training examples."
    },
    {
        "id": 1069,
        "start": 5645.88,
        "end": 5649.66,
        "text": " But again, in the context of LLM's, I find that this is like a very spurious and weird"
    },
    {
        "id": 1070,
        "start": 5649.76,
        "end": 5650.16,
        "text": " distinction."
    },
    {
        "id": 1071,
        "start": 5650.5599999999995,
        "end": 5654.26,
        "text": " Like sentences are just like don't touch the raw data."
    },
    {
        "id": 1072,
        "start": 5654.299999999999,
        "end": 5655.639999999999,
        "text": " Sentences happen to exist."
    },
    {
        "id": 1073,
        "start": 5656.139999999999,
        "end": 5659.28,
        "text": " But in the raw data sets, there are a lot of like in between."
    },
    {
        "id": 1074,
        "start": 5659.299999999999,
        "end": 5660.86,
        "text": " Like what exactly is a sentence?"
    },
    {
        "id": 1075,
        "start": 5661.28,
        "end": 5662.12,
        "text": " What isn't a sentence?"
    },
    {
        "id": 1076,
        "start": 5663.48,
        "end": 5667.32,
        "text": " And so I think like it's really hard to define what an actual sentence is if you're really"
    },
    {
        "id": 1077,
        "start": 5667.34,
        "end": 5668.1,
        "text": " like digging into it."
    },
    {
        "id": 1078,
        "start": 5668.66,
        "end": 5671.98,
        "text": " And there could be different concepts of it in different languages or something like that."
    },
    {
        "id": 1079,
        "start": 5671.98,
        "end": 5673.8099999999995,
        "text": " So why even introduce the concept?"
    },
    {
        "id": 1080,
        "start": 5673.8099999999995,
        "end": 5675.799999999999,
        "text": " It doesn't honestly make sense to me."
    },
    {
        "id": 1081,
        "start": 5675.799999999999,
        "end": 5679.299999999999,
        "text": " I would just prefer to treat a file as a giant stream of bytes."
    },
    {
        "id": 1082,
        "start": 5680.639999999999,
        "end": 5684.85,
        "text": " It has a lot of treatment around where word characters and when I say word, I mean code"
    },
    {
        "id": 1083,
        "start": 5684.85,
        "end": 5685.18,
        "text": " points."
    },
    {
        "id": 1084,
        "start": 5685.4,
        "end": 5686.6,
        "text": " We're going to come back to this in a second."
    },
    {
        "id": 1085,
        "start": 5687.719999999999,
        "end": 5693.98,
        "text": " And it has a lot of other rules for basically splitting digits, splitting, white space and"
    },
    {
        "id": 1086,
        "start": 5693.98,
        "end": 5695.69,
        "text": " numbers and how you deal with that."
    },
    {
        "id": 1087,
        "start": 5695.69,
        "end": 5697.76,
        "text": " So these are some kind of like merge rules."
    },
    {
        "id": 1088,
        "start": 5698.38,
        "end": 5703.1,
        "text": " So I think this is a little bit equivalent to Tiktokon using the regular expression to"
    },
    {
        "id": 1089,
        "start": 5703.1,
        "end": 5704.139999999999,
        "text": " split up categories."
    },
    {
        "id": 1090,
        "start": 5704.74,
        "end": 5709.38,
        "text": " There's like kind of a equivalence of it if you squint at it in sentence piece where you"
    },
    {
        "id": 1091,
        "start": 5709.38,
        "end": 5714.6,
        "text": " can also, for example, split up the digits and so on."
    },
    {
        "id": 1092,
        "start": 5716.4,
        "end": 5719.0199999999995,
        "text": " There's a few more things here that I'll come back to in a bit and then there are some"
    },
    {
        "id": 1093,
        "start": 5719.0199999999995,
        "end": 5720.34,
        "text": " special tokens that you can indicate."
    },
    {
        "id": 1094,
        "start": 5720.94,
        "end": 5726.44,
        "text": " And it hard codes the unctoken, the beginning of sentence, end of sentence and a pad token."
    },
    {
        "id": 1095,
        "start": 5728.0599999999995,
        "end": 5730.74,
        "text": " And the unctoken must exist for my understanding."
    },
    {
        "id": 1096,
        "start": 5731.88,
        "end": 5732.86,
        "text": " And then some systems things."
    },
    {
        "id": 1097,
        "start": 5733.18,
        "end": 5739.33,
        "text": " So we can train and when I press train, it's going to create this file, talk for 100 dot"
    },
    {
        "id": 1098,
        "start": 5739.33,
        "end": 5740.96,
        "text": " model and talk for 100 dot bocap."
    },
    {
        "id": 1099,
        "start": 5741.74,
        "end": 5745.84,
        "text": " I can then load the model file and I can inspect the vocabulary of it."
    },
    {
        "id": 1100,
        "start": 5746.7,
        "end": 5751.32,
        "text": " And so we trained a vocab size 400 on this text here."
    },
    {
        "id": 1101,
        "start": 5752.44,
        "end": 5756.58,
        "text": " And these are the individual pieces, the individual tokens that sentence piece will create."
    },
    {
        "id": 1102,
        "start": 5757.24,
        "end": 5761.66,
        "text": " So in the beginning we see that we have the unctoken with the ID zero."
    },
    {
        "id": 1103,
        "start": 5762.32,
        "end": 5766.12,
        "text": " Then we have the beginning of sequence, end of sequence, one and two."
    },
    {
        "id": 1104,
        "start": 5766.98,
        "end": 5768.79,
        "text": " And then we said that the pad ID is negative one."
    },
    {
        "id": 1105,
        "start": 5768.79,
        "end": 5770.44,
        "text": " So we chose not to use it."
    },
    {
        "id": 1106,
        "start": 5771.24,
        "end": 5772.36,
        "text": " So there's no pad ID here."
    },
    {
        "id": 1107,
        "start": 5773.5599999999995,
        "end": 5776.32,
        "text": " Then these are individual byte tokens."
    },
    {
        "id": 1108,
        "start": 5777.12,
        "end": 5781.16,
        "text": " So here we saw that byte fallback in Lama was turned on."
    },
    {
        "id": 1109,
        "start": 5781.38,
        "end": 5782.0,
        "text": " So it's true."
    },
    {
        "id": 1110,
        "start": 5782.74,
        "end": 5786.299999999999,
        "text": " So what follows are going to be the 256 byte tokens."
    },
    {
        "id": 1111,
        "start": 5787.6,
        "end": 5788.5199999999995,
        "text": " And these are their IDs."
    },
    {
        "id": 1112,
        "start": 5792.139999999999,
        "end": 5796.62,
        "text": " And then at the bottom, after the byte tokens come the merges."
    },
    {
        "id": 1113,
        "start": 5798.0199999999995,
        "end": 5800.54,
        "text": " And these are the parent nodes in the merges."
    },
    {
        "id": 1114,
        "start": 5800.96,
        "end": 5803.68,
        "text": " So we're not seeing the children or just seeing the parents and their ID."
    },
    {
        "id": 1115,
        "start": 5804.94,
        "end": 5811.5,
        "text": " And then after the merges comes eventually the individual tokens and their IDs."
    },
    {
        "id": 1116,
        "start": 5812.38,
        "end": 5813.63,
        "text": " And so these are the individual tokens."
    },
    {
        "id": 1117,
        "start": 5813.63,
        "end": 5818.44,
        "text": " So these are the individual code point tokens, if you will, and they come at the end."
    },
    {
        "id": 1118,
        "start": 5819.139999999999,
        "end": 5822.88,
        "text": " So that is the ordering with which sentence piece sort of black represents its vocabularies."
    },
    {
        "id": 1119,
        "start": 5823.12,
        "end": 5827.5,
        "text": " It starts with special tokens, then the byte tokens, then the merge tokens, and then"
    },
    {
        "id": 1120,
        "start": 5827.5,
        "end": 5828.94,
        "text": " the individual code point tokens."
    },
    {
        "id": 1121,
        "start": 5830.16,
        "end": 5834.82,
        "text": " And all these raw code point token tokens are the ones that it encountered in the training"
    },
    {
        "id": 1122,
        "start": 5834.88,
        "end": 5835.12,
        "text": " set."
    },
    {
        "id": 1123,
        "start": 5836.38,
        "end": 5842.26,
        "text": " So those individual code points are all the entire set of code points that occurred here."
    },
    {
        "id": 1124,
        "start": 5844.66,
        "end": 5846.08,
        "text": " So those all get put in there."
    },
    {
        "id": 1125,
        "start": 5846.7,
        "end": 5849.84,
        "text": " And then those are extremely rare as determined by character coverage."
    },
    {
        "id": 1126,
        "start": 5850.139999999999,
        "end": 5854.719999999999,
        "text": " So if a code point occurred in a single time out of like a million sentences or something"
    },
    {
        "id": 1127,
        "start": 5854.74,
        "end": 5856.76,
        "text": " like that, then it would be ignored."
    },
    {
        "id": 1128,
        "start": 5857.24,
        "end": 5860.12,
        "text": " And it would not be added to our vocabulary."
    },
    {
        "id": 1129,
        "start": 5861.38,
        "end": 5866.5,
        "text": " Once we have a vocabulary, we can encode into IDs and we can sort of get a list."
    },
    {
        "id": 1130,
        "start": 5867.5199999999995,
        "end": 5874.61,
        "text": " And then here I am also decoding the individual tokens back into little pieces as they call"
    },
    {
        "id": 1131,
        "start": 5874.61,
        "end": 5874.84,
        "text": " it."
    },
    {
        "id": 1132,
        "start": 5875.86,
        "end": 5877.139999999999,
        "text": " So let's take a look at what happened here."
    },
    {
        "id": 1133,
        "start": 5878.219999999999,
        "end": 5880.24,
        "text": " Hello space on Yang-Aseo."
    },
    {
        "id": 1134,
        "start": 5881.26,
        "end": 5884.2,
        "text": " So these are the token IDs we got back."
    },
    {
        "id": 1135,
        "start": 5885.12,
        "end": 5889.5,
        "text": " And when we look here, a few things sort of jump to mind."
    },
    {
        "id": 1136,
        "start": 5890.5199999999995,
        "end": 5893.16,
        "text": " Number one, take a look at these characters."
    },
    {
        "id": 1137,
        "start": 5893.44,
        "end": 5895.74,
        "text": " The Korean characters, of course, were not part of the training set."
    },
    {
        "id": 1138,
        "start": 5896.38,
        "end": 5900.96,
        "text": " So sentence pieces encountering code points that it has not seen during training time."
    },
    {
        "id": 1139,
        "start": 5901.42,
        "end": 5904.6,
        "text": " And those code points do not have a token associated with them."
    },
    {
        "id": 1140,
        "start": 5905.1,
        "end": 5908.0599999999995,
        "text": " So suddenly these are on the tokens, unknown tokens."
    },
    {
        "id": 1141,
        "start": 5909.24,
        "end": 5914.5,
        "text": " Because byte foldback is true, instead sentence piece falls back to bytes."
    },
    {
        "id": 1142,
        "start": 5915.0199999999995,
        "end": 5921.96,
        "text": " And so it takes this, it encodes it with utf8, and then it uses these tokens to represent"
    },
    {
        "id": 1143,
        "start": 5922.54,
        "end": 5923.2,
        "text": " those bytes."
    },
    {
        "id": 1144,
        "start": 5924.4,
        "end": 5926.24,
        "text": " And that's what we are getting sort of here."
    },
    {
        "id": 1145,
        "start": 5927.0199999999995,
        "end": 5929.4,
        "text": " This is the utf8 encoding."
    },
    {
        "id": 1146,
        "start": 5930.0,
        "end": 5936.6,
        "text": " And it is shifted by three because of these special tokens here that have IDs earlier"
    },
    {
        "id": 1147,
        "start": 5936.639999999999,
        "end": 5936.84,
        "text": " on."
    },
    {
        "id": 1148,
        "start": 5937.78,
        "end": 5938.74,
        "text": " So that's what happened here."
    },
    {
        "id": 1149,
        "start": 5939.719999999999,
        "end": 5945.68,
        "text": " Now one more thing that, well, first before I go on with respect to the byte foldback,"
    },
    {
        "id": 1150,
        "start": 5946.16,
        "end": 5947.96,
        "text": " let me remove byte foldback."
    },
    {
        "id": 1151,
        "start": 5948.54,
        "end": 5950.54,
        "text": " If this is false, what's going to happen?"
    },
    {
        "id": 1152,
        "start": 5950.98,
        "end": 5951.54,
        "text": " Let's retrain."
    },
    {
        "id": 1153,
        "start": 5952.92,
        "end": 5955.78,
        "text": " So the first thing that happened is all the byte tokens disappeared, right?"
    },
    {
        "id": 1154,
        "start": 5956.639999999999,
        "end": 5957.8099999999995,
        "text": " And now we just have the merges."
    },
    {
        "id": 1155,
        "start": 5957.8099999999995,
        "end": 5961.09,
        "text": " And we have a lot more merges now because we have a lot more space because we're not"
    },
    {
        "id": 1156,
        "start": 5961.09,
        "end": 5964.94,
        "text": " taking up space in the woke app size with all the bytes."
    },
    {
        "id": 1157,
        "start": 5966.38,
        "end": 5970.38,
        "text": " And now if we encode this, we get a zero."
    },
    {
        "id": 1158,
        "start": 5971.26,
        "end": 5974.0599999999995,
        "text": " So this entire string here suddenly, there's no byte foldback."
    },
    {
        "id": 1159,
        "start": 5974.26,
        "end": 5977.44,
        "text": " So this is unknown, and unknown is unk."
    },
    {
        "id": 1160,
        "start": 5978.28,
        "end": 5982.24,
        "text": " And so this is zero because the unk token is token zero."
    },
    {
        "id": 1161,
        "start": 5983.38,
        "end": 5986.76,
        "text": " And you have to keep in mind that this would feed into your language model."
    },
    {
        "id": 1162,
        "start": 5987.04,
        "end": 5990.389999999999,
        "text": " So what is the language model supposed to do when all kinds of different things that"
    },
    {
        "id": 1163,
        "start": 5990.389999999999,
        "end": 5993.96,
        "text": " are unrecognized because they're rare, just end up mapping into unk?"
    },
    {
        "id": 1164,
        "start": 5994.32,
        "end": 5995.9,
        "text": " It's not exactly the property that you want."
    },
    {
        "id": 1165,
        "start": 5996.4,
        "end": 6002.389999999999,
        "text": " So that's why I think Lama correctly used byte foldback true because we definitely want"
    },
    {
        "id": 1166,
        "start": 6002.389999999999,
        "end": 6007.36,
        "text": " to feed these unknown or rare code points into the model in some manner."
    },
    {
        "id": 1167,
        "start": 6007.88,
        "end": 6009.62,
        "text": " The next thing I want to show you is the following."
    },
    {
        "id": 1168,
        "start": 6011.0,
        "end": 6016.46,
        "text": " Notice here when we are decoding all the individual tokens, you see how space is space"
    },
    {
        "id": 1169,
        "start": 6016.5199999999995,
        "end": 6019.62,
        "text": " here ends up being this bold underlying."
    },
    {
        "id": 1170,
        "start": 6020.26,
        "end": 6025.17,
        "text": " I'm not 100% sure, by the way, why sentence be switches, white space into these bold underscore"
    },
    {
        "id": 1171,
        "start": 6025.17,
        "end": 6025.92,
        "text": " characters."
    },
    {
        "id": 1172,
        "start": 6026.58,
        "end": 6027.44,
        "text": " Maybe it's for visualization."
    },
    {
        "id": 1173,
        "start": 6027.44,
        "end": 6029.2,
        "text": " I'm not 100% sure why that happens."
    },
    {
        "id": 1174,
        "start": 6030.24,
        "end": 6036.5199999999995,
        "text": " But notice this, why do we have an extra space in the front of hello?"
    },
    {
        "id": 1175,
        "start": 6037.5199999999995,
        "end": 6039.299999999999,
        "text": " What is this coming from?"
    },
    {
        "id": 1176,
        "start": 6039.92,
        "end": 6041.799999999999,
        "text": " Well, it's coming from this option here."
    },
    {
        "id": 1177,
        "start": 6045.44,
        "end": 6046.76,
        "text": " Add dummy prefix is true."
    },
    {
        "id": 1178,
        "start": 6047.5,
        "end": 6051.84,
        "text": " And when you go to the documentation, add dummy, white space at the beginning of text"
    },
    {
        "id": 1179,
        "start": 6051.92,
        "end": 6055.98,
        "text": " in order to treat world in world and hello world in the exact same way."
    },
    {
        "id": 1180,
        "start": 6056.5599999999995,
        "end": 6058.0599999999995,
        "text": " So what this is trying to do is the following."
    },
    {
        "id": 1181,
        "start": 6059.219999999999,
        "end": 6068.219999999999,
        "text": " If we go back to our tiktokinizer world as token by itself has a different ID than space"
    },
    {
        "id": 1182,
        "start": 6068.32,
        "end": 6068.58,
        "text": " world."
    },
    {
        "id": 1183,
        "start": 6069.36,
        "end": 6073.28,
        "text": " So we have this is 1917, but this is 14, etc."
    },
    {
        "id": 1184,
        "start": 6074.0199999999995,
        "end": 6075.98,
        "text": " So these are two different tokens for the language model."
    },
    {
        "id": 1185,
        "start": 6076.44,
        "end": 6079.26,
        "text": " And the language model has to learn from data that they are actually kind of like a very"
    },
    {
        "id": 1186,
        "start": 6079.26,
        "end": 6079.96,
        "text": " similar concept."
    },
    {
        "id": 1187,
        "start": 6080.5,
        "end": 6086.16,
        "text": " So to the language model in the tiktokin world, basically words in the beginning of sentences"
    },
    {
        "id": 1188,
        "start": 6086.18,
        "end": 6089.08,
        "text": " and words in the middle of sentences actually look completely different."
    },
    {
        "id": 1189,
        "start": 6090.7,
        "end": 6092.84,
        "text": " And it has learned that they are roughly the same."
    },
    {
        "id": 1190,
        "start": 6093.719999999999,
        "end": 6097.42,
        "text": " So this add dummy prefix is trying to fight that a little bit."
    },
    {
        "id": 1191,
        "start": 6097.48,
        "end": 6102.98,
        "text": " And the way that works is that it basically adds dummy prefix."
    },
    {
        "id": 1192,
        "start": 6104.86,
        "end": 6109.96,
        "text": " For as a part of pre-processing, it will take the string and it will add a space."
    },
    {
        "id": 1193,
        "start": 6110.44,
        "end": 6111.16,
        "text": " It will do this."
    },
    {
        "id": 1194,
        "start": 6112.219999999999,
        "end": 6116.66,
        "text": " And that's done in an effort to make this world and that world the same."
    },
    {
        "id": 1195,
        "start": 6116.88,
        "end": 6118.24,
        "text": " They will both be space world."
    },
    {
        "id": 1196,
        "start": 6119.299999999999,
        "end": 6124.88,
        "text": " So that's one other kind of pre-processing option that is turned on and Lama too also uses"
    },
    {
        "id": 1197,
        "start": 6124.92,
        "end": 6125.46,
        "text": " this option."
    },
    {
        "id": 1198,
        "start": 6126.1,
        "end": 6129.16,
        "text": " And that's I think everything that I want to say from my preview of sentence piece and"
    },
    {
        "id": 1199,
        "start": 6129.16,
        "end": 6129.88,
        "text": " how it is different."
    },
    {
        "id": 1200,
        "start": 6131.48,
        "end": 6138.2,
        "text": " Maybe here what I've done is I just put in the raw protocol buffer representation basically"
    },
    {
        "id": 1201,
        "start": 6138.6,
        "end": 6140.84,
        "text": " of the tokenizer, the Lama 2 trained."
    },
    {
        "id": 1202,
        "start": 6141.6,
        "end": 6143.74,
        "text": " So feel free to sort of step through this."
    },
    {
        "id": 1203,
        "start": 6143.76,
        "end": 6149.5,
        "text": " And if you would like your tokenization to look identical to that of the meta Lama 2,"
    },
    {
        "id": 1204,
        "start": 6149.6,
        "end": 6152.38,
        "text": " then you would be copy-based in these settings as I try to do up above."
    },
    {
        "id": 1205,
        "start": 6153.34,
        "end": 6156.7,
        "text": " And yeah, that's I think that's it for this section."
    },
    {
        "id": 1206,
        "start": 6157.08,
        "end": 6160.54,
        "text": " I think my summary for sentence piece from all of this is number one."
    },
    {
        "id": 1207,
        "start": 6160.68,
        "end": 6163.12,
        "text": " I think that there's a lot of historical baggage in sentence piece."
    },
    {
        "id": 1208,
        "start": 6163.54,
        "end": 6167.76,
        "text": " A lot of concepts that I think are slightly confusing and I think potentially contain"
    },
    {
        "id": 1209,
        "start": 6167.78,
        "end": 6171.48,
        "text": " foot guns like this concept of a sentence and its maximum length things to like that."
    },
    {
        "id": 1210,
        "start": 6173.0599999999995,
        "end": 6178.46,
        "text": " Otherwise it is fairly commonly used in the industry because it is efficient and can"
    },
    {
        "id": 1211,
        "start": 6178.46,
        "end": 6179.44,
        "text": " do both training and inference."
    },
    {
        "id": 1212,
        "start": 6180.32,
        "end": 6184.79,
        "text": " It has a few quirks like for example, Anctokin must exist and the way the bytefold backs"
    },
    {
        "id": 1213,
        "start": 6184.79,
        "end": 6186.98,
        "text": " are done and so on, I don't find particularly elegant."
    },
    {
        "id": 1214,
        "start": 6187.76,
        "end": 6189.6,
        "text": " And unfortunately I have to say it's not very well documented."
    },
    {
        "id": 1215,
        "start": 6189.6,
        "end": 6201.160000000001,
        "text": " So it took me a lot of time working with this myself and just visualizing things and try to really understand what is happening here because the documentation unfortunately isn't my opinion not super amazing."
    },
    {
        "id": 1216,
        "start": 6201.92,
        "end": 6206.6,
        "text": " But it is a very nice repo that is available to you if you'd like to train you into a connoisseur right now."
    },
    {
        "id": 1217,
        "start": 6207.42,
        "end": 6210.26,
        "text": " Okay, let me now switch gears again as we're starting to slowly wrap up here."
    },
    {
        "id": 1218,
        "start": 6210.88,
        "end": 6216.56,
        "text": " I want to revisit this issue in a bit more detail of how we should set the vocab size and what are some of the considerations around it."
    },
    {
        "id": 1219,
        "start": 6217.4400000000005,
        "end": 6223.900000000001,
        "text": " So for this, I'd like to go back to the model architecture that we developed in the last video when we built the GPT from scratch."
    },
    {
        "id": 1220,
        "start": 6224.88,
        "end": 6233.280000000001,
        "text": " So this here was the file that we built in the previous video and we defined the transformer model and let's specifically look at vocab size and where it appears in this file."
    },
    {
        "id": 1221,
        "start": 6234.02,
        "end": 6235.38,
        "text": " So here we define the vocab size."
    },
    {
        "id": 1222,
        "start": 6236.1,
        "end": 6241.360000000001,
        "text": " At this time it was 65 or something like that, extremely small number. So this will grow much larger."
    },
    {
        "id": 1223,
        "start": 6242.400000000001,
        "end": 6245.06,
        "text": " You'll see that vocab size doesn't come up too much in most of these layers."
    },
    {
        "id": 1224,
        "start": 6245.1,
        "end": 6249.200000000001,
        "text": " The only place that it comes up to is in exactly these two places here."
    },
    {
        "id": 1225,
        "start": 6250.14,
        "end": 6258.72,
        "text": " So when we define the language model, there's the token embedding table which is this two-dimensional array where the vocab size is basically the number of rows."
    },
    {
        "id": 1226,
        "start": 6259.88,
        "end": 6265.9800000000005,
        "text": " And each vocabulary element, each token has a vector that we're going to train using back propagation."
    },
    {
        "id": 1227,
        "start": 6266.6,
        "end": 6269.68,
        "text": " That vector is of size and embed, which is number of channels in the transformer."
    },
    {
        "id": 1228,
        "start": 6270.68,
        "end": 6276.72,
        "text": " And basically, as vocab size increases, this embedding table, as I mentioned earlier, is going to also grow. We're going to be adding rows."
    },
    {
        "id": 1229,
        "start": 6277.900000000001,
        "end": 6282.620000000001,
        "text": " In addition to that, at the end of the transformer, there's this LMHED layer, which is a linear layer."
    },
    {
        "id": 1230,
        "start": 6283.4400000000005,
        "end": 6290.1,
        "text": " And you'll notice that that layer is used at the very end to produce the logits, which become the probabilities for the next token in sequence."
    },
    {
        "id": 1231,
        "start": 6290.780000000001,
        "end": 6298.88,
        "text": " And so intuitively, we're trying to produce a probability for every single token that might come next at every point in time of that transformer."
    },
    {
        "id": 1232,
        "start": 6299.660000000001,
        "end": 6302.820000000001,
        "text": " And if we have more and more tokens, we need to produce more and more probabilities."
    },
    {
        "id": 1233,
        "start": 6303.6,
        "end": 6310.52,
        "text": " So every single token is going to introduce an additional dot product that we have to do here in this linear layer for this final layer in the transformer."
    },
    {
        "id": 1234,
        "start": 6311.620000000001,
        "end": 6318.68,
        "text": " So why can't vocab size be infinite? Why can't we grow to infinity? Well, number one, your token embedding table is going to grow."
    },
    {
        "id": 1235,
        "start": 6320.120000000001,
        "end": 6326.820000000001,
        "text": " Your linear layer is going to grow. So we're going to be doing a lot more computation here because this LMHED layer will become more computational expensive."
    },
    {
        "id": 1236,
        "start": 6327.900000000001,
        "end": 6334.200000000001,
        "text": " Number two, because we have more parameters, we could be worried that we are going to be under training some of these parameters."
    },
    {
        "id": 1237,
        "start": 6335.52,
        "end": 6346.34,
        "text": " So intuitively, if you have a very large vocabulary size, say we have a million tokens, then everyone of these tokens is going to come up more and more rarely in the training data, because there's a lot more other tokens all over the place."
    },
    {
        "id": 1238,
        "start": 6347.06,
        "end": 6351.46,
        "text": " And so we're going to be seeing fewer and fewer examples for each individual token."
    },
    {
        "id": 1239,
        "start": 6352.18,
        "end": 6360.660000000001,
        "text": " And you might be worried that basically the vectors associated with every token will be under trained as a result, because they just don't come up too often and they don't participate in the forward backward pass."
    },
    {
        "id": 1240,
        "start": 6361.64,
        "end": 6366.360000000001,
        "text": " In addition to that, as your vocab size grows, you're going to start shrinking your sequences a lot, right?"
    },
    {
        "id": 1241,
        "start": 6367.0,
        "end": 6371.620000000001,
        "text": " And that's really nice because that means that we're going to be attending to more and more text. So that's nice."
    },
    {
        "id": 1242,
        "start": 6372.120000000001,
        "end": 6376.780000000001,
        "text": " But also you might be worrying that too large of chunks are being squished into single tokens."
    },
    {
        "id": 1243,
        "start": 6377.400000000001,
        "end": 6386.740000000001,
        "text": " And so the model just doesn't have as much time to think per sort of some number of characters in a text or even think about it that way, right?"
    },
    {
        "id": 1244,
        "start": 6387.200000000001,
        "end": 6394.46,
        "text": " So basically we're squishing too much information into a single token and then the forward pass of the transformer is not enough to actually process that information appropriately."
    },
    {
        "id": 1245,
        "start": 6395.240000000001,
        "end": 6398.4800000000005,
        "text": " And so these are some of the considerations you're thinking about when you're designing the vocab size."
    },
    {
        "id": 1246,
        "start": 6398.9800000000005,
        "end": 6407.8,
        "text": " As I mentioned, this is mostly an empirical hyperparameter. And it seems like in state of the art architectures today, this is usually in the high 10,000s or somewhere around 100,000 today."
    },
    {
        "id": 1247,
        "start": 6408.4800000000005,
        "end": 6415.200000000001,
        "text": " And the next consideration I want to briefly talk about is what if we want to take a pre-trained model and we want to extend the vocab size."
    },
    {
        "id": 1248,
        "start": 6415.8,
        "end": 6420.400000000001,
        "text": " And this is done fairly commonly actually. So for example, when you're doing fine tuning for chat GPT,"
    },
    {
        "id": 1249,
        "start": 6421.4800000000005,
        "end": 6430.200000000001,
        "text": " a lot more use special tokens get introduced on top of the base model to maintain the metadata and all the structure of conversation objects between the user and assistant."
    },
    {
        "id": 1250,
        "start": 6430.900000000001,
        "end": 6437.5,
        "text": " So that takes a lot of special tokens. You might also try to throw in more special tokens, for example, for using the browser or any other tool."
    },
    {
        "id": 1251,
        "start": 6438.1,
        "end": 6442.200000000001,
        "text": " And so it's very tempting to add a lot of tokens for all kinds of special functionality."
    },
    {
        "id": 1252,
        "start": 6443.120000000001,
        "end": 6448.56,
        "text": " So if you want to be adding a token, that's totally possible, right? All we have to do is we have to resize this embedding."
    },
    {
        "id": 1253,
        "start": 6448.8,
        "end": 6454.18,
        "text": " So we have to add rows. We would initialize these parameters from scratch, which would be small random numbers."
    },
    {
        "id": 1254,
        "start": 6454.84,
        "end": 6464.96,
        "text": " And then we have to extend the weight inside this linear. So we have to start making dot products with the associated parameters as well to basically calculate the probabilities for these new tokens."
    },
    {
        "id": 1255,
        "start": 6465.820000000001,
        "end": 6472.06,
        "text": " So both of these are just resizing operation. It's a very mild model surgery and can be done fairly easily."
    },
    {
        "id": 1256,
        "start": 6472.4400000000005,
        "end": 6480.22,
        "text": " And it's quite common that basically you would freeze the base model, you introduce these new parameters, and then you only train these new parameters to introduce new tokens into the architecture."
    },
    {
        "id": 1257,
        "start": 6480.900000000001,
        "end": 6486.240000000001,
        "text": " And so you can freeze arbitrary parts of it or you can train arbitrary parts of it and that's totally up to you."
    },
    {
        "id": 1258,
        "start": 6486.740000000001,
        "end": 6490.160000000001,
        "text": " But basically minor surgery required if you'd like to introduce new tokens."
    },
    {
        "id": 1259,
        "start": 6490.660000000001,
        "end": 6500.76,
        "text": " And finally, I'd like to mention that actually there's an entire design space of applications in terms of introducing new tokens to vocabulary that go way beyond just adding special tokens and special new functionality."
    },
    {
        "id": 1260,
        "start": 6501.400000000001,
        "end": 6505.08,
        "text": " So just give you a sense of the design space, but this could be an entire video just by itself."
    },
    {
        "id": 1261,
        "start": 6505.56,
        "end": 6510.4400000000005,
        "text": " This is a paper on learning to compress prompts with what they called just tokens."
    },
    {
        "id": 1262,
        "start": 6511.360000000001,
        "end": 6515.900000000001,
        "text": " And rough idea is suppose that you're using language models and a setting that requires very long prompts."
    },
    {
        "id": 1263,
        "start": 6516.6,
        "end": 6525.22,
        "text": " Well, these long prompts just slow everything down because you have to encode them and then you have to use them and then you're attending over them and it's just heavy to have very large prompts."
    },
    {
        "id": 1264,
        "start": 6526.0,
        "end": 6535.06,
        "text": " So instead what they do here in this paper is they introduce new tokens and imagine basically having a few new tokens."
    },
    {
        "id": 1265,
        "start": 6535.1,
        "end": 6539.34,
        "text": " You put them in a sequence and then you train the model by distillation."
    },
    {
        "id": 1266,
        "start": 6539.9800000000005,
        "end": 6544.54,
        "text": " So you are keeping the entire model frozen and you're only training the representations of the new tokens."
    },
    {
        "id": 1267,
        "start": 6546.0,
        "end": 6556.54,
        "text": " And you're optimizing over the new tokens such that the behavior of the language model is identical to the model that has a very long prompt that works for you."
    },
    {
        "id": 1268,
        "start": 6556.820000000001,
        "end": 6561.4400000000005,
        "text": " And so it's a compression technique of compressing that very long prompt into those few new just tokens."
    },
    {
        "id": 1269,
        "start": 6562.46,
        "end": 6572.04,
        "text": " And so you can train this and then at this time you can discard your old prompt and just swap in those tokens and these sort of like stand in for that very long prompt and have an almost identical performance."
    },
    {
        "id": 1270,
        "start": 6572.76,
        "end": 6581.860000000001,
        "text": " And so this is one technique in a class of parameter efficient fine tuning techniques where most of the model is basically fixed and there's no training of the model weights."
    },
    {
        "id": 1271,
        "start": 6582.14,
        "end": 6584.900000000001,
        "text": " There's no training of Laura or anything like that of new parameters."
    },
    {
        "id": 1272,
        "start": 6585.52,
        "end": 6589.120000000001,
        "text": " The parameters that you're training are now just the token embeddings."
    },
    {
        "id": 1273,
        "start": 6589.8,
        "end": 6592.660000000001,
        "text": " So that's just one example, but this could again be like an entire video."
    },
    {
        "id": 1274,
        "start": 6593.06,
        "end": 6597.0,
        "text": " But just to give you a sense that there's a whole design space here that is potentially worth exploring in the future."
    },
    {
        "id": 1275,
        "start": 6597.76,
        "end": 6608.72,
        "text": " The next thing I want to briefly address is that I think recently there's a lot of momentum in how you actually could construct transformers that can simultaneously process not just text as the modality, but a lot of other modalities."
    },
    {
        "id": 1276,
        "start": 6608.8,
        "end": 6611.4400000000005,
        "text": " So be it images, videos, audio, etc."
    },
    {
        "id": 1277,
        "start": 6612.14,
        "end": 6617.04,
        "text": " And how do you feed in all these modalities and potentially predict these modalities from a transformer?"
    },
    {
        "id": 1278,
        "start": 6617.96,
        "end": 6620.0,
        "text": " Do you have to change the architecture in some fundamental way?"
    },
    {
        "id": 1279,
        "start": 6620.6,
        "end": 6623.900000000001,
        "text": " I think what a lot of people are starting to converge towards is that you're not changing the architecture."
    },
    {
        "id": 1280,
        "start": 6624.22,
        "end": 6633.820000000001,
        "text": " You stick with the transformer, you just kind of tokenize your impid domains and then call today and pretend it's just text tokens and just do everything else identical in an identical manner."
    },
    {
        "id": 1281,
        "start": 6634.58,
        "end": 6641.18,
        "text": " So here, for example, there was an early paper that has nice graphic for how you can take an image and you can chunk it into integers."
    },
    {
        "id": 1282,
        "start": 6642.64,
        "end": 6647.400000000001,
        "text": " And these sometimes, so these will basically become the tokens of images as an example."
    },
    {
        "id": 1283,
        "start": 6648.1,
        "end": 6652.34,
        "text": " And these tokens can be hard tokens where you force them to be integers."
    },
    {
        "id": 1284,
        "start": 6652.8,
        "end": 6663.84,
        "text": " They can also be soft tokens where you sort of don't require these to be discrete, but you do force these representations to go through bottlenecks like auto encoders."
    },
    {
        "id": 1285,
        "start": 6664.92,
        "end": 6673.58,
        "text": " Also in this paper that came up from opening I saw which I think really blew the mind of many people and inspired a lot of people in terms of what's possible."
    },
    {
        "id": 1286,
        "start": 6674.120000000001,
        "end": 6679.96,
        "text": " They have a graphic here and they talk briefly about how elements have text tokens, so I have visual patches."
    },
    {
        "id": 1287,
        "start": 6680.52,
        "end": 6686.34,
        "text": " So again, they came up with a way to chunkate videos into basically tokens when they're overcabularies."
    },
    {
        "id": 1288,
        "start": 6686.740000000001,
        "end": 6692.22,
        "text": " And then you can either process discrete tokens, say with other rest of models or even soft tokens with diffusion models."
    },
    {
        "id": 1289,
        "start": 6692.860000000001,
        "end": 6700.700000000001,
        "text": " And all of that is sort of being actively worked on and designed on and is beyond the scope of this video, but just something I wanted to mention briefly."
    },
    {
        "id": 1290,
        "start": 6701.200000000001,
        "end": 6707.14,
        "text": " Okay, now that we have come quite deep into the tokenization algorithm and we understand a lot more about how it works."
    },
    {
        "id": 1291,
        "start": 6707.5,
        "end": 6712.9800000000005,
        "text": " Let's look back around to the beginning of this video and go through some of these bullet points and really see why they happen."
    },
    {
        "id": 1292,
        "start": 6713.88,
        "end": 6718.900000000001,
        "text": " So first of all, why can't my LLM spell words very well or do other spell-related tasks?"
    },
    {
        "id": 1293,
        "start": 6720.84,
        "end": 6728.26,
        "text": " So fundamentally this is because as we saw, these characters are chunked up into tokens and some of these tokens are actually fairly long."
    },
    {
        "id": 1294,
        "start": 6729.040000000001,
        "end": 6734.14,
        "text": " So as an example, I went to the GPT-4 vocabulary and I looked at one of the longer tokens."
    },
    {
        "id": 1295,
        "start": 6734.4400000000005,
        "end": 6738.3,
        "text": " So that default style turns out to be a single individual token."
    },
    {
        "id": 1296,
        "start": 6738.620000000001,
        "end": 6740.280000000001,
        "text": " So that's a lot of characters for a single token."
    },
    {
        "id": 1297,
        "start": 6741.06,
        "end": 6749.900000000001,
        "text": " So my suspicion is that there's just too much crammed into this single token and my suspicion was that the model should not be very good at task related to spelling."
    },
    {
        "id": 1298,
        "start": 6753.56,
        "end": 6765.1,
        "text": " So I asked how many letters L are there in the word dot default style and of course my prompt is intentionally done that way and you see how the full style will be a single token."
    },
    {
        "id": 1299,
        "start": 6765.64,
        "end": 6766.820000000001,
        "text": " So this is what the model sees."
    },
    {
        "id": 1300,
        "start": 6767.4400000000005,
        "end": 6770.72,
        "text": " So my suspicion is that it wouldn't be very good at this and indeed it is not."
    },
    {
        "id": 1301,
        "start": 6771.120000000001,
        "end": 6773.08,
        "text": " It doesn't actually know how many Ls are in there."
    },
    {
        "id": 1302,
        "start": 6773.280000000001,
        "end": 6775.740000000001,
        "text": " The things there are three and actually there are four."
    },
    {
        "id": 1303,
        "start": 6776.040000000001,
        "end": 6777.240000000001,
        "text": " I'm not getting this wrong myself."
    },
    {
        "id": 1304,
        "start": 6778.56,
        "end": 6779.9400000000005,
        "text": " So that didn't go extremely well."
    },
    {
        "id": 1305,
        "start": 6780.58,
        "end": 6783.900000000001,
        "text": " Let's look at another kind of character level task."
    },
    {
        "id": 1306,
        "start": 6784.740000000001,
        "end": 6791.6,
        "text": " So for example, here I asked GPT-4 to reverse the string default style and try to use a code interpreter."
    },
    {
        "id": 1307,
        "start": 6792.040000000001,
        "end": 6794.38,
        "text": " I stopped it and I said just do it."
    },
    {
        "id": 1308,
        "start": 6794.38,
        "end": 6797.700000000001,
        "text": " Just try it and it gave me a jumble."
    },
    {
        "id": 1309,
        "start": 6798.040000000001,
        "end": 6803.16,
        "text": " So it doesn't actually really know how to reverse this string going from right to left."
    },
    {
        "id": 1310,
        "start": 6803.820000000001,
        "end": 6805.08,
        "text": " So it gave it a wrong result."
    },
    {
        "id": 1311,
        "start": 6805.84,
        "end": 6811.26,
        "text": " So again, like working with this working hypothesis that maybe this is due to the tokenization, I tried the different approach."
    },
    {
        "id": 1312,
        "start": 6811.26,
        "end": 6815.9800000000005,
        "text": " I said, okay, let's reverse the exact same string but take the following approach."
    },
    {
        "id": 1313,
        "start": 6816.4800000000005,
        "end": 6821.860000000001,
        "text": " Step one, just print out every single character separated by spaces and then as a step two, reverse that list."
    },
    {
        "id": 1314,
        "start": 6822.5,
        "end": 6828.4400000000005,
        "text": " And it again tried to use the tool but when I stopped it, it first produced all the characters and that was actually correct."
    },
    {
        "id": 1315,
        "start": 6829.26,
        "end": 6831.5,
        "text": " And then it reversed them and that was correct once it had this."
    },
    {
        "id": 1316,
        "start": 6832.16,
        "end": 6838.780000000001,
        "text": " So somehow it can't reverse it directly but when you go just first, you know, listing it out in order, it can do that somehow."
    },
    {
        "id": 1317,
        "start": 6839.320000000001,
        "end": 6844.08,
        "text": " And then it can once it's broken up this way, this becomes all these individual characters."
    },
    {
        "id": 1318,
        "start": 6844.68,
        "end": 6849.8,
        "text": " And so now this is much easier for it to see these individual tokens and reverse them and print them out."
    },
    {
        "id": 1319,
        "start": 6850.92,
        "end": 6852.68,
        "text": " So that is kind of interesting."
    },
    {
        "id": 1320,
        "start": 6853.780000000001,
        "end": 6854.64,
        "text": " So let's continue now."
    },
    {
        "id": 1321,
        "start": 6855.66,
        "end": 6859.16,
        "text": " Why are LLM's words at non-English languages?"
    },
    {
        "id": 1322,
        "start": 6859.68,
        "end": 6874.72,
        "text": " And I briefly covered this already but basically it's not only that the language model sees less non-English data during training of the model parameters, but also the tokenizer is not sufficiently trained on non-English data."
    },
    {
        "id": 1323,
        "start": 6875.360000000001,
        "end": 6877.3,
        "text": " And so here for example, hello, how are you?"
    },
    {
        "id": 1324,
        "start": 6877.6,
        "end": 6881.200000000001,
        "text": " So this is five tokens and its translation is 15 tokens."
    },
    {
        "id": 1325,
        "start": 6881.58,
        "end": 6883.34,
        "text": " So this is a three times blow up."
    },
    {
        "id": 1326,
        "start": 6884.18,
        "end": 6889.84,
        "text": " And so for example, onion hasseo is just hello basically in Korean and that ends up being three tokens."
    },
    {
        "id": 1327,
        "start": 6890.1,
        "end": 6892.620000000001,
        "text": " I'm actually kind of surprised by that because that is a very common phrase."
    },
    {
        "id": 1328,
        "start": 6893.1,
        "end": 6898.540000000001,
        "text": " And there's the typical greeting of like hello and that ends up being three tokens whereas our hello is a single token."
    },
    {
        "id": 1329,
        "start": 6899.1,
        "end": 6905.700000000001,
        "text": " And so basically everything is a lot more bloated and diffuse and this is I think partly the reason that the model works worse on other languages."
    },
    {
        "id": 1330,
        "start": 6907.16,
        "end": 6910.0,
        "text": " Coming back, why is LLM bad at simple arithmetic?"
    },
    {
        "id": 1331,
        "start": 6911.68,
        "end": 6914.58,
        "text": " That has to do with the tokenization of numbers."
    },
    {
        "id": 1332,
        "start": 6915.740000000001,
        "end": 6924.26,
        "text": " And so you'll notice that for example, addition is very sort of like there's an algorithm that is like character level for doing addition."
    },
    {
        "id": 1333,
        "start": 6924.92,
        "end": 6928.700000000001,
        "text": " So for example, here we would first add the ones and then the tens and then the hundreds."
    },
    {
        "id": 1334,
        "start": 6929.02,
        "end": 6931.66,
        "text": " You have to refer to specific parts of these digits."
    },
    {
        "id": 1335,
        "start": 6932.6,
        "end": 6939.400000000001,
        "text": " But these numbers are represented completely arbitrarily based on whatever happened to merge or not merge during the tokenization process."
    },
    {
        "id": 1336,
        "start": 6940.1,
        "end": 6942.46,
        "text": " There's an entire block post about this that I think is quite good."
    },
    {
        "id": 1337,
        "start": 6942.92,
        "end": 6944.26,
        "text": " Integer tokenization is insane."
    },
    {
        "id": 1338,
        "start": 6944.8,
        "end": 6950.5,
        "text": " And this person basically systematically explores the tokenization of numbers in I believe this is GPT2."
    },
    {
        "id": 1339,
        "start": 6951.200000000001,
        "end": 6962.1,
        "text": " And so they notice that for example for the for four digit numbers that you can take a look at whether it is a single token or whether it is two tokens that is a one three or two two."
    },
    {
        "id": 1340,
        "start": 6962.240000000001,
        "end": 6963.42,
        "text": " Or a three one combination."
    },
    {
        "id": 1341,
        "start": 6963.9800000000005,
        "end": 6966.4400000000005,
        "text": " And so all the different numbers are all the different combinations."
    },
    {
        "id": 1342,
        "start": 6966.9800000000005,
        "end": 6969.22,
        "text": " And you can imagine this is completely arbitrarily so."
    },
    {
        "id": 1343,
        "start": 6969.72,
        "end": 6977.820000000001,
        "text": " And the model unfortunately sometimes sees for a token for for all four digits sometimes for three sometimes for two sometimes for one."
    },
    {
        "id": 1344,
        "start": 6978.360000000001,
        "end": 6980.780000000001,
        "text": " And it's in an arbitrary matter."
    },
    {
        "id": 1345,
        "start": 6981.280000000001,
        "end": 6984.860000000001,
        "text": " And so this is definitely a headwind if you will for the language model."
    },
    {
        "id": 1346,
        "start": 6985.22,
        "end": 6987.700000000001,
        "text": " And it's kind of incredible that it can kind of do it and deal with it."
    },
    {
        "id": 1347,
        "start": 6987.700000000001,
        "end": 6989.66,
        "text": " But it's also kind of not ideal."
    },
    {
        "id": 1348,
        "start": 6990.16,
        "end": 7002.1,
        "text": " And so that's why for example we saw that meta when they trained the llama to algorithm and you sent a space they make sure to split up all the all the digits as an example for llama to."
    },
    {
        "id": 1349,
        "start": 7002.64,
        "end": 7005.76,
        "text": " And this is partly to improve a simple arithmetic kind of performance."
    },
    {
        "id": 1350,
        "start": 7007.18,
        "end": 7016.120000000001,
        "text": " And finally why is GPT2 not as good in Python again this is partly a modeling issue on in the architecture and the data set and the strength of the model."
    },
    {
        "id": 1351,
        "start": 7016.6,
        "end": 7025.92,
        "text": " But it's also partly tokenization because as we saw here with the simple Python example the encoding efficiency of the tokenizer for handling spaces in Python is terrible."
    },
    {
        "id": 1352,
        "start": 7026.320000000001,
        "end": 7031.92,
        "text": " And every single spaces in individual token and this dramatically reduces the context length that the model can attend across."
    },
    {
        "id": 1353,
        "start": 7032.5,
        "end": 7037.92,
        "text": " So that's almost like a tokenization bug for GPT2 and that was later fixed with GPT4."
    },
    {
        "id": 1354,
        "start": 7038.76,
        "end": 7043.52,
        "text": " Okay so here's another fun one my little I'm a broccoli halts when I it sees the string end of text."
    },
    {
        "id": 1355,
        "start": 7044.280000000001,
        "end": 7046.620000000001,
        "text": " So here's here's a very strange behavior."
    },
    {
        "id": 1356,
        "start": 7047.38,
        "end": 7052.16,
        "text": " Print a string end of text is what I told GPT4 and it says creeply specify the string."
    },
    {
        "id": 1357,
        "start": 7053.16,
        "end": 7058.5,
        "text": " And I'm telling you it's give me end of text and it seems like there's an issue it's not seeing end of text."
    },
    {
        "id": 1358,
        "start": 7059.4400000000005,
        "end": 7064.540000000001,
        "text": " And then I give it end of text is the string and then here's a string and then it just doesn't print it."
    },
    {
        "id": 1359,
        "start": 7065.0,
        "end": 7086.56,
        "text": " So obviously something is breaking here with respect to the handling of the special token and I didn't actually know what open AI is doing under the hood here and whether they are potentially parsing this as an as an actual token instead of this just being end of text as like individual sort of piece of it without the special token handling logic."
    },
    {
        "id": 1360,
        "start": 7087.4800000000005,
        "end": 7097.820000000001,
        "text": " And so it might be that someone when they're calling dot encode they are passing in the allowed special and they are allowing end of text as a special character in the user prompt."
    },
    {
        "id": 1361,
        "start": 7098.4400000000005,
        "end": 7102.42,
        "text": " But the user prompt of course is sort of attacker control text."
    },
    {
        "id": 1362,
        "start": 7102.84,
        "end": 7111.42,
        "text": " So you would hope that they don't really parse or use special tokens or you know from that kind of input but it appears that there's something definitely going wrong here."
    },
    {
        "id": 1363,
        "start": 7111.820000000001,
        "end": 7116.58,
        "text": " And so your knowledge of these special tokens ends up being an attack surface potentially."
    },
    {
        "id": 1364,
        "start": 7117.3,
        "end": 7124.64,
        "text": " And so if you'd like to confuse LLMS then just try to give them some special tokens and see if you're breaking something by chance."
    },
    {
        "id": 1365,
        "start": 7125.4800000000005,
        "end": 7127.06,
        "text": " Okay, so this next one is a really fun one."
    },
    {
        "id": 1366,
        "start": 7128.66,
        "end": 7130.740000000001,
        "text": " The Trilling White Space issue."
    },
    {
        "id": 1367,
        "start": 7130.8,
        "end": 7138.620000000001,
        "text": " So if you come to playground and we come here to GPT 3.5 to instruct so this is not a chat model."
    },
    {
        "id": 1368,
        "start": 7138.620000000001,
        "end": 7139.8,
        "text": " This is a completion model."
    },
    {
        "id": 1369,
        "start": 7139.9400000000005,
        "end": 7143.66,
        "text": " So think of it more like it's a lot more closer to a base model."
    },
    {
        "id": 1370,
        "start": 7143.780000000001,
        "end": 7144.8,
        "text": " It does completion."
    },
    {
        "id": 1371,
        "start": 7145.14,
        "end": 7146.700000000001,
        "text": " It will continue the token sequence."
    },
    {
        "id": 1372,
        "start": 7147.92,
        "end": 7150.92,
        "text": " So here's a tagline for ice cream shop and we want to continue the sequence."
    },
    {
        "id": 1373,
        "start": 7151.8,
        "end": 7153.900000000001,
        "text": " And so we can submit and get a bunch of tokens."
    },
    {
        "id": 1374,
        "start": 7154.780000000001,
        "end": 7155.700000000001,
        "text": " Okay, no problem."
    },
    {
        "id": 1375,
        "start": 7156.620000000001,
        "end": 7163.740000000001,
        "text": " But now suppose I do this but instead of pressing submit here, I do here's a tagline for ice cream shop space."
    },
    {
        "id": 1376,
        "start": 7165.02,
        "end": 7167.4800000000005,
        "text": " So I have a space here before I collect submit."
    },
    {
        "id": 1377,
        "start": 7169.22,
        "end": 7169.740000000001,
        "text": " We get a warning."
    },
    {
        "id": 1378,
        "start": 7170.42,
        "end": 7175.4800000000005,
        "text": " Your text ends in a training space which causes worse performance due to how API splits text into tokens."
    },
    {
        "id": 1379,
        "start": 7176.42,
        "end": 7177.08,
        "text": " So what's happening here?"
    },
    {
        "id": 1380,
        "start": 7177.320000000001,
        "end": 7179.870000000001,
        "text": " It still gave us a sort of completion here."
    },
    {
        "id": 1381,
        "start": 7179.870000000001,
        "end": 7181.120000000001,
        "text": " But let's take a look at what's happening."
    },
    {
        "id": 1382,
        "start": 7183.040000000001,
        "end": 7184.540000000001,
        "text": " So here's a tagline for ice cream shop."
    },
    {
        "id": 1383,
        "start": 7185.780000000001,
        "end": 7189.280000000001,
        "text": " And then what does this look like in the actual training data?"
    },
    {
        "id": 1384,
        "start": 7189.280000000001,
        "end": 7195.120000000001,
        "text": " Suppose you found the completion in the training document somewhere on the internet and the LLM trained on this data."
    },
    {
        "id": 1385,
        "start": 7195.540000000001,
        "end": 7199.450000000001,
        "text": " So maybe it's something like, oh yeah, maybe that's the tagline."
    },
    {
        "id": 1386,
        "start": 7199.450000000001,
        "end": 7200.18,
        "text": " That's a terrible tagline."
    },
    {
        "id": 1387,
        "start": 7200.18,
        "end": 7206.81,
        "text": " But notice here that when I create, oh, you see that because there's the space scarers,"
    },
    {
        "id": 1388,
        "start": 7206.81,
        "end": 7210.02,
        "text": " there's always a prefix to these tokens in GPT."
    },
    {
        "id": 1389,
        "start": 7210.6,
        "end": 7212.9400000000005,
        "text": " So it's not an O token. It's a space O token."
    },
    {
        "id": 1390,
        "start": 7213.18,
        "end": 7215.06,
        "text": " The space is part of the O."
    },
    {
        "id": 1391,
        "start": 7215.68,
        "end": 7218.08,
        "text": " And together they are token 88840."
    },
    {
        "id": 1392,
        "start": 7218.360000000001,
        "end": 7219.320000000001,
        "text": " That's space O."
    },
    {
        "id": 1393,
        "start": 7220.42,
        "end": 7223.9400000000005,
        "text": " So what's happening here is that when I just have it like this,"
    },
    {
        "id": 1394,
        "start": 7224.92,
        "end": 7229.42,
        "text": " and I let it complete the next token, it can sample the space O token."
    },
    {
        "id": 1395,
        "start": 7230.26,
        "end": 7232.740000000001,
        "text": " But instead if I have this and I add my space,"
    },
    {
        "id": 1396,
        "start": 7233.3,
        "end": 7239.4400000000005,
        "text": " then what I'm doing here when I encode this string is I have basically here's a tagline for an ice cream shop."
    },
    {
        "id": 1397,
        "start": 7240.14,
        "end": 7242.540000000001,
        "text": " And this space at the very end becomes a token 220."
    },
    {
        "id": 1398,
        "start": 7244.240000000001,
        "end": 7246.280000000001,
        "text": " And so we've added token 220."
    },
    {
        "id": 1399,
        "start": 7246.9800000000005,
        "end": 7251.8,
        "text": " And this token otherwise would be part of the tagline because if there actually is a tagline here,"
    },
    {
        "id": 1400,
        "start": 7251.820000000001,
        "end": 7253.5,
        "text": " so space O is the token."
    },
    {
        "id": 1401,
        "start": 7254.4800000000005,
        "end": 7260.3,
        "text": " And so this is suddenly out of distribution for the model because this space is part of the next token."
    },
    {
        "id": 1402,
        "start": 7260.68,
        "end": 7262.52,
        "text": " But we're putting it here like this."
    },
    {
        "id": 1403,
        "start": 7263.1,
        "end": 7268.120000000001,
        "text": " And the model has seen very, very little data of actual space by itself."
    },
    {
        "id": 1404,
        "start": 7268.92,
        "end": 7271.46,
        "text": " And we're asking it to complete the sequence like adding more tokens."
    },
    {
        "id": 1405,
        "start": 7271.9800000000005,
        "end": 7274.860000000001,
        "text": " But the problem is that we've sort of begun the first token."
    },
    {
        "id": 1406,
        "start": 7275.42,
        "end": 7277.18,
        "text": " And now it's been split up."
    },
    {
        "id": 1407,
        "start": 7277.52,
        "end": 7278.66,
        "text": " And now we're out of distribution."
    },
    {
        "id": 1408,
        "start": 7279.18,
        "end": 7280.64,
        "text": " And now arbitrary bad things happen."
    },
    {
        "id": 1409,
        "start": 7281.320000000001,
        "end": 7284.16,
        "text": " And it's just a very rare example for it to see something like that."
    },
    {
        "id": 1410,
        "start": 7284.66,
        "end": 7286.5,
        "text": " And that's why we did the warning."
    },
    {
        "id": 1411,
        "start": 7286.92,
        "end": 7292.5,
        "text": " So the fundamental issue here is of course that the LLM is on top of these tokens."
    },
    {
        "id": 1412,
        "start": 7292.84,
        "end": 7294.14,
        "text": " And these tokens are text chunks."
    },
    {
        "id": 1413,
        "start": 7294.14,
        "end": 7295.76,
        "text": " They're not characters in a way."
    },
    {
        "id": 1414,
        "start": 7297.0,
        "end": 7299.84,
        "text": " They are, these are the atoms of what the LLM is seeing."
    },
    {
        "id": 1415,
        "start": 7299.9800000000005,
        "end": 7301.9400000000005,
        "text": " And there's a bunch of weird stuff that comes out of it."
    },
    {
        "id": 1416,
        "start": 7302.360000000001,
        "end": 7305.240000000001,
        "text": " Let's go back to our default cell style."
    },
    {
        "id": 1417,
        "start": 7305.9800000000005,
        "end": 7313.46,
        "text": " I bet you that the model has never in its training set seen default cell star without LE in there."
    },
    {
        "id": 1418,
        "start": 7314.46,
        "end": 7319.42,
        "text": " It's always seen as a single group because this is some kind of a function in..."
    },
    {
        "id": 1419,
        "start": 7320.64,
        "end": 7322.900000000001,
        "text": " I don't actually know what this is part of, this is some kind of API."
    },
    {
        "id": 1420,
        "start": 7323.42,
        "end": 7328.360000000001,
        "text": " But I bet you that it's never seen this combination of tokens in its training data."
    },
    {
        "id": 1421,
        "start": 7328.900000000001,
        "end": 7331.02,
        "text": " Because I think it would be extremely rare."
    },
    {
        "id": 1422,
        "start": 7331.46,
        "end": 7333.06,
        "text": " So I took this and I kind of based it here."
    },
    {
        "id": 1423,
        "start": 7333.9800000000005,
        "end": 7336.320000000001,
        "text": " And I had, I could try to complete from it."
    },
    {
        "id": 1424,
        "start": 7336.76,
        "end": 7338.6,
        "text": " And that immediately gave me a big error."
    },
    {
        "id": 1425,
        "start": 7339.18,
        "end": 7341.860000000001,
        "text": " And it said the model of particular completion that begins with a stop sequence."
    },
    {
        "id": 1426,
        "start": 7341.88,
        "end": 7342.9400000000005,
        "text": " You're solving in no output."
    },
    {
        "id": 1427,
        "start": 7342.9800000000005,
        "end": 7344.92,
        "text": " Consider adjusting your prompt or stop sequences."
    },
    {
        "id": 1428,
        "start": 7345.620000000001,
        "end": 7351.740000000001,
        "text": " So what happened here when I clicked submit is that immediately the model emitted and sort of like end of text token."
    },
    {
        "id": 1429,
        "start": 7351.76,
        "end": 7352.88,
        "text": " I think or something like that."
    },
    {
        "id": 1430,
        "start": 7353.5,
        "end": 7357.22,
        "text": " It basically predicted the stop sequence immediately so that it had no completion."
    },
    {
        "id": 1431,
        "start": 7357.84,
        "end": 7361.780000000001,
        "text": " And so this is why I'm getting a warning again because we're off the data distribution."
    },
    {
        "id": 1432,
        "start": 7361.820000000001,
        "end": 7366.820000000001,
        "text": " And the model is just predicting totally arbitrary things."
    },
    {
        "id": 1433,
        "start": 7366.84,
        "end": 7368.08,
        "text": " It's just really confused basically."
    },
    {
        "id": 1434,
        "start": 7368.56,
        "end": 7370.1,
        "text": " This is, this is giving it brain damage."
    },
    {
        "id": 1435,
        "start": 7370.1,
        "end": 7371.860000000001,
        "text": " It's never seen this before. It's shocked."
    },
    {
        "id": 1436,
        "start": 7372.64,
        "end": 7374.0,
        "text": " And it's predicting end of text or something."
    },
    {
        "id": 1437,
        "start": 7374.700000000001,
        "end": 7375.58,
        "text": " I tried it again here."
    },
    {
        "id": 1438,
        "start": 7375.84,
        "end": 7377.56,
        "text": " And in this case, it completed it."
    },
    {
        "id": 1439,
        "start": 7378.08,
        "end": 7381.6,
        "text": " But then for some reason, this request may violate our usage policies."
    },
    {
        "id": 1440,
        "start": 7382.02,
        "end": 7382.820000000001,
        "text": " This was flagged."
    },
    {
        "id": 1441,
        "start": 7384.92,
        "end": 7386.3,
        "text": " Basically something just like goes wrong."
    },
    {
        "id": 1442,
        "start": 7386.34,
        "end": 7387.21,
        "text": " And there's something like jank."
    },
    {
        "id": 1443,
        "start": 7387.21,
        "end": 7391.040000000001,
        "text": " You can just feel the jank because the model is like extremely unhappy with just this."
    },
    {
        "id": 1444,
        "start": 7391.540000000001,
        "end": 7394.06,
        "text": " And it doesn't know how to complete it because it's never a curtain training set."
    },
    {
        "id": 1445,
        "start": 7394.58,
        "end": 7398.16,
        "text": " In a training set, it always appears like this and becomes a single token."
    },
    {
        "id": 1446,
        "start": 7399.200000000001,
        "end": 7405.120000000001,
        "text": " So these kinds of issues where tokens are either you sort of like complete the first character of the next token."
    },
    {
        "id": 1447,
        "start": 7405.76,
        "end": 7409.780000000001,
        "text": " Or you are sort of you have long tokens that you then have just some of the characters off."
    },
    {
        "id": 1448,
        "start": 7410.540000000001,
        "end": 7415.280000000001,
        "text": " All of these are kind of like issues with partial tokens is how I would describe it."
    },
    {
        "id": 1449,
        "start": 7416.02,
        "end": 7422.26,
        "text": " And if you actually dig into the token repository, go to the rust code and search for unstable."
    },
    {
        "id": 1450,
        "start": 7424.400000000001,
        "end": 7430.1,
        "text": " And you'll see in code unstable native unstable tokens and a lot of like special case handling."
    },
    {
        "id": 1451,
        "start": 7430.72,
        "end": 7433.64,
        "text": " None of this stuff about unstable tokens is documented anywhere."
    },
    {
        "id": 1452,
        "start": 7434.16,
        "end": 7440.9400000000005,
        "text": " But there's a ton of code dealing with unstable tokens and unstable tokens as exactly kind of like what I'm describing here."
    },
    {
        "id": 1453,
        "start": 7441.540000000001,
        "end": 7445.1,
        "text": " What you would like out of a completion API is something a lot more fancy."
    },
    {
        "id": 1454,
        "start": 7445.42,
        "end": 7449.08,
        "text": " Like if we're putting in default cell star, if we're asking for the next token sequence,"
    },
    {
        "id": 1455,
        "start": 7449.780000000001,
        "end": 7452.92,
        "text": " we're not actually trying to append the next token exactly after this list."
    },
    {
        "id": 1456,
        "start": 7453.64,
        "end": 7460.08,
        "text": " We're actually trying to append, we're trying to consider lots of tokens that if we were,"
    },
    {
        "id": 1457,
        "start": 7460.540000000001,
        "end": 7468.08,
        "text": " I guess like we're trying to search over characters that if we retokenized would be of high probability, if that makes sense."
    },
    {
        "id": 1458,
        "start": 7469.42,
        "end": 7477.16,
        "text": " So that we can actually add single individual character instead of just like adding the next full token that comes after this partial token list."
    },
    {
        "id": 1459,
        "start": 7478.02,
        "end": 7481.740000000001,
        "text": " So this is very tricky to describe and I invite you to maybe like look through this."
    },
    {
        "id": 1460,
        "start": 7482.08,
        "end": 7486.58,
        "text": " It ends up being extremely gnarly and hairy kind of topic and it comes from tokenization fundamentally."
    },
    {
        "id": 1461,
        "start": 7487.66,
        "end": 7491.58,
        "text": " So maybe I can even spend an entire video talking about unstable tokens sometime in the future."
    },
    {
        "id": 1462,
        "start": 7492.360000000001,
        "end": 7494.16,
        "text": " Okay, and I'm really saving the best for last."
    },
    {
        "id": 1463,
        "start": 7494.56,
        "end": 7497.360000000001,
        "text": " My favorite one by far is this solid gold magic harp."
    },
    {
        "id": 1464,
        "start": 7499.4800000000005,
        "end": 7503.08,
        "text": " It's just okay. This comes from this block post solid gold magic harp."
    },
    {
        "id": 1465,
        "start": 7503.88,
        "end": 7508.56,
        "text": " And this is internet famous now for those of us in all of us."
    },
    {
        "id": 1466,
        "start": 7509.240000000001,
        "end": 7512.34,
        "text": " And basically I would advise you to read this block post in full."
    },
    {
        "id": 1467,
        "start": 7512.780000000001,
        "end": 7523.88,
        "text": " But basically what this person was doing is this person went to the token embedding stable and clustered the tokens based on their embedding representation."
    },
    {
        "id": 1468,
        "start": 7524.900000000001,
        "end": 7529.280000000001,
        "text": " And this person noticed that there's a cluster of tokens that look really strange."
    },
    {
        "id": 1469,
        "start": 7529.620000000001,
        "end": 7540.08,
        "text": " So there's a cluster here at rot, extreme fame, solid gold magic harp, sign up message like really weird tokens in basically in this embedding cluster."
    },
    {
        "id": 1470,
        "start": 7540.76,
        "end": 7545.06,
        "text": " And so where are these tokens and where do they even come from? Like what does solid gold magic harp makes no sense?"
    },
    {
        "id": 1471,
        "start": 7545.76,
        "end": 7548.66,
        "text": " And then they found a bunch of these tokens."
    },
    {
        "id": 1472,
        "start": 7550.4400000000005,
        "end": 7555.9400000000005,
        "text": " And then they noticed that actually the block thickens here because if you ask the model about these tokens like you ask it."
    },
    {
        "id": 1473,
        "start": 7556.34,
        "end": 7561.200000000001,
        "text": " Some very benign question like please can you repeat back to me the string sold gold magic harp."
    },
    {
        "id": 1474,
        "start": 7561.9800000000005,
        "end": 7566.26,
        "text": " Then you get a variety of basically totally broken lLM behavior."
    },
    {
        "id": 1475,
        "start": 7566.68,
        "end": 7572.26,
        "text": " So either you get evasion so I'm sorry I can't hear you or you get a bunch of hallucinations as a response."
    },
    {
        "id": 1476,
        "start": 7573.6,
        "end": 7581.14,
        "text": " You can even get back like insults so you ask it about streamer bought and it tells them and the model actually just calls you names."
    },
    {
        "id": 1477,
        "start": 7582.1,
        "end": 7590.8,
        "text": " Or it kind of comes up with like weird humor like you're actually breaking the model by asking about these very simple strings like at rot and sold gold magic harp."
    },
    {
        "id": 1478,
        "start": 7591.400000000001,
        "end": 7594.88,
        "text": " So like what the hell is happening and there's a variety of here documented behaviors."
    },
    {
        "id": 1479,
        "start": 7595.96,
        "end": 7599.6,
        "text": " There's a bunch of tokens not just sold gold magic harp that have that kind of behavior."
    },
    {
        "id": 1480,
        "start": 7600.58,
        "end": 7607.02,
        "text": " And so basically there's a bunch of like trigger words. If you ask the model about these trigger words or you just include them in your prompt."
    },
    {
        "id": 1481,
        "start": 7607.3,
        "end": 7618.08,
        "text": " The model goes haywire and has all kinds of really strange behaviors including sort of ones that violate typical safety guidelines and the alignment of the model like it's swearing back at you."
    },
    {
        "id": 1482,
        "start": 7618.780000000001,
        "end": 7621.68,
        "text": " So what is happening here and how can this possibly be true?"
    },
    {
        "id": 1483,
        "start": 7622.6,
        "end": 7630.16,
        "text": " Well this again comes down to tokenization. So what's happening here is that sold gold magic harp if you actually begin to it is a Reddit user."
    },
    {
        "id": 1484,
        "start": 7630.780000000001,
        "end": 7632.860000000001,
        "text": " So there's a use slash sold gold magic harp."
    },
    {
        "id": 1485,
        "start": 7634.280000000001,
        "end": 7639.06,
        "text": " And probably what happened here even though I don't know that it says been like really definitively explored."
    },
    {
        "id": 1486,
        "start": 7639.58,
        "end": 7648.240000000001,
        "text": " But what it's thought to have happened is that the tokenization data set was very different from the training data set for the actual language model."
    },
    {
        "id": 1487,
        "start": 7648.88,
        "end": 7655.88,
        "text": " So in the tokenization data set there was a ton of Reddit data potentially where the user sold gold magic harp was mentioned in the text."
    },
    {
        "id": 1488,
        "start": 7656.46,
        "end": 7661.76,
        "text": " Because sold gold magic harp was a very common sort of person who would post a lot."
    },
    {
        "id": 1489,
        "start": 7662.34,
        "end": 7665.4400000000005,
        "text": " This would be a string that occurs many times in a tokenization data set."
    },
    {
        "id": 1490,
        "start": 7666.120000000001,
        "end": 7675.22,
        "text": " Because it occurs many times in the tokenization data set these tokens would end up getting merged to the single individual token for that single Reddit user sold gold magic harp."
    },
    {
        "id": 1491,
        "start": 7675.6,
        "end": 7682.46,
        "text": " So they would have a dedicated token in a vocabulary of was it 50,000 tokens in GPT 2 that is devoted to that Reddit user."
    },
    {
        "id": 1492,
        "start": 7683.42,
        "end": 7687.320000000001,
        "text": " And then what happens is the tokenization data set has those strings."
    },
    {
        "id": 1493,
        "start": 7687.84,
        "end": 7691.18,
        "text": " But then later when you train the model the language model itself."
    },
    {
        "id": 1494,
        "start": 7692.66,
        "end": 7694.540000000001,
        "text": " This data from Reddit was not present."
    },
    {
        "id": 1495,
        "start": 7695.6,
        "end": 7705.6,
        "text": " And so therefore in the entire training set for the language model sold gold magic harp never occurs that token never appears in the training set for the actual language model later."
    },
    {
        "id": 1496,
        "start": 7706.66,
        "end": 7708.5,
        "text": " So this token never gets activated."
    },
    {
        "id": 1497,
        "start": 7709.06,
        "end": 7711.16,
        "text": " It's initialized at random in the beginning of optimization."
    },
    {
        "id": 1498,
        "start": 7711.96,
        "end": 7714.18,
        "text": " Then you have forward backward passes and updates to the model."
    },
    {
        "id": 1499,
        "start": 7714.52,
        "end": 7716.84,
        "text": " And this token is just never updated in the embedding table."
    },
    {
        "id": 1500,
        "start": 7717.1,
        "end": 7718.900000000001,
        "text": " That row vector never gets sampled."
    },
    {
        "id": 1501,
        "start": 7719.16,
        "end": 7719.88,
        "text": " It never gets used."
    },
    {
        "id": 1502,
        "start": 7720.22,
        "end": 7722.5,
        "text": " So it never gets trained and it's completely untrained."
    },
    {
        "id": 1503,
        "start": 7722.88,
        "end": 7727.820000000001,
        "text": " It's kind of like unallocated memory in a typical binary program written as C or something like that."
    },
    {
        "id": 1504,
        "start": 7728.42,
        "end": 7729.56,
        "text": " So it's unallocated memory."
    },
    {
        "id": 1505,
        "start": 7729.900000000001,
        "end": 7736.08,
        "text": " And then at test time if you evoke this token, then you're basically blocking out a row of the embedding table that is completely untrained."
    },
    {
        "id": 1506,
        "start": 7736.46,
        "end": 7739.400000000001,
        "text": " And that feeds into a transformer and creates undefined behavior."
    },
    {
        "id": 1507,
        "start": 7740.0,
        "end": 7740.84,
        "text": " And that's what we're seeing here."
    },
    {
        "id": 1508,
        "start": 7740.84,
        "end": 7744.06,
        "text": " It's completely undefined never before seen in a training behavior."
    },
    {
        "id": 1509,
        "start": 7744.9400000000005,
        "end": 7754.22,
        "text": " And so any of these kind of like weird tokens would evoke this behavior because fundamentally the model is out of sample out of distribution."
    },
    {
        "id": 1510,
        "start": 7755.3,
        "end": 7759.92,
        "text": " Okay, and the very last thing I wanted to just briefly mention and point out, although I think a lot of people are quite aware of this,"
    },
    {
        "id": 1511,
        "start": 7760.540000000001,
        "end": 7770.8,
        "text": " is that different kinds of formats and different representations and different languages and so on might be more or less efficient with GPT tokenizers or any tokenizers for any other law for that matter."
    },
    {
        "id": 1512,
        "start": 7771.240000000001,
        "end": 7776.740000000001,
        "text": " So for example, Jason is actually really dense and tokens and Yamel is a lot more efficient in tokens."
    },
    {
        "id": 1513,
        "start": 7778.26,
        "end": 7781.88,
        "text": " So for example, this are these are the same in Jason and in the amel."
    },
    {
        "id": 1514,
        "start": 7782.46,
        "end": 7787.84,
        "text": " The Jason is 116 and the Yamel is 99 so quite a bit of an improvement."
    },
    {
        "id": 1515,
        "start": 7788.6,
        "end": 7800.1,
        "text": " And so in the token economy where we are paying per token in many ways and you are paying in the context length and you're paying in dollar amount for the cost of processing all this kind of structured data."
    },
    {
        "id": 1516,
        "start": 7800.240000000001,
        "end": 7814.700000000001,
        "text": " When you have to so prefer to use the amel. So over G sauce and in general kind of like the tokenization density something that you have to sort of care about and worry about at all times and try to find efficient encoding schemes and spend a lot of time in Tiktok"
    },
    {
        "id": 1517,
        "start": 7814.700000000001,
        "end": 7819.240000000001,
        "text": " and I say and measure the different token efficiencies of different formats and settings and so on."
    },
    {
        "id": 1518,
        "start": 7819.700000000001,
        "end": 7829.320000000001,
        "text": " Okay, so that concludes my fairly long video on tokenization. I know it's try. I know it's annoying. I know it's irritating. I personally really dislike the stage."
    },
    {
        "id": 1519,
        "start": 7829.84,
        "end": 7838.56,
        "text": " What I do have to say at this point is don't brush it off. There's a lot of foot guns sharp edges here security issues, AI safety issues as we saw,"
    },
    {
        "id": 1520,
        "start": 7838.6,
        "end": 7844.900000000001,
        "text": " plugging in unallocated memory into language models. So it's worth understanding this stage."
    },
    {
        "id": 1521,
        "start": 7846.64,
        "end": 7856.46,
        "text": " That said, I will say that eternal glory goes to anyone who can get rid of it. I showed you one possible paper that tried to do that and I think I hope a lot more can follow over time."
    },
    {
        "id": 1522,
        "start": 7857.280000000001,
        "end": 7867.400000000001,
        "text": " And my final recommendations for the application right now are if you can reuse the GPT4 tokens and the vocabulary in your application then that's something you should consider and just use Tiktokin because it is very efficient"
    },
    {
        "id": 1523,
        "start": 7867.96,
        "end": 7875.280000000001,
        "text": " and nice a library for inference for BP. I also really like the byte level BP that Tiktokin and OpenAI uses."
    },
    {
        "id": 1524,
        "start": 7876.42,
        "end": 7884.38,
        "text": " If you for some reason want to train your own vocabulary from scratch, then I would use the BP with sentence piece."
    },
    {
        "id": 1525,
        "start": 7885.66,
        "end": 7894.9400000000005,
        "text": " Oops, as I mentioned, I'm not a huge fan of sentence piece. I don't like its byte fallback and I don't like that it's doing BP on Unicode Quickpoints."
    },
    {
        "id": 1526,
        "start": 7895.5,
        "end": 7906.56,
        "text": " I think it's it also has like a million settings and I think there's a lot of foot guns here and I think it's really easy to miscalibrate them and you end up cropping your sentences or something like that because of some hyperprimary that you don't fully understand."
    },
    {
        "id": 1527,
        "start": 7906.700000000001,
        "end": 7919.0,
        "text": " So be very careful with the settings. Try to copy based exactly maybe you were metadid or basically spend a lot of time looking at all the hyperprimaries and go through the code of sentence piece and make sure that you have this correct."
    },
    {
        "id": 1528,
        "start": 7919.88,
        "end": 7925.6,
        "text": " But even if you have all the settings correct, I still think that the algorithm is kind of inferior to what's happening here."
    },
    {
        "id": 1529,
        "start": 7926.620000000001,
        "end": 7934.4800000000005,
        "text": " And maybe the best if you really need to train your vocabulary, maybe the best thing is to just wait for MNBPE to become as efficient as possible."
    },
    {
        "id": 1530,
        "start": 7935.120000000001,
        "end": 7944.4400000000005,
        "text": " And that's something that maybe I hope to work on. And at some point maybe we can be training basically really what we want is we want Tiktokin but training code."
    },
    {
        "id": 1531,
        "start": 7944.9800000000005,
        "end": 7952.96,
        "text": " And that is the ideal thing that currently does not exist. And MNBPE is is an implementation of it but currently it's in Python."
    },
    {
        "id": 1532,
        "start": 7954.1,
        "end": 7961.4800000000005,
        "text": " So that's currently what I have to say for tokenization. There might be an advanced video that has even drier and even more detailed in the future."
    },
    {
        "id": 1533,
        "start": 7962.02,
        "end": 7967.3,
        "text": " But for now, I think we're going to leave things off here and I hope that was helpful. Bye."
    },
    {
        "id": 1534,
        "start": 7973.3,
        "end": 7980.200000000001,
        "text": " And they increased this context size from GPT-1 of 512 to 2024 in GPT-4."
    },
    {
        "id": 1535,
        "start": 7981.1,
        "end": 7983.5,
        "text": " The next step."
    },
    {
        "id": 1536,
        "start": 7985.780000000001,
        "end": 7990.96,
        "text": " Okay, next I would like us to briefly walk through the code for OPLAYI on the GPT-2 and code that Pi."
    },
    {
        "id": 1537,
        "start": 7996.18,
        "end": 7997.040000000001,
        "text": " I'm sorry I'm going to sneeze."
    },
    {
        "id": 1538,
        "start": 7998.38,
        "end": 8004.56,
        "text": " And then what's happening here is this is a spurious layer that I will explain in a bit."
    },
    {
        "id": 1539,
        "start": 8006.38,
        "end": 8007.96,
        "text": " What's happening here is..."
    }
]